<!DOCTYPE html><html lang="en"><head>
<meta charset="utf-8">

<link rel="preload" href="/images/logo-smooth.svg" as="image" type="image/svg+xml">


<script type="text/javascript" src="https://ff.kis.v2.scr.kaspersky-labs.com/FD126C42-EBFA-4E12-B309-BB3FDD723AC1/main.js?attr=5HTfnEueB6kJXLjIMEgYgErT0uvmHGAI_5aUPvHxgukCR8XkMQZbLQ9VedkU3h4Y" charset="UTF-8"></script><style id="inlined-styles">blockquote{margin:1.625em 0 1.75em 0;border:1px solid #ccc;font-size:0.95em;padding:1em 1.25em}blockquote,blockquote blockquote blockquote,blockquote blockquote blockquote blockquote blockquote{z-index:-2;background-color:var(--GW-blockquote-background-color)}blockquote blockquote,blockquote blockquote blockquote blockquote,blockquote blockquote blockquote blockquote blockquote blockquote{background-color:#e6e6e6}:root{--GW-blockquote-background-color:#f5f5f5}html{padding:0;margin:0;background-color:#fff;color:#000;font-weight:400;font-family:"Source Serif Pro","Apple Garamond","Baskerville","Libre Baskerville","Times New Roman","Droid Serif","Times",serif}body{max-width:112ch}@media only screen and (max-width:64.9ch){html{font-size:18px}}@media only screen and (min-width:65ch){body{padding:0 1.5ch 0 0.5ch;margin:0 auto}@media only screen and (min-width:118.5ch){body{padding:0 6ch 0 0.5ch}}main{min-height:100vh;display:flex;flex-flow:column}@media only screen and (min-width:176.1ch){main{position:relative;right:4ch}}@supports (-moz-user-focus:normal){@media only screen and (min-width:176.1ch){main{right:0}}}article{flex:1 1 auto}#sidebar{position:absolute}header,article{margin-left:15ch}@media only screen and (max-width:120ch){header,article{margin-left:14.5ch}}@media only screen and (max-width:112ch){header,article{margin-left:14ch}}@media only screen and (max-width:104ch){header,article{margin-left:13.5ch}}@media only screen and (max-width:96ch){header,article{margin-left:13ch}}}@media only screen and (max-width:64.9ch){body{margin:0 1ch}}#sidebar code{border:none;background-color:transparent;padding:0}#sidebar a{display:block}@media only screen and (min-width:65ch){#sidebar{font-variant:small-caps;padding:0 4ch 0 1ch;width:10ch}#sidebar a#logo{margin:1em 0 2em 0}#sidebar a#logo img{width:64px}#sidebar a.patreon{border-top:1px dotted #aaa}#sidebar a.new{box-shadow:0 1.125em 0 0 #fff inset}#sidebar a.patreon{font-size:0.9em;margin-top:1.375em;padding-top:1.25em;box-shadow:0 1.25em 0 0 #fff inset;white-space:nowrap}#sidebar a.mail,#sidebar a.r_gwern{line-height:1;padding-left:0.25em;font-size:1.05em}#sidebar a.mail::before,#sidebar a.r_gwern::before{content:"°\2007";text-shadow:0 0 0 #000;position:relative;top:0.25em}#sidebar code{font-variant:none;text-transform:uppercase;font-size:0.7em}}@media only screen and (max-width:64.9ch){#sidebar{justify-content:center;margin:0 0 0.5em 0}#sidebar a{border:1px dotted #000;padding:3px 10px;text-align:center;margin:1px}#sidebar a#logo{padding:8px 5px 3px 5px}#sidebar,#sidebar-links{display:flex}#sidebar-links{flex-flow:row wrap;margin:0.5em 0 0 0}#sidebar a.site,#sidebar a.links{flex:1 1 20%}#sidebar a.new,#sidebar a.mail,#sidebar a.r_gwern{padding:3px 10px;flex:1 1 auto}#sidebar a.patreon{display:none}#sidebar #logo{margin:calc(0.5em + 1px) 1px 1px 0}#sidebar #logo img{width:2.5rem}}header{overflow:auto}header h1{margin:0.75em 0;text-align:center;text-transform:none;font-variant:small-caps;font-size:2.5em;line-height:1.15;font-weight:600;letter-spacing:-1px}@media only screen and (max-width:64.9ch){header h1{font-size:2em}}#page-metadata hr{display:none}#page-metadata{margin:0 0 2rem 0;overflow:auto;font-size:0.95em;line-height:1.5}#page-metadata #page-description{display:block;margin:0 auto 0.75em auto}#page-metadata #page-description~*{font-size:0.9em}#page-metadata #page-description+br{display:none}#page-metadata-block span:nth-of-type(n+3){white-space:nowrap}#page-metadata-block>span:not(:last-child)::after{content:"\00B7";display:inline-block;margin:0 0.5em 0 0.75em}#page-metadata-block>span:nth-child(1)::after{margin-left:0.875em}#page-metadata-block>span:nth-child(2)::after{margin-left:0.8125em}#TOC{border:1px solid #ccc;background-color:#f9f9f9;font-family:"Lucida Sans Unicode","Source Sans Pro",Helvetica,"Trebuchet MS",sans-serif;margin:0 2rem 1.5rem 0;line-height:1.25;padding:10px 10px 2px 14px;position:relative;z-index:11}#TOC:empty{display:none}@media only screen and (max-width:128ch){#TOC{font-size:0.95rem}}@media only screen and (max-width:120ch){#TOC{font-size:0.90rem}}@media only screen and (max-width:112ch){#TOC{font-size:0.85rem;margin:0 1.5rem 1.25rem 0}}@media only screen and (max-width:104ch){#TOC{font-size:0.80rem;margin:0 1.25rem 1rem 0}}@media only screen and (max-width:96ch){#TOC{margin:0 1rem 1rem 0}}@media only screen and (min-width:90ch){#TOC{float:left;max-width:30ch}}@media only screen and (max-width:90ch){#TOC{float:none;margin:2em auto;font-size:1rem}#TOC>ul>li>ul{column-count:2}}@media only screen and (max-width:64.9ch){#TOC a{display:inline-block}#TOC>ul>li>ul{column-count:1}#TOC li li a{padding:0 0 1px 0}#TOC li li li a{padding:0 0 2px 0}#TOC li li li li a{padding:0 0 3px 0}#TOC li li li li a{padding:0 0 4px 0}}#TOC ul{list-style-type:none;padding-left:0em;margin-bottom:0;margin-top:4px;padding-left:1.4em;text-indent:0;padding:0}#TOC ul ul{list-style-type:none;padding-left:0.7em;margin-top:2px}#TOC li{font-weight:bold;margin:5px 0 10px 0;padding-left:1.125em;position:relative;overflow-wrap:break-word}#TOC li li{margin-bottom:0;font-weight:normal;font-size:0.9em}#TOC p{margin-top:9px;margin-bottom:3px}#TOC a{border:0;display:block;position:relative}#TOC a:hover{background-color:rgba(0,0,0,0.05);color:#000}#TOC a:hover::after{content:"";display:inline-block;position:absolute;left:100%;top:0;background-color:#ccc;width:0.25em;height:100%}#TOC code{font-family:inherit;font-size:inherit;border:none;padding:0;background-color:inherit}#TOC>ul{counter-reset:htoc_1}#TOC>ul>li::before{counter-increment:htoc_1;content:counter(htoc_1) "\2006  "}#TOC>ul ul{counter-reset:htoc_2}#TOC>ul ul li::before{counter-increment:htoc_2;content:counter(htoc_1) "." counter(htoc_2) "\2006  "}#TOC>ul ul ul{counter-reset:htoc_3}#TOC>ul ul ul li::before{counter-increment:htoc_3;content:counter(htoc_1) "." counter(htoc_2) "." counter(htoc_3) "\2006  "}#TOC>ul ul ul ul{counter-reset:htoc_4}#TOC>ul ul ul ul li::before{counter-increment:htoc_4;content:counter(htoc_1) "." counter(htoc_2) "." counter(htoc_3) "." counter(htoc_4) "\2006  "}#TOC>ul ul ul ul ul{counter-reset:htoc_5}#TOC>ul ul ul ul ul li::before{counter-increment:htoc_5;content:counter(htoc_1) "." counter(htoc_2) "." counter(htoc_3) "." counter(htoc_4) "." counter(htoc_5) "\2006  "}#TOC>ul ul ul ul ul ul{counter-reset:htoc_6}#TOC>ul ul ul ul ul ul li::before{counter-increment:htoc_6;content:counter(htoc_1) "." counter(htoc_2) "." counter(htoc_3) "." counter(htoc_4) "." counter(htoc_5) "." counter(htoc_6) "\2006  "}#TOC ul li::before{position:absolute;right:calc(100% - 1em);width:12ch;text-align:right;font-weight:normal;opacity:0.4;pointer-events:none}#TOC ul li:hover::before{opacity:0.7}#markdownBody{overflow-wrap:break-word}@media only screen and (min-width:176.1ch){#markdownBody{position:relative}}@media only screen and (min-width:65ch){@media only screen and (max-width:100ch){#markdownBody{line-height:1.45}}@media only screen and (min-width:100.1ch) and (max-width:120ch){#markdownBody{line-height:1.5}}@media only screen and (min-width:120.1ch){#markdownBody{line-height:1.55}}}@supports (-webkit-hyphens:auto) or (-ms-hyphens:auto) or (hyphens:auto){#markdownBody p,#markdownBody li{text-align:justify;-webkit-hyphens:auto;-ms-hyphens:auto;hyphens:auto}}#markdownBody p{font-variant-numeric:oldstyle-nums}#abstract>blockquote{margin:0 0 1.5em 0;border-color:#bbb;padding:0.9rem 1.25rem 0.95rem 1.25rem;clear:none}#abstract+p::first-line,#markdownBody>p:first-child::first-line{font-feature-settings:'smcp'}h1{margin:1.25em 0 0.5em -0.75rem;font-weight:bold;position:relative}@media only screen and (max-width:65ch){h1{margin:1.25em 0 0.5em 0}}h1{font-feature-settings:'smcp';font-size:1.75em;line-height:1.25;letter-spacing:-0.75px}a{color:#3c3c3c;text-decoration:none}#markdownBody a{word-wrap:break-word}article>:not(#TOC) a:link{text-decoration:none;background-image:linear-gradient(#fff,#fff),linear-gradient(#fff,#fff),linear-gradient(#333,#333);background-size:0.05em 1px,0.05em 1px,1px 1px;background-repeat:no-repeat,no-repeat,repeat-x;background-position:0% 90%,100% 90%,0% 90%;text-shadow:0.03em 0 #fff,-0.03em 0 #fff,0 0.03em #fff,0 -0.03em #fff,0.06em 0 #fff,-0.06em 0 #fff,0.09em 0 #fff,-0.09em 0 #fff,0.12em 0 #fff,-0.12em 0 #fff,0.15em 0 #fff,-0.15em 0 #fff;font-variant-numeric:lining-nums}article>:not(#TOC) a:hover{background-image:linear-gradient(#fff,#fff),linear-gradient(#fff,#fff),linear-gradient(#999,#999)}p{margin:0}p+p{text-indent:2.5em}@media only screen and (max-width:64.9ch){p+p{text-indent:1em}}span.smallcaps{font-feature-settings:'smcp'}span.smallcaps-auto{font-feature-settings:'smcp','c2sc';font-variant-numeric:tabular-nums}hr{border:none;height:0;border-bottom:1px solid #aaa;margin:1em 0}</style>
<link rel="preload" as="style" href="/static/css/default.css">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-RegularItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-Semibold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Semibold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-SemiboldItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-RegularItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">

<style>
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 400;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Regular.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 400;
        font-style: italic;
        src: url('/static/font/SourceSerifPro-BASIC-RegularItalic.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap;
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 600;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Semibold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 700;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Bold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }

    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 400;
        font-style: normal;
        src: url('/static/font/SourceSansPro-BASIC-Regular.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 400;
        font-style: italic;
        src: url('/static/font/SourceSansPro-BASIC-RegularItalic.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 700;
        font-style: normal;
        src: url('/static/font/SourceSansPro-BASIC-Bold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    </style>

<script src="/static/js/sidenotes.js" async=""></script> 

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://cdnjs.cloudflare.com">
<meta name="title" content="GPT-2 Neural Network Poetry">
<meta name="citation_title" content="GPT-2 Neural Network Poetry">
<meta name="og:title" content="GPT-2 Neural Network Poetry">
<meta name="twitter:title" content="GPT-2 Neural Network Poetry">
<meta name="generator" content="hakyll">
<meta name="creator" content="gwern.net">
<meta name="author" content="Gwern Branwen">
<meta name="citation_author" content="Gwern Branwen">
<meta name="contact" content="https://www.gwern.net/Links#contact">
<link rel="index" title="Gwern.net" href="http://www.gwern.net/">
<meta name="twitter:creator" content="gwern">
<meta name="twitter:site" content="gwern.net">
<meta name="og:site" content="gwern.net">
<meta name="og:type" content="article">
<meta name="description" content="Demonstration tutorial of retraining OpenAI's GPT-2 (a text-generating Transformer neural network) on large poetry corpuses to generate high-quality English verse.">
<meta name="og:description" content="Demonstration tutorial of retraining OpenAI's GPT-2 (a text-generating Transformer neural network) on large poetry corpuses to generate high-quality English verse.">
<meta property="og:image" content="https://www.gwern.net/images/logo-whitebg-large-border.png">
<meta name="keywords" content="statistics, NN, fiction, shell, GPT, tutorial, poetry">
<meta name="dc.date.issued" content="3 March 2019">
<meta name="citation_publication_date" content="3 March 2019">
<meta name="dcterms.modified" content="29 Oct 2019">
<link rel="schema.dcterms" href="http://purl.org/dc/terms/">
<meta name="dcterms.rights" content="CC PD-0">
<meta name="dc.rights" content="https://creativecommons.org/publicdomain/zero/1.0/">
<link rel="canonical" href="https://www.gwern.net/GPT-2">
<meta name="citation_fulltext_html_url" content="https://www.gwern.net/GPT-2">
<meta name="og:url" content="https://www.gwern.net/GPT-2">
<meta name="page-source" content="https://www.gwern.net/GPT-2.page">
<meta name="citation_fulltext_world_readable" content="">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">
<title>GPT-2 Neural Network Poetry · Gwern.net</title>
<link href="https://old.reddit.com/r/gwern/search.rss?q=flair:'Gwern'&amp;sort=new&amp;restrict_sr=on" type="application/atom+xml" rel="alternate" title="ATOM/RSS feed of Gwern.net additions/miscellaneous writings">
<link rel="icon" type="image/x-icon" href="/static/img/favicon.png">
<link rel="apple-touch-icon" href="/images/logo-smooth-appletouch.png">
<style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-display: swap; font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-display: swap; font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-display: swap; font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-display: swap; font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-display: swap; font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-display: swap; font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-display: swap; font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-display: swap; font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-display: swap; font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-display: swap; font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-display: swap; font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-display: swap; font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-display: swap; font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-display: swap; font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-display: swap; font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-display: swap; font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-display: swap; font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-display: swap; font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-display: swap; font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-display: swap; font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-display: swap; font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head>
<body class="gpt drop-caps-kanzlei">
<main>
<div id="sidebar">
<a id="logo" rel="home me contents" href="/index" title="Homepage: categorized list of articles index">
<img alt="Gwern.net Site Logo (a Gothic/Fraktur blackletter capital G)" src="/images/logo-smooth.svg">
</a>
<div id="sidebar-links">
<a class="site" href="/About" title="Site ideals, source, content, traffic, examples, license" rel="author">Site</a>
<a class="links" href="/Links" title="Who am I online, what have I done, what am I like? Contact information; sites I use; things I've worked on" rel="author">Me</a>
<a class="new" href="/Changelog" title="Changelog of what's new or updated">New:</a>
<a class="mail" href="https://gwern.substack.com/" title="Monthly mailing list: newsletter signup form" rel="me"><i>mail</i></a>
<a class="r_gwern" href="https://old.reddit.com/r/gwern/" title="Gwern subreddit: link-sharing &amp; commentary (the links typically are included in the monthly newsletter)" rel="me"><em><code>/r/gwern</code></em></a>
<a class="patreon" href="https://www.patreon.com/gwern" title="Link to Patreon donation profile to support my writing" rel="me">support on<br><span>PATREON</span></a>
</div>
</div>
<header>
<h1>GPT-2 Neural Network Poetry</h1>
</header>
<article>
<div id="page-metadata">
<p>
<span id="page-description" title="Short description of page: longer than title, but shorter than abstract."><em>Demonstration tutorial of retraining OpenAI's GPT-2 (a text-generating Transformer neural network) on large poetry corpuses to generate high-quality English verse.</em></span>
<br>
<span id="page-tags" title="List of tags for this page."><em><a title="All pages tagged 'statistics'." href="/tags/statistics">statistics</a>, <a title="All pages tagged 'NN'." href="/tags/NN">NN</a>, <a title="All pages tagged 'fiction'." href="/tags/fiction">fiction</a>, <a title="All pages tagged 'shell'." href="/tags/shell">shell</a>, <a title="All pages tagged 'GPT'." href="/tags/GPT">GPT</a>, <a title="All pages tagged 'tutorial'." href="/tags/tutorial">tutorial</a>, <a title="All pages tagged 'poetry'." href="/tags/poetry">poetry</a></em></span>
<br>
<span id="page-metadata-block">
<span id="page-date-range"><span id="page-creation" title="Date page contents were begun."><em>3 March 2019</em></span>–<span id="page-source"><a title="Link to latest revision of the raw Markdown text source for this page, 'GPT-2 Neural Network Poetry': /GPT-2.page" href="/GPT-2.page"><span id="page-modified" title="Date of last major modification to this page."><em>29 Oct 2019</em></span></a></span></span>
<span id="page-status" title="Writing status of current page: ranges 'abandoned'/'notes'/'draft'/'in progress'/'finished'"><em>finished</em></span>
<span id="page-confidence"><a href="/About#confidence-tags" title="Explanation of 'confidence' metadata: probability of overall being meaningfully correct, expressed as Kesselman Estimative Words (ranging 0–100%: 'certain'/'highly likely'/'likely'/'possible'/'unlikely'/'highly unlikely'/'remote'/'impossible')">certainty</a>: <em>likely</em></span>
<span id="page-importance"><a href="/About#importance-tags" title="Explanation of 'importance' metadata: rating 1–10 about how much a topic matters to the world.">importance</a>: <em>7</em></span></span>
</p><hr>
</div>
<noscript><div class="admonition warning"><div class="admonition-title">JS Disabled</div> For support of <a href="/About#design" title="About: Gwern.net Design: principles, features, links, tricks">website features</a> (table-sorting/collapsible-sections/image-zooms/link-annotations/floating-footnotes/Disqus-comments), please enable JavaScript.</div></noscript>
<div id="TOC"><ul>
<li><a href="#gpt-2-117m-generating-poetry"><span><span class="smallcaps-auto">GPT-2-117M</span>: Generating Poetry</span></a></li>
<li><a href="#training-gpt-2-117m-to-generate-poetry"><span>Training <span class="smallcaps-auto">GPT-2-117M</span> To Generate Poetry</span></a>
<ul>
<li><a href="#data-the-project-gutenberg-poetry-corpus"><span>Data: The Project Gutenberg Poetry Corpus</span></a></li>
</ul></li>
<li><a href="#training-gpt-2-poetry"><span>Training <code>GPT-2-poetry</code></span></a>
<ul>
<li><a href="#gpt-2-poetry-samples"><span><code>GPT-2-poetry</code> Samples</span></a></li>
<li><a href="#cleaning-project-gutenberg-contemporary-poetry"><span>Cleaning Project Gutenberg &amp; Contemporary Poetry</span></a></li>
</ul></li>
<li><a href="#training-gpt-2-poetry-prefix"><span>Training <code>GPT-2-poetry-prefix</code></span></a>
<ul>
<li><a href="#gpt-2-poetry-prefix-samples"><span><code>GPT-2-poetry-prefix</code> Samples</span></a>
<ul>
<li><a href="#training-samples"><span>Training samples</span></a></li>
<li><a href="#unconditional-samples"><span>Unconditional samples</span></a></li>
</ul></li>
<li><a href="#gpt-2-poetry-prefix-completions"><span><code>GPT-2-poetry-prefix</code> completions</span></a>
<ul>
<li><a href="#howl"><span>“Howl”</span></a></li>
<li><a href="#ozymandias"><span>“Ozymandias”</span></a></li>
<li><a href="#essay-on-criticism"><span><em>Essay on Criticism</em></span></a></li>
<li><a href="#famous-first-lines"><span>8 Famous First Lines</span></a>
<ul>
<li><a href="#ulysses-lord-alfred-tennyson"><span>“Ulysses”, Lord Alfred Tennyson</span></a></li>
<li><a href="#sailing-to-byzantium-yeats"><span>“Sailing to Byzantium”, Yeats</span></a></li>
<li><a href="#sonnet-29-shakespeare"><span>Sonnet #29, Shakespeare</span></a></li>
<li><a href="#invictus-william-ernest-henley"><span>“Invictus”, William Ernest Henley</span></a></li>
<li><a href="#pioneers-o-pioneers-walt-whitman"><span>“Pioneers! O Pioneers!”, Walt Whitman</span></a></li>
<li><a href="#the-love-song-of-j.-alfred-prufrock-t.s.-eliot"><span>“The Love Song of J. Alfred Prufrock”, T.S. Eliot</span></a></li>
<li><a href="#hamlet-william-shakespeare"><span><em>Hamlet</em>, William Shakespeare</span></a></li>
<li><a href="#romeo-juliet-william-shakespeare"><span><em>Romeo &amp; Juliet</em>, William Shakespeare</span></a></li>
</ul></li>
<li><a href="#jabberwocky-lewis-carroll"><span>“Jabberwocky”, Lewis Carroll</span></a></li>
</ul></li>
<li><a href="#gpt-2-345m"><span><span class="smallcaps-auto">GPT-2-345M</span></span></a>
<ul>
<li><a href="#training"><span>Training</span></a></li>
<li><a href="#samples"><span>Samples</span></a>
<ul>
<li><a href="#training-samples-1"><span>Training samples</span></a></li>
<li><a href="#random-samples"><span>Random Samples</span></a></li>
</ul></li>
<li><a href="#tao-te-ching"><span><em>Tao Te Ching</em></span></a></li>
</ul></li>
</ul></li>
<li><a href="#gpt-2-15b"><span><span class="smallcaps-auto">GPT-2-1</span>.5b</span></a>
<ul>
<li><a href="#b-training"><span>1.5b Training</span></a>
<ul>
<li><a href="#gpu-failures"><span><span class="smallcaps-auto">GPU</span> Failures</span></a></li>
<li><a href="#google-colab"><span>Google Colab</span></a></li>
<li><a href="#gcp"><span><span class="smallcaps-auto">GCP</span></span></a></li>
<li><a href="#b-hyperparameters"><span>1.5b Hyperparameters</span></a></li>
</ul></li>
<li><a href="#b-samples"><span>1.5b Samples</span></a>
<ul>
<li><a href="#loss-2-6"><span>Loss: 2.6</span></a></li>
<li><a href="#loss-1-6"><span>Loss: 1.6</span></a></li>
<li><a href="#loss-1-3"><span>Loss: 1.3</span></a></li>
</ul></li>
</ul></li>
<li><a href="#overall"><span>Overall</span></a></li>
<li><a href="#improvements"><span>Improvements</span></a></li>
<li><a href="#external-links"><span>External Links</span></a></li>
<li><a href="#appendix"><span>Appendix</span></a>
<ul>
<li><a href="#archive-of-our-own-ao3-gpt-2-1-5b"><span>Archive of Our Own (Ao3) <span class="smallcaps-auto">GPT-2-1</span>.5b</span></a></li>
<li><a href="#video-game-walkthrough-gpt-2-1-5b"><span>Video Game Walkthrough <span class="smallcaps-auto">GPT-2-1</span>.5b</span></a></li>
<li><a href="#rdota2"><span>/r/DoTA2</span></a></li>
<li><a href="#bradley-terry-preference-learning"><span>Bradley-Terry Preference Learning</span></a></li>
<li><a href="#efficient-attention"><span>Efficient Attention</span></a></li>
</ul></li>
</ul></div>
<div id="markdownBody"><div id="abstract">
<blockquote>
<p>In February 2019, following up on my <a href="/RNN-metadata" class="docMetadata" data-popup-title="RNN metadata for mimicking individual author style" data-popup-author="Gwern Branwen" data-popup-date="12 Sep 2015" data-popup-abstract="<p>Char-<span class=&quot;smallcaps-auto&quot;>RNN</span>s are unsupervised generative models which learn to mimic text sequences. I suggest extending char-<span class=&quot;smallcaps-auto&quot;>RNN</span>s with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. A 2015 experiment using <code>torch-rnn</code> on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-<span class=&quot;smallcaps-auto&quot;>RNN</span> shows that a char-<span class=&quot;smallcaps-auto&quot;>RNN</span> can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.</p><p>I further try &amp;amp; fail to train <a href=&quot;/RNN-metadata#geocities-char-rnn&quot;>a char-<span class=&quot;smallcaps-auto&quot;>RNN</span> on Geocities <span class=&quot;smallcaps-auto&quot;>HTML</span></a> for unclear reasons.</p><p>More successfully, <a href=&quot;/GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my &amp;lt;a href=&amp;quot;/RNN-metadata&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;RNN metadata for mimicking individual author style&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;12 Sep 2015&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s are unsupervised generative models which learn to mimic text sequences. I suggest extending char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp;amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. A 2015 experiment using &amp;amp;lt;code&amp;amp;gt;torch-rnn&amp;amp;lt;/code&amp;amp;gt; on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; shows that a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I further try &amp;amp;amp;amp; fail to train &amp;amp;lt;a href=&amp;amp;quot;#geocities-char-rnn&amp;amp;quot;&amp;amp;gt;a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; on Geocities &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;HTML&amp;amp;lt;/span&amp;amp;gt;&amp;amp;lt;/a&amp;amp;gt; for unclear reasons.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;More successfully, &amp;amp;lt;a href=&amp;amp;quot;/GPT-2&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;3 March 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “GPT-2-poetry”, trained on the poems as a continuous stream of text, and “GPT-2-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;English Wikipedia&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;b&amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;lt;i&amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;quot;&amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;lt;/a&amp;amp;amp;gt;’s &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-image-height=&amp;amp;amp;quot;768&amp;amp;amp;quot; data-popup-image-width=&amp;amp;amp;quot;768&amp;amp;amp;quot;&amp;amp;amp;gt;website&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;For generating ABC-formatted folk music, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Music&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;quot;&amp;amp;amp;gt;“GPT-2 Folk Music”&amp;amp;amp;lt;/a&amp;amp;amp;gt;&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;I experiment in 2019 with a recently-developed alternative to char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s&amp;amp;lt;/a&amp;amp;gt;, the Transformer NN architecture, by finetuning training OpenAI’s &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-117M&amp;amp;lt;/span&amp;amp;gt; Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp;amp;amp; lines with inline metadata (the source book). The generated poetry is much better.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;2015–2016 text-generation experiments with char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt;s&amp;lt;/a&amp;gt;, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAI’s &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;-poetry”, trained on the poems as a continuous stream of text, and “&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Poetry Foundation&amp;quot; data-popup-author=&amp;quot;English Wikipedia&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;&amp;amp;lt;b&amp;amp;gt;The Poetry Foundation&amp;amp;lt;/b&amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;lt;i&amp;amp;gt;Poetry&amp;amp;lt;/i&amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Wikipedia: Poetry Foundation&amp;quot;&amp;gt;Poetry Foundation&amp;lt;/a&amp;gt;’s &amp;lt;a href=&amp;quot;https://www.poetryfoundation.org/poems&amp;quot;&amp;gt;website&amp;lt;/a&amp;gt;. .&amp;amp;gt; With just a few &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPU&amp;lt;/span&amp;gt;-days on 1080ti &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPU&amp;lt;/span&amp;gt;s, &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; finetuning can produce high-quality poetry which is more thematically consistent than my char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems. For the highest-quality AI poetry to date, see my followup page, &amp;lt;a href=&amp;quot;https://www.gwern.net/GPT-3&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-3 Creative Fiction&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;19 June 2020&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I continue my AI poetry generation experiments with OpenAI’s &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; successor, &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;, which is 116✕ larger, and much more powerful. &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;, however, is not merely a quantitative tweak yielding “&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; but better”—it is qualitatively different, exhibiting eerie runtime learning capabilities allowing even the raw model, with zero finetuning, to tackle almost any imaginable textual task purely by example or instruction. One does not train or program &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; in a normal way, but one engages in dialogue and writes prompts to teach &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; what one wants.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Experimenting through the OpenAI Beta &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;API&amp;amp;lt;/span&amp;amp;gt; in June 2020, I find that &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; does not just match my &amp;amp;lt;a href=&amp;amp;quot;/GPT-2#gpt-2-1.5b&amp;amp;quot;&amp;amp;gt;finetuned &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-1&amp;amp;lt;/span&amp;amp;gt;.5b-poetry&amp;amp;lt;/a&amp;amp;gt; for poem-writing quality, but exceeds it, while being versatile in handling &amp;amp;lt;a href=&amp;amp;quot;#poetry&amp;amp;quot;&amp;amp;gt;poetry&amp;amp;lt;/a&amp;amp;gt;, &amp;amp;lt;a href=&amp;amp;quot;#tom-swifties&amp;amp;quot;&amp;amp;gt;Tom Swifty puns&amp;amp;lt;/a&amp;amp;gt;, science fiction, dialogue like Turing’s &amp;amp;lt;a href=&amp;amp;quot;#turing-dialogue&amp;amp;quot;&amp;amp;gt;Turing-test dialogue&amp;amp;lt;/a&amp;amp;gt;, &amp;amp;lt;a href=&amp;amp;quot;#literary-parodies&amp;amp;quot;&amp;amp;gt;literary style parodies&amp;amp;lt;/a&amp;amp;gt;… As the &amp;amp;lt;em&amp;amp;gt;pièce de résistance&amp;amp;lt;/em&amp;amp;gt;, I recreate Stanislaw Lem’s &amp;amp;lt;em&amp;amp;gt;Cyberiad&amp;amp;lt;/em&amp;amp;gt;’s &amp;amp;lt;a href=&amp;amp;quot;#stanislaw-lems-cyberiad&amp;amp;quot;&amp;amp;gt;“Trurl’s Electronic Bard”&amp;amp;lt;/a&amp;amp;gt; poetry using &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;. (Along the way, I document instances of how the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;BPE&amp;amp;lt;/span&amp;amp;gt; text encoding &amp;amp;lt;a href=&amp;amp;quot;#bpes&amp;amp;quot;&amp;amp;gt;unnecessarily damages&amp;amp;lt;/a&amp;amp;gt; &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;’s performance on a variety of tasks.)&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;’s samples are not just close to human level: they are creative, witty, deep, meta, and often beautiful. They demonstrate an ability to handle abstractions, like style parodies, I have not seen in &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; at all. Chatting with &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; feels uncannily like chatting with a human. I was impressed by the results reported in the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; paper, and after spending a week trying it out, I remain impressed.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;This page records &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; samples I generated in my explorations, and thoughts on &amp;amp;lt;a href=&amp;amp;quot;#prompts-as-programming&amp;amp;quot;&amp;amp;gt;how to use &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;&amp;amp;lt;/a&amp;amp;gt; and its remaining &amp;amp;lt;a href=&amp;amp;quot;#weaknesses&amp;amp;quot;&amp;amp;gt;weaknesses&amp;amp;lt;/a&amp;amp;gt;. I hope you enjoy them even a tenth as much as I enjoyed testing &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; and watching the completions scroll across my screen.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Creative writing by OpenAI&amp;amp;#39;s GPT-3 model, demonstrating poetry, dialogue, puns, literary parodies, and storytelling.&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; Creative Writing”&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;For anime plot summaries, see &amp;lt;a href=&amp;quot;/TWDE#text&amp;quot;&amp;gt;&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TWDNE&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;; for generating &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;-formatted folk music, see &amp;lt;a href=&amp;quot;/GPT-2-music&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Folk Music&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;1 Nov 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;In November 2019, I experimented with training a &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; hypothetically can improve on an &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; by better global coherence &amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I encountered problems with the standard &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;lt;a href=&amp;amp;quot;#spaceless-model&amp;amp;quot;&amp;amp;gt;fixing that&amp;amp;lt;/a&amp;amp;gt;, I successfully trained it on &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;=205,304 &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp; &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;The &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; folk model &amp;amp;amp;amp; dataset are &amp;amp;lt;a href=&amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;quot;&amp;amp;gt;available for download&amp;amp;lt;/a&amp;amp;gt;, and I provide for listening selected &amp;amp;lt;a href=&amp;amp;quot;#samples&amp;amp;quot;&amp;amp;gt;music samples&amp;amp;lt;/a&amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;We followed the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; folk model with &amp;amp;lt;a href=&amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;quot;&amp;amp;gt;an &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt;-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; model&amp;amp;lt;/a&amp;amp;gt;: a &amp;amp;lt;a href=&amp;amp;quot;#midi-dataset&amp;amp;quot;&amp;amp;gt;dataset of 453k &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; pieces&amp;amp;lt;/a&amp;amp;gt; decompiled from &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; pieces, which fit into &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-117M&amp;amp;lt;/span&amp;amp;gt; with an expanded context window when trained on &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;TPU&amp;amp;lt;/span&amp;amp;gt;s. The &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;lt;a href=&amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;quot;&amp;amp;gt;even better musical samples&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; Folk Music”&amp;lt;/a&amp;gt; &amp;amp;amp; &amp;lt;a href=&amp;quot;/GPT-2-preference-learning&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Preference Learning for Music and Poetry Generation&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;16 Dec 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Standard language generation neural network models, like &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt;, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp;amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. &amp;amp;lt;em&amp;amp;gt;Preference learning&amp;amp;lt;/em&amp;amp;gt; trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;DRL&amp;amp;lt;/span&amp;amp;gt; algorithm like &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;PPO&amp;amp;lt;/span&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt;. Having previously used &amp;amp;lt;a href=&amp;amp;quot;/GPT-2&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;3 March 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In February 2019, following up on my &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/RNN-metadata&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;RNN metadata for mimicking individual author style&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;12 Sep 2015&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Char-RNNs are unsupervised generative models which learn to mimic text sequences. I suggest extending char-RNNs with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp;amp;amp;amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. A 2015 experiment using &amp;amp;amp;amp;lt;code&amp;amp;amp;amp;gt;torch-rnn&amp;amp;amp;amp;lt;/code&amp;amp;amp;amp;gt; on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-RNN shows that a char-RNN can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I further try &amp;amp;amp;amp;amp;amp; fail to train &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#geocities-char-rnn&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;a char-RNN on Geocities HTML&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; for unclear reasons.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;More successfully, &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;gwern&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;3 March 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: &amp;amp;amp;amp;amp;lt;q&amp;amp;amp;amp;amp;gt;“GPT-2-poetry”&amp;amp;amp;amp;amp;lt;/q&amp;amp;amp;amp;amp;gt;, trained on the poems as a continuous stream of text, and &amp;amp;amp;amp;amp;lt;q&amp;amp;amp;amp;amp;gt;“GPT-2-poetry-prefix”&amp;amp;amp;amp;amp;lt;/q&amp;amp;amp;amp;amp;gt;, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;I experiment in 2019 with a recently-developed alternative to char-RNNs&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, the Transformer NN architecture, by finetuning training OpenAI’s GPT-2-117M Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp;amp;amp;amp;amp; lines with inline metadata (the source book). The generated poetry is much better.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot;&amp;amp;amp;gt;2015–2016 text-generation experiments with char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt;s&amp;amp;amp;lt;/a&amp;amp;amp;gt;, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp; text generation. Using OpenAI’s &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;-poetry”, trained on the poems as a continuous stream of text, and “&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;English Wikipedia&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;b&amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;lt;i&amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;quot;&amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;lt;/a&amp;amp;amp;gt;’s &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;quot;&amp;amp;amp;gt;website&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;With just a few &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;-days on 1080ti &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;s, &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; finetuning can produce high-quality poetry which is more thematically consistent than my char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;For anime plot summaries, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/TWDE#text&amp;amp;amp;quot;&amp;amp;amp;gt;&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;TWDNE&amp;amp;amp;lt;/span&amp;amp;amp;gt;&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for generating &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;-formatted folk music, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Folk Music&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;In November 2019, I experimented with training a &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;RNN&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; hypothetically can improve on an &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;RNN&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; by better global coherence &amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I encountered problems with the standard &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#spaceless-model&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;fixing that&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;=205,304 &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp;amp;amp; &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; folk model &amp;amp;amp;amp;amp;amp; dataset are &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;available for download&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, and I provide for listening selected &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#samples&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;music samples&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;We followed the &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; folk model with &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;an &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC-MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; model&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;: a &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#midi-dataset&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;dataset of 453k &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; decompiled from &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces, which fit into &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; with an expanded context window when trained on &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;TPU&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;s. The &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;even better musical samples&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; Folk Music”&amp;amp;amp;lt;/a&amp;amp;amp;gt; &amp;amp;amp;amp;amp; &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-preference-learning&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Preference Learning for Music and Poetry Generation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;16 Dec 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Standard language generation neural network models, like &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp;amp;amp;amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;Preference learning&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;DRL&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; algorithm like &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;PPO&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;. Having previously used &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;3 March 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “GPT-2-poetry”, trained on the poems as a continuous stream of text, and “GPT-2-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;amp;quot;English Wikipedia&amp;amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;b&amp;amp;amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;amp;amp;lt;i&amp;amp;amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;quot; title=&amp;amp;amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;’s &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-image-height=&amp;amp;amp;amp;amp;quot;768&amp;amp;amp;amp;amp;quot; data-popup-image-width=&amp;amp;amp;amp;amp;quot;768&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;website&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;For generating ABC-formatted folk music, see &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;amp;quot;GPT-2 Music&amp;amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;quot; title=&amp;amp;amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;“GPT-2 Folk Music”&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; for poetry&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp; &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Folk Music&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;#spaceless-model&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;fixing that&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;amp;amp;lt;em&amp;amp;amp;amp;amp;gt;n&amp;amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;amp;gt;=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;#samples&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;music samples&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;music generation&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, I experimented with &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; preference learning for unconditional music and poetry generation.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;NLP&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp;amp;amp;amp;amp; succumb to adversarial examples easily.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp;amp;amp;amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#bradley-terry-preference-learning&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;directly model the implied ranking&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#optimization-by-backprop-not-blackbox&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;optimizes sequences to maximize the reward model’s output&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; Preference Learning for Music and Poetry Generation”&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for playing chess, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;A Very Unlikely Chess Game&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Scott Alexander&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-01-06&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;within&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://slatestarcodex.com/2019/02/28/meaningful/&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;meaningful&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Scott Alexander&amp;amp;amp;quot;&amp;amp;amp;gt;“A Very Unlikely Chess Game”&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for the Reddit comment generator, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;disumbrationist&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-01-12&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://openai.com/blog/gpt-2-1-5b-release/&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;finally released the full 1.5 billion parameter model&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://old.reddit.com/u/gwern&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;/u/gwern&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; (&amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://www.patreon.com/gwern&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;here’s his Patreon&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;) and Shawn Presser (&amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://old.reddit.com/u/shawwwn&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;/u/shawwwn&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://twitter.com/gwern/status/1215005375407112193&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;around $70K worth of TPU credits&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;all&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; rel=&amp;amp;amp;quot;archived alternate nofollow&amp;amp;amp;quot; data-url-original=&amp;amp;amp;quot;https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/&amp;amp;amp;quot; title=&amp;amp;amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )&amp;amp;amp;quot;&amp;amp;amp;gt;SubSimulator&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for fanfiction, the &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#archive-of-our-own-ao3-gpt-2-1.5b&amp;amp;amp;quot;&amp;amp;amp;gt;Ao3&amp;amp;amp;lt;/a&amp;amp;amp;gt;; and for video games, &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#video-game-walkthrough-gpt-2-1.5b&amp;amp;amp;quot;&amp;amp;amp;gt;the walkthrough model&amp;amp;amp;lt;/a&amp;amp;amp;gt;. For OpenAI’s &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-3&amp;amp;amp;lt;/span&amp;amp;amp;gt; followup, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://arxiv.org/abs/2005.14165#openai&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Language Models are Few-Shot Learners&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al [...]&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-05-28&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&amp;amp;amp;amp;#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.&amp;amp;amp;quot; title=&amp;amp;amp;quot;Brown et al 2020&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-3&amp;amp;amp;lt;/span&amp;amp;amp;gt;: Language Models are Few-Shot Learners”&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; for poetry&amp;amp;lt;/a&amp;amp;gt; &amp;amp;amp;amp; &amp;amp;lt;a href=&amp;amp;quot;/GPT-2-music&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Folk Music&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;1 Nov 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In November 2019, I experimented with training a &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; hypothetically can improve on an &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; by better global coherence &amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;I encountered problems with the standard &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#spaceless-model&amp;amp;amp;quot;&amp;amp;amp;gt;fixing that&amp;amp;amp;lt;/a&amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;lt;em&amp;amp;amp;gt;n&amp;amp;amp;lt;/em&amp;amp;amp;gt;=205,304 &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp;amp; &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;The &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; folk model &amp;amp;amp;amp;amp; dataset are &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;amp;quot;&amp;amp;amp;gt;available for download&amp;amp;amp;lt;/a&amp;amp;amp;gt;, and I provide for listening selected &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#samples&amp;amp;amp;quot;&amp;amp;amp;gt;music samples&amp;amp;amp;lt;/a&amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;We followed the &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; folk model with &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;amp;quot;&amp;amp;amp;gt;an &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; model&amp;amp;amp;lt;/a&amp;amp;amp;gt;: a &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#midi-dataset&amp;amp;amp;quot;&amp;amp;amp;gt;dataset of 453k &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces&amp;amp;amp;lt;/a&amp;amp;amp;gt; decompiled from &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces, which fit into &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; with an expanded context window when trained on &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;TPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;s. The &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;amp;quot;&amp;amp;amp;gt;even better musical samples&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;music generation&amp;amp;lt;/a&amp;amp;gt;, I experimented with &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; preference learning for unconditional music and poetry generation.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp;amp;amp; succumb to adversarial examples easily.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp;amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model &amp;amp;lt;a href=&amp;amp;quot;#bradley-terry-preference-learning&amp;amp;quot;&amp;amp;gt;directly model the implied ranking&amp;amp;lt;/a&amp;amp;gt; of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which &amp;amp;lt;a href=&amp;amp;quot;#optimization-by-backprop-not-blackbox&amp;amp;quot;&amp;amp;gt;optimizes sequences to maximize the reward model’s output&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; Preference Learning for Music and Poetry Generation”&amp;lt;/a&amp;gt;; for playing chess, see &amp;lt;a href=&amp;quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;A Very Unlikely Chess Game&amp;quot; data-popup-author=&amp;quot;Scott Alexander&amp;quot; data-popup-date=&amp;quot;2020-01-06&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running &amp;amp;lt;em&amp;amp;gt;within&amp;amp;lt;/em&amp;amp;gt; a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this &amp;amp;lt;a href=&amp;amp;quot;https://slatestarcodex.com/2019/02/28/meaningful/&amp;amp;quot;&amp;amp;gt;meaningful&amp;amp;lt;/a&amp;amp;gt;? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Scott Alexander&amp;quot;&amp;gt;“A Very Unlikely Chess Game”&amp;lt;/a&amp;gt;; for the Reddit comment generator, see &amp;lt;a href=&amp;quot;/docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots&amp;quot; data-popup-author=&amp;quot;disumbrationist&amp;quot; data-popup-date=&amp;quot;2020-01-12&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI &amp;amp;lt;a href=&amp;amp;quot;https://openai.com/blog/gpt-2-1-5b-release/&amp;amp;quot;&amp;amp;gt;finally released the full 1.5 billion parameter model&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by &amp;amp;lt;a href=&amp;amp;quot;https://old.reddit.com/u/gwern&amp;amp;quot;&amp;amp;gt;/u/gwern&amp;amp;lt;/a&amp;amp;gt; (&amp;amp;lt;a href=&amp;amp;quot;https://www.patreon.com/gwern&amp;amp;quot;&amp;amp;gt;here’s his Patreon&amp;amp;lt;/a&amp;amp;gt;) and Shawn Presser (&amp;amp;lt;a href=&amp;amp;quot;https://old.reddit.com/u/shawwwn&amp;amp;quot;&amp;amp;gt;/u/shawwwn&amp;amp;lt;/a&amp;amp;gt;), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required &amp;amp;lt;a href=&amp;amp;quot;https://twitter.com/gwern/status/1215005375407112193&amp;amp;quot;&amp;amp;gt;around $70K worth of TPU credits&amp;amp;lt;/a&amp;amp;gt;, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from &amp;amp;lt;em&amp;amp;gt;all&amp;amp;lt;/em&amp;amp;gt; the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen&amp;amp;lt;/p&amp;amp;gt;&amp;quot; rel=&amp;quot;archived alternate nofollow&amp;quot; data-url-original=&amp;quot;https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/&amp;quot; title=&amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )&amp;quot;&amp;gt;SubSimulator&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;; for fanfiction, the &amp;lt;a href=&amp;quot;#archive-of-our-own-ao3-gpt-2-1.5b&amp;quot;&amp;gt;Ao3&amp;lt;/a&amp;gt;; and for video games, &amp;lt;a href=&amp;quot;#video-game-walkthrough-gpt-2-1.5b&amp;quot;&amp;gt;the walkthrough model&amp;lt;/a&amp;gt;. For OpenAI’s &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; followup, see &amp;lt;a href=&amp;quot;https://arxiv.org/abs/2005.14165#openai&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Language Models are Few-Shot Learners&amp;quot; data-popup-author=&amp;quot;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al [...]&amp;quot; data-popup-date=&amp;quot;2020-05-28&amp;quot; data-popup-abstract=&amp;quot;Recent work has demonstrated substantial gains on many &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 achieves strong performance on many &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3&amp;amp;#39;s few-shot learning still struggles, as well as some datasets where &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 faces methodological issues related to training on large web corpora. Finally, we find that &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 in general.&amp;quot; title=&amp;quot;Brown et al 2020&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;: Language Models are Few-Shot Learners”&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&quot;>I experiment in 2019 with a recently-developed alternative to char-<span class=&quot;smallcaps-auto&quot;>RNN</span>s</a>, the Transformer NN architecture, by finetuning training OpenAI’s <span class=&quot;smallcaps-auto&quot;>GPT-2-117M</span> Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp; lines with inline metadata (the source book). The generated poetry is much better. And <a href=&quot;/GPT-3&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-3 Creative Fiction&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;19 June 2020&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I continue my AI poetry generation experiments with OpenAI’s 2020 &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;, which is 116✕ larger, and much more powerful, than the 2019 &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;. &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;, however, is not merely a quantitative tweak yielding “&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; but better”—it is qualitatively different, exhibiting eerie runtime learning capabilities allowing even the raw model, with zero finetuning, to “meta-learn” many textual tasks purely by example or instruction. One does not train or program &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; in a normal way, but one engages in dialogue and writes prompts to teach &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; what one wants.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Experimenting through the OpenAI Beta &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;API&amp;lt;/span&amp;gt; in June 2020, I find that &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; does not just match my &amp;lt;a href=&amp;quot;/GPT-2#gpt-2-1.5b&amp;quot;&amp;gt;finetuned &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-1&amp;lt;/span&amp;gt;.5b-poetry&amp;lt;/a&amp;gt; for poem-writing quality, but exceeds it, while being versatile in handling &amp;lt;a href=&amp;quot;#poetry&amp;quot;&amp;gt;poetry&amp;lt;/a&amp;gt;, &amp;lt;a href=&amp;quot;#tom-swifties&amp;quot;&amp;gt;Tom Swifty puns&amp;lt;/a&amp;gt;, science fiction, dialogue like Turing’s &amp;lt;a href=&amp;quot;#turing-dialogue&amp;quot;&amp;gt;Turing-test dialogue&amp;lt;/a&amp;gt;, &amp;lt;a href=&amp;quot;#literary-parodies&amp;quot;&amp;gt;literary style parodies&amp;lt;/a&amp;gt;… As the &amp;lt;em&amp;gt;pièce de résistance&amp;lt;/em&amp;gt;, I recreate Stanislaw Lem’s &amp;lt;em&amp;gt;Cyberiad&amp;lt;/em&amp;gt;’s &amp;lt;a href=&amp;quot;#stanislaw-lems-cyberiad&amp;quot;&amp;gt;“Trurl’s Electronic Bard”&amp;lt;/a&amp;gt; poetry using &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;. (Along the way, I document instances of how the &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;BPE&amp;lt;/span&amp;gt; text encoding &amp;lt;a href=&amp;quot;#bpes&amp;quot;&amp;gt;unnecessarily damages&amp;lt;/a&amp;gt; &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;’s performance on a variety of tasks.)&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;’s samples are not just close to human level: they are creative, witty, deep, meta, and often beautiful. They demonstrate an ability to handle abstractions, like style parodies, I have not seen in &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; at all. Chatting with &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; feels uncannily like chatting with a human. I was impressed by the results reported in the &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; paper, and after spending a week trying it out, I remain impressed.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This page records &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; samples I generated in my explorations, and thoughts on &amp;lt;a href=&amp;quot;#prompts-as-programming&amp;quot;&amp;gt;how to use &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt; and its remaining &amp;lt;a href=&amp;quot;#weaknesses&amp;quot;&amp;gt;weaknesses&amp;lt;/a&amp;gt;. I hope you enjoy them even a tenth as much as I enjoyed testing &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; and watching the completions scroll across my screen.&amp;lt;/p&amp;gt;&quot;><span class=&quot;smallcaps-auto&quot;>GPT-3</span></a> is better still.</p>">2015–2016 text-generation experiments with char-<span class="smallcaps-auto">RNN</span>s</a>, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp; text generation. Using OpenAI’s <span class="smallcaps-auto">GPT-2-117M</span> (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain <span class="smallcaps-auto">GPT-2-117M</span> on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “<span class="smallcaps-auto">GPT-2</span>-poetry”, trained on the poems as a continuous stream of text, and “<span class="smallcaps-auto">GPT-2</span>-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest <span class="smallcaps-auto">GPT-2</span>, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on <a href="https://en.wikipedia.org/wiki/Poetry_Foundation" class="docMetadata" data-popup-title="Poetry Foundation" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>The Poetry Foundation</b> is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from <i>Poetry</i> magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.</p>" title="Wikipedia: Poetry Foundation">Poetry Foundation</a>’s <a href="https://www.poetryfoundation.org/poems">website</a>. .&gt; With just a few <span class="smallcaps-auto">GPU</span>-days on 1080ti <span class="smallcaps-auto">GPU</span>s, <span class="smallcaps-auto">GPT-2-117M</span> finetuning can produce high-quality poetry which is more thematically consistent than my char-<span class="smallcaps-auto">RNN</span> poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems. For the highest-quality AI poetry to date, see my followup page, <a href="https://www.gwern.net/GPT-3" class="docMetadata" data-popup-title="GPT-3 Creative Fiction" data-popup-author="Gwern Branwen" data-popup-date="19 June 2020" data-popup-abstract="<p>I continue my AI poetry generation experiments with OpenAI’s 2020 <span class=&quot;smallcaps-auto&quot;>GPT-3</span>, which is 116× larger, and much more powerful, than the 2019 <span class=&quot;smallcaps-auto&quot;>GPT-2</span>. <span class=&quot;smallcaps-auto&quot;>GPT-3</span>, however, is not merely a quantitative tweak yielding “<span class=&quot;smallcaps-auto&quot;>GPT-2</span> but better”—it is qualitatively different, exhibiting eerie runtime learning capabilities allowing even the raw model, with zero finetuning, to “meta-learn” many textual tasks purely by example or instruction. One does not train or program <span class=&quot;smallcaps-auto&quot;>GPT-3</span> in a normal way, but one engages in dialogue and writes prompts to teach <span class=&quot;smallcaps-auto&quot;>GPT-3</span> what one wants.</p><p>Experimenting through the OpenAI Beta <span class=&quot;smallcaps-auto&quot;>API</span> in June 2020, I find that <span class=&quot;smallcaps-auto&quot;>GPT-3</span> does not just match my <a href=&quot;/GPT-2#gpt-2-1.5b&quot;>finetuned <span class=&quot;smallcaps-auto&quot;>GPT-2-1</span>.5b-poetry</a> for poem-writing quality, but exceeds it, while being versatile in handling <a href=&quot;/GPT-3#poetry&quot;>poetry</a>, <a href=&quot;/GPT-3#tom-swifties&quot;>Tom Swifty puns</a>, science fiction, dialogue like Turing’s <a href=&quot;/GPT-3#turing-dialogue&quot;>Turing-test dialogue</a>, <a href=&quot;/GPT-3#literary-parodies&quot;>literary style parodies</a>… As the <em>pièce de résistance</em>, I recreate Stanislaw Lem’s <em>Cyberiad</em>’s <a href=&quot;/GPT-3#stanislaw-lems-cyberiad&quot;>“Trurl’s Electronic Bard”</a> poetry using <span class=&quot;smallcaps-auto&quot;>GPT-3</span>. (Along the way, I document instances of how the <span class=&quot;smallcaps-auto&quot;>BPE</span> text encoding <a href=&quot;/GPT-3#bpes&quot;>unnecessarily damages</a> <span class=&quot;smallcaps-auto&quot;>GPT-3</span>’s performance on a variety of tasks.)</p><p><span class=&quot;smallcaps-auto&quot;>GPT-3</span>’s samples are not just close to human level: they are creative, witty, deep, meta, and often beautiful. They demonstrate an ability to handle abstractions, like style parodies, I have not seen in <span class=&quot;smallcaps-auto&quot;>GPT-2</span> at all. Chatting with <span class=&quot;smallcaps-auto&quot;>GPT-3</span> feels uncannily like chatting with a human. I was impressed by the results reported in the <span class=&quot;smallcaps-auto&quot;>GPT-3</span> paper, and after spending a week trying it out, I remain impressed.</p><p>This page records <span class=&quot;smallcaps-auto&quot;>GPT-3</span> samples I generated in my explorations, and thoughts on <a href=&quot;/GPT-3#prompts-as-programming&quot;>how to use <span class=&quot;smallcaps-auto&quot;>GPT-3</span></a> and its remaining <a href=&quot;/GPT-3#weaknesses&quot;>weaknesses</a>. I hope you enjoy them even a tenth as much as I enjoyed testing <span class=&quot;smallcaps-auto&quot;>GPT-3</span> and watching the completions scroll across my screen.</p>" title="Creative writing by OpenAI's GPT-3 model, demonstrating poetry, dialogue, puns, literary parodies, and storytelling.">“<span class="smallcaps-auto">GPT-3</span> Creative Writing”</a>.</p>
<p>For anime plot summaries, see <a href="/TWDE#text"><span class="smallcaps-auto">TWDNE</span></a>; for generating <span class="smallcaps-auto">ABC</span>-formatted folk music, see <a href="/GPT-2-music" class="docMetadata" data-popup-title="GPT-2 Folk Music" data-popup-author="Gwern Branwen" data-popup-date="1 Nov 2019" data-popup-abstract="<p>In November 2019, I experimented with training a <span class=&quot;smallcaps-auto&quot;>GPT-2</span> neural net model to generate folk music in the high-level <span class=&quot;smallcaps-auto&quot;>ABC</span> music text format, following previous work in 2016 which used a char-<span class=&quot;smallcaps-auto&quot;>RNN</span> trained on a ‘The Session’ dataset. A <span class=&quot;smallcaps-auto&quot;>GPT-2</span> hypothetically can improve on an <span class=&quot;smallcaps-auto&quot;>RNN</span> by better global coherence &amp;amp; copying of patterns, without problems with the hidden-state bottleneck.</p><p>I encountered problems with the standard <span class=&quot;smallcaps-auto&quot;>GPT-2</span> model’s encoding of text which damaged results, but after <a href=&quot;/GPT-2-music#spaceless-model&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Folk Music: Training a Spaceless Model&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2019-12-12&quot; data-popup-abstract=&quot;While training a GPT-2-117M on a folk music corpus written in ABC format, persistent syntax errors kept being generated by an otherwise-high-quality model: random spaces would be generated, rendering a music piece either erroneous or lower-quality. Why? It seems to be some issue with the GPT BPE encoder handling of spaces which makes it difficult to emit the right space-separated characters. We found that ABC does not actually require spaces, and we simply removed all spaces from the corpus—noticeably improving quality of generated pieces.&quot;>fixing that</a>, I successfully trained it on <em>n</em>=205,304 <span class=&quot;smallcaps-auto&quot;>ABC</span> music pieces taken from The Session &amp;amp; <span class=&quot;smallcaps-auto&quot;>ABC</span>notation.com. The resulting music samples are in my opinion quite pleasant.</p><p>The <span class=&quot;smallcaps-auto&quot;>ABC</span> folk model &amp;amp; dataset are <a href=&quot;/GPT-2-music#combined-model-the-session-abcnotation.com&quot;>available for download</a>, and I provide for listening selected <a href=&quot;/GPT-2-music#samples&quot;>music samples</a> as well as medleys of random samples from throughout training.</p><p>We followed the <span class=&quot;smallcaps-auto&quot;>ABC</span> folk model with <a href=&quot;/GPT-2-music#generating-midi-with-10k30k-context-windows&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Generating MIDI Music With GPT-2: Generating MIDI by converting to ABC and expanding the GPT-2 context window—works, if only just&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2020-04-25&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;To expand the &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; model to cover a wider variety of musical genres, I turn to the next-most compact widespread music encoding format: &amp;lt;strong&amp;gt;&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt;&amp;lt;/strong&amp;gt;. There are hundreds of thousands of &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt;s which can be &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Decompiler&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Decompiler&amp;quot; data-popup-author=&amp;quot;English Wikipedia&amp;quot; data-popup-abstract=&amp;quot;&amp;lt;p&amp;gt;A &amp;lt;b&amp;gt;decompiler&amp;lt;/b&amp;gt; is a computer program that takes an executable file as input, and attempts to create a high level source file which can be recompiled successfully. It is therefore the opposite of a compiler, which takes a source file and makes an executable. Decompilers are usually unable to perfectly reconstruct the original source code, and as such, will frequently produce obfuscated code. Nonetheless, decompilers remain an important tool in the reverse engineering of computer software.&amp;lt;/p&amp;gt;&amp;quot; title=&amp;quot;Wikipedia: Decompiler&amp;quot;&amp;gt;decompiled&amp;lt;/a&amp;gt; to &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; format, averaging ~10k &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;BPE&amp;lt;/span&amp;gt;s—within &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt;’s feasible context window when trained on &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TPU&amp;lt;/span&amp;gt;s (which permit training of context windows up to 30k wide).&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;We compile the &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; from before and 2 large &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; datasets, and convert to &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;, yielding ~453k usable &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC-MIDI&amp;lt;/span&amp;gt; musical files (~5.1GB of text). We trained January–April 2020 on our &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TPU&amp;lt;/span&amp;gt; swarm (with many interruptions), achieving a final loss of ~0.2 (underfit).&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Sampling from the final model is hit-or-miss as it is prone to the likelihood repetition trap and it generates instruments one-by-one so it is common for instruments to be cut off or otherwise broken during sampling (indicating that &amp;lt;em&amp;gt;sampling&amp;lt;/em&amp;gt; is increasingly a bigger problem than &amp;lt;em&amp;gt;training&amp;lt;/em&amp;gt; for long-range sequence modeling). However, successful pieces are possible, and are musically far more diverse than the folk &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; corpus, with many pleasingly complex samples.&amp;lt;/p&amp;gt;&quot;>an <span class=&quot;smallcaps-auto&quot;>ABC</span>-<span class=&quot;smallcaps-auto&quot;>MIDI</span> model</a>: a <a href=&quot;/GPT-2-music#midi-dataset&quot;>dataset of 453k <span class=&quot;smallcaps-auto&quot;>ABC</span> pieces</a> decompiled from <span class=&quot;smallcaps-auto&quot;>MIDI</span> pieces, which fit into <span class=&quot;smallcaps-auto&quot;>GPT-2-117M</span> with an expanded context window when trained on <span class=&quot;smallcaps-auto&quot;>TPU</span>s. The <span class=&quot;smallcaps-auto&quot;>MIDI</span> pieces are far more diverse and challenging, and <span class=&quot;smallcaps-auto&quot;>GPT-2</span> underfits and struggles to produce valid samples but when sampling succeeds, it can generate <a href=&quot;/GPT-2-music#midi-samples&quot;>even better musical samples</a>.</p>" title="Generating Irish and folk music in ABC format using GPT-2-117M, with good results.">“<span class="smallcaps-auto">GPT-2</span> Folk Music”</a> &amp; <a href="/GPT-2-preference-learning" class="docMetadata" data-popup-title="GPT-2 Preference Learning for Music and Poetry Generation" data-popup-author="Gwern Branwen" data-popup-date="16 Dec 2019" data-popup-abstract="<p>Standard language generation neural network models, like <span class=&quot;smallcaps-auto&quot;>GPT-2</span>, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.</p><p>A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. <em>Preference learning</em> trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox <span class=&quot;smallcaps-auto&quot;>DRL</span> algorithm like <span class=&quot;smallcaps-auto&quot;>PPO</span>.</p><p>OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on <span class=&quot;smallcaps-auto&quot;>GPT-2</span>. Having previously used <a href=&quot;/GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my &amp;lt;a href=&amp;quot;/RNN-metadata&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;RNN metadata for mimicking individual author style&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;12 Sep 2015&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s are unsupervised generative models which learn to mimic text sequences. I suggest extending char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp;amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. A 2015 experiment using &amp;amp;lt;code&amp;amp;gt;torch-rnn&amp;amp;lt;/code&amp;amp;gt; on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; shows that a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I further try &amp;amp;amp;amp; fail to train &amp;amp;lt;a href=&amp;amp;quot;#geocities-char-rnn&amp;amp;quot;&amp;amp;gt;a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; on Geocities &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;HTML&amp;amp;lt;/span&amp;amp;gt;&amp;amp;lt;/a&amp;amp;gt; for unclear reasons.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;More successfully, &amp;amp;lt;a href=&amp;amp;quot;/GPT-2&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;3 March 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “GPT-2-poetry”, trained on the poems as a continuous stream of text, and “GPT-2-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;English Wikipedia&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;b&amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;lt;i&amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;quot;&amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;lt;/a&amp;amp;amp;gt;’s &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-image-height=&amp;amp;amp;quot;768&amp;amp;amp;quot; data-popup-image-width=&amp;amp;amp;quot;768&amp;amp;amp;quot;&amp;amp;amp;gt;website&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;For generating ABC-formatted folk music, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Music&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;quot;&amp;amp;amp;gt;“GPT-2 Folk Music”&amp;amp;amp;lt;/a&amp;amp;amp;gt;&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;I experiment in 2019 with a recently-developed alternative to char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s&amp;amp;lt;/a&amp;amp;gt;, the Transformer NN architecture, by finetuning training OpenAI’s &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-117M&amp;amp;lt;/span&amp;amp;gt; Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp;amp;amp; lines with inline metadata (the source book). The generated poetry is much better.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;2015–2016 text-generation experiments with char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt;s&amp;lt;/a&amp;gt;, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAI’s &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;-poetry”, trained on the poems as a continuous stream of text, and “&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Poetry Foundation&amp;quot; data-popup-author=&amp;quot;English Wikipedia&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;&amp;amp;lt;b&amp;amp;gt;The Poetry Foundation&amp;amp;lt;/b&amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;lt;i&amp;amp;gt;Poetry&amp;amp;lt;/i&amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Wikipedia: Poetry Foundation&amp;quot;&amp;gt;Poetry Foundation&amp;lt;/a&amp;gt;’s &amp;lt;a href=&amp;quot;https://www.poetryfoundation.org/poems&amp;quot;&amp;gt;website&amp;lt;/a&amp;gt;. .&amp;amp;gt; With just a few &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPU&amp;lt;/span&amp;gt;-days on 1080ti &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPU&amp;lt;/span&amp;gt;s, &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; finetuning can produce high-quality poetry which is more thematically consistent than my char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems. For the highest-quality AI poetry to date, see my followup page, &amp;lt;a href=&amp;quot;https://www.gwern.net/GPT-3&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-3 Creative Fiction&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;19 June 2020&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I continue my AI poetry generation experiments with OpenAI’s &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; successor, &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;, which is 116✕ larger, and much more powerful. &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;, however, is not merely a quantitative tweak yielding “&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; but better”—it is qualitatively different, exhibiting eerie runtime learning capabilities allowing even the raw model, with zero finetuning, to tackle almost any imaginable textual task purely by example or instruction. One does not train or program &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; in a normal way, but one engages in dialogue and writes prompts to teach &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; what one wants.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Experimenting through the OpenAI Beta &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;API&amp;amp;lt;/span&amp;amp;gt; in June 2020, I find that &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; does not just match my &amp;amp;lt;a href=&amp;amp;quot;/GPT-2#gpt-2-1.5b&amp;amp;quot;&amp;amp;gt;finetuned &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-1&amp;amp;lt;/span&amp;amp;gt;.5b-poetry&amp;amp;lt;/a&amp;amp;gt; for poem-writing quality, but exceeds it, while being versatile in handling &amp;amp;lt;a href=&amp;amp;quot;#poetry&amp;amp;quot;&amp;amp;gt;poetry&amp;amp;lt;/a&amp;amp;gt;, &amp;amp;lt;a href=&amp;amp;quot;#tom-swifties&amp;amp;quot;&amp;amp;gt;Tom Swifty puns&amp;amp;lt;/a&amp;amp;gt;, science fiction, dialogue like Turing’s &amp;amp;lt;a href=&amp;amp;quot;#turing-dialogue&amp;amp;quot;&amp;amp;gt;Turing-test dialogue&amp;amp;lt;/a&amp;amp;gt;, &amp;amp;lt;a href=&amp;amp;quot;#literary-parodies&amp;amp;quot;&amp;amp;gt;literary style parodies&amp;amp;lt;/a&amp;amp;gt;… As the &amp;amp;lt;em&amp;amp;gt;pièce de résistance&amp;amp;lt;/em&amp;amp;gt;, I recreate Stanislaw Lem’s &amp;amp;lt;em&amp;amp;gt;Cyberiad&amp;amp;lt;/em&amp;amp;gt;’s &amp;amp;lt;a href=&amp;amp;quot;#stanislaw-lems-cyberiad&amp;amp;quot;&amp;amp;gt;“Trurl’s Electronic Bard”&amp;amp;lt;/a&amp;amp;gt; poetry using &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;. (Along the way, I document instances of how the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;BPE&amp;amp;lt;/span&amp;amp;gt; text encoding &amp;amp;lt;a href=&amp;amp;quot;#bpes&amp;amp;quot;&amp;amp;gt;unnecessarily damages&amp;amp;lt;/a&amp;amp;gt; &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;’s performance on a variety of tasks.)&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;’s samples are not just close to human level: they are creative, witty, deep, meta, and often beautiful. They demonstrate an ability to handle abstractions, like style parodies, I have not seen in &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; at all. Chatting with &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; feels uncannily like chatting with a human. I was impressed by the results reported in the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; paper, and after spending a week trying it out, I remain impressed.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;This page records &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; samples I generated in my explorations, and thoughts on &amp;amp;lt;a href=&amp;amp;quot;#prompts-as-programming&amp;amp;quot;&amp;amp;gt;how to use &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;&amp;amp;lt;/a&amp;amp;gt; and its remaining &amp;amp;lt;a href=&amp;amp;quot;#weaknesses&amp;amp;quot;&amp;amp;gt;weaknesses&amp;amp;lt;/a&amp;amp;gt;. I hope you enjoy them even a tenth as much as I enjoyed testing &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; and watching the completions scroll across my screen.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Creative writing by OpenAI&amp;amp;#39;s GPT-3 model, demonstrating poetry, dialogue, puns, literary parodies, and storytelling.&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; Creative Writing”&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;For anime plot summaries, see &amp;lt;a href=&amp;quot;/TWDE#text&amp;quot;&amp;gt;&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TWDNE&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;; for generating &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;-formatted folk music, see &amp;lt;a href=&amp;quot;/GPT-2-music&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Folk Music&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;1 Nov 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;In November 2019, I experimented with training a &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; hypothetically can improve on an &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; by better global coherence &amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I encountered problems with the standard &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;lt;a href=&amp;amp;quot;#spaceless-model&amp;amp;quot;&amp;amp;gt;fixing that&amp;amp;lt;/a&amp;amp;gt;, I successfully trained it on &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;=205,304 &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp; &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;The &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; folk model &amp;amp;amp;amp; dataset are &amp;amp;lt;a href=&amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;quot;&amp;amp;gt;available for download&amp;amp;lt;/a&amp;amp;gt;, and I provide for listening selected &amp;amp;lt;a href=&amp;amp;quot;#samples&amp;amp;quot;&amp;amp;gt;music samples&amp;amp;lt;/a&amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;We followed the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; folk model with &amp;amp;lt;a href=&amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;quot;&amp;amp;gt;an &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt;-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; model&amp;amp;lt;/a&amp;amp;gt;: a &amp;amp;lt;a href=&amp;amp;quot;#midi-dataset&amp;amp;quot;&amp;amp;gt;dataset of 453k &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; pieces&amp;amp;lt;/a&amp;amp;gt; decompiled from &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; pieces, which fit into &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-117M&amp;amp;lt;/span&amp;amp;gt; with an expanded context window when trained on &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;TPU&amp;amp;lt;/span&amp;amp;gt;s. The &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;lt;a href=&amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;quot;&amp;amp;gt;even better musical samples&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; Folk Music”&amp;lt;/a&amp;gt; &amp;amp;amp; &amp;lt;a href=&amp;quot;/GPT-2-preference-learning&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Preference Learning for Music and Poetry Generation&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;16 Dec 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Standard language generation neural network models, like &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt;, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp;amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. &amp;amp;lt;em&amp;amp;gt;Preference learning&amp;amp;lt;/em&amp;amp;gt; trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;DRL&amp;amp;lt;/span&amp;amp;gt; algorithm like &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;PPO&amp;amp;lt;/span&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt;. Having previously used &amp;amp;lt;a href=&amp;amp;quot;/GPT-2&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;3 March 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In February 2019, following up on my &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/RNN-metadata&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;RNN metadata for mimicking individual author style&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;12 Sep 2015&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Char-RNNs are unsupervised generative models which learn to mimic text sequences. I suggest extending char-RNNs with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp;amp;amp;amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. A 2015 experiment using &amp;amp;amp;amp;lt;code&amp;amp;amp;amp;gt;torch-rnn&amp;amp;amp;amp;lt;/code&amp;amp;amp;amp;gt; on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-RNN shows that a char-RNN can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I further try &amp;amp;amp;amp;amp;amp; fail to train &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#geocities-char-rnn&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;a char-RNN on Geocities HTML&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; for unclear reasons.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;More successfully, &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;gwern&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;3 March 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: &amp;amp;amp;amp;amp;lt;q&amp;amp;amp;amp;amp;gt;“GPT-2-poetry”&amp;amp;amp;amp;amp;lt;/q&amp;amp;amp;amp;amp;gt;, trained on the poems as a continuous stream of text, and &amp;amp;amp;amp;amp;lt;q&amp;amp;amp;amp;amp;gt;“GPT-2-poetry-prefix”&amp;amp;amp;amp;amp;lt;/q&amp;amp;amp;amp;amp;gt;, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;I experiment in 2019 with a recently-developed alternative to char-RNNs&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, the Transformer NN architecture, by finetuning training OpenAI’s GPT-2-117M Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp;amp;amp;amp;amp; lines with inline metadata (the source book). The generated poetry is much better.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot;&amp;amp;amp;gt;2015–2016 text-generation experiments with char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt;s&amp;amp;amp;lt;/a&amp;amp;amp;gt;, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp; text generation. Using OpenAI’s &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;-poetry”, trained on the poems as a continuous stream of text, and “&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;English Wikipedia&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;b&amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;lt;i&amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;quot;&amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;lt;/a&amp;amp;amp;gt;’s &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;quot;&amp;amp;amp;gt;website&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;With just a few &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;-days on 1080ti &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;s, &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; finetuning can produce high-quality poetry which is more thematically consistent than my char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;For anime plot summaries, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/TWDE#text&amp;amp;amp;quot;&amp;amp;amp;gt;&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;TWDNE&amp;amp;amp;lt;/span&amp;amp;amp;gt;&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for generating &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;-formatted folk music, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Folk Music&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;In November 2019, I experimented with training a &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;RNN&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; hypothetically can improve on an &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;RNN&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; by better global coherence &amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I encountered problems with the standard &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#spaceless-model&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;fixing that&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;=205,304 &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp;amp;amp; &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; folk model &amp;amp;amp;amp;amp;amp; dataset are &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;available for download&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, and I provide for listening selected &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#samples&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;music samples&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;We followed the &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; folk model with &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;an &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC-MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; model&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;: a &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#midi-dataset&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;dataset of 453k &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; decompiled from &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces, which fit into &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; with an expanded context window when trained on &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;TPU&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;s. The &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;even better musical samples&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; Folk Music”&amp;amp;amp;lt;/a&amp;amp;amp;gt; &amp;amp;amp;amp;amp; &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-preference-learning&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Preference Learning for Music and Poetry Generation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;16 Dec 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Standard language generation neural network models, like &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp;amp;amp;amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;Preference learning&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;DRL&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; algorithm like &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;PPO&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;. Having previously used &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;3 March 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “GPT-2-poetry”, trained on the poems as a continuous stream of text, and “GPT-2-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;amp;quot;English Wikipedia&amp;amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;b&amp;amp;amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;amp;amp;lt;i&amp;amp;amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;quot; title=&amp;amp;amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;’s &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-image-height=&amp;amp;amp;amp;amp;quot;768&amp;amp;amp;amp;amp;quot; data-popup-image-width=&amp;amp;amp;amp;amp;quot;768&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;website&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;For generating ABC-formatted folk music, see &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;amp;quot;GPT-2 Music&amp;amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;quot; title=&amp;amp;amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;“GPT-2 Folk Music”&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; for poetry&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp; &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Folk Music&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;#spaceless-model&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;fixing that&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;amp;amp;lt;em&amp;amp;amp;amp;amp;gt;n&amp;amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;amp;gt;=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;#samples&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;music samples&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;music generation&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, I experimented with &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; preference learning for unconditional music and poetry generation.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;NLP&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp;amp;amp;amp;amp; succumb to adversarial examples easily.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp;amp;amp;amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#bradley-terry-preference-learning&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;directly model the implied ranking&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#optimization-by-backprop-not-blackbox&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;optimizes sequences to maximize the reward model’s output&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; Preference Learning for Music and Poetry Generation”&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for playing chess, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;A Very Unlikely Chess Game&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Scott Alexander&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-01-06&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;within&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://slatestarcodex.com/2019/02/28/meaningful/&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;meaningful&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Scott Alexander&amp;amp;amp;quot;&amp;amp;amp;gt;“A Very Unlikely Chess Game”&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for the Reddit comment generator, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;disumbrationist&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-01-12&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://openai.com/blog/gpt-2-1-5b-release/&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;finally released the full 1.5 billion parameter model&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://old.reddit.com/u/gwern&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;/u/gwern&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; (&amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://www.patreon.com/gwern&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;here’s his Patreon&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;) and Shawn Presser (&amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://old.reddit.com/u/shawwwn&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;/u/shawwwn&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://twitter.com/gwern/status/1215005375407112193&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;around $70K worth of TPU credits&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;all&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; rel=&amp;amp;amp;quot;archived alternate nofollow&amp;amp;amp;quot; data-url-original=&amp;amp;amp;quot;https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/&amp;amp;amp;quot; title=&amp;amp;amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )&amp;amp;amp;quot;&amp;amp;amp;gt;SubSimulator&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for fanfiction, the &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#archive-of-our-own-ao3-gpt-2-1.5b&amp;amp;amp;quot;&amp;amp;amp;gt;Ao3&amp;amp;amp;lt;/a&amp;amp;amp;gt;; and for video games, &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#video-game-walkthrough-gpt-2-1.5b&amp;amp;amp;quot;&amp;amp;amp;gt;the walkthrough model&amp;amp;amp;lt;/a&amp;amp;amp;gt;. For OpenAI’s &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-3&amp;amp;amp;lt;/span&amp;amp;amp;gt; followup, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://arxiv.org/abs/2005.14165#openai&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Language Models are Few-Shot Learners&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al [...]&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-05-28&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&amp;amp;amp;amp;#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.&amp;amp;amp;quot; title=&amp;amp;amp;quot;Brown et al 2020&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-3&amp;amp;amp;lt;/span&amp;amp;amp;gt;: Language Models are Few-Shot Learners”&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; for poetry&amp;amp;lt;/a&amp;amp;gt; &amp;amp;amp;amp; &amp;amp;lt;a href=&amp;amp;quot;/GPT-2-music&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Folk Music&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;1 Nov 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In November 2019, I experimented with training a &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; hypothetically can improve on an &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; by better global coherence &amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;I encountered problems with the standard &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#spaceless-model&amp;amp;amp;quot;&amp;amp;amp;gt;fixing that&amp;amp;amp;lt;/a&amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;lt;em&amp;amp;amp;gt;n&amp;amp;amp;lt;/em&amp;amp;amp;gt;=205,304 &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp;amp; &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;The &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; folk model &amp;amp;amp;amp;amp; dataset are &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;amp;quot;&amp;amp;amp;gt;available for download&amp;amp;amp;lt;/a&amp;amp;amp;gt;, and I provide for listening selected &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#samples&amp;amp;amp;quot;&amp;amp;amp;gt;music samples&amp;amp;amp;lt;/a&amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;We followed the &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; folk model with &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;amp;quot;&amp;amp;amp;gt;an &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; model&amp;amp;amp;lt;/a&amp;amp;amp;gt;: a &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#midi-dataset&amp;amp;amp;quot;&amp;amp;amp;gt;dataset of 453k &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces&amp;amp;amp;lt;/a&amp;amp;amp;gt; decompiled from &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces, which fit into &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; with an expanded context window when trained on &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;TPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;s. The &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;amp;quot;&amp;amp;amp;gt;even better musical samples&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;music generation&amp;amp;lt;/a&amp;amp;gt;, I experimented with &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; preference learning for unconditional music and poetry generation.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp;amp;amp; succumb to adversarial examples easily.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp;amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model &amp;amp;lt;a href=&amp;amp;quot;#bradley-terry-preference-learning&amp;amp;quot;&amp;amp;gt;directly model the implied ranking&amp;amp;lt;/a&amp;amp;gt; of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which &amp;amp;lt;a href=&amp;amp;quot;#optimization-by-backprop-not-blackbox&amp;amp;quot;&amp;amp;gt;optimizes sequences to maximize the reward model’s output&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; Preference Learning for Music and Poetry Generation”&amp;lt;/a&amp;gt;; for playing chess, see &amp;lt;a href=&amp;quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;A Very Unlikely Chess Game&amp;quot; data-popup-author=&amp;quot;Scott Alexander&amp;quot; data-popup-date=&amp;quot;2020-01-06&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running &amp;amp;lt;em&amp;amp;gt;within&amp;amp;lt;/em&amp;amp;gt; a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this &amp;amp;lt;a href=&amp;amp;quot;https://slatestarcodex.com/2019/02/28/meaningful/&amp;amp;quot;&amp;amp;gt;meaningful&amp;amp;lt;/a&amp;amp;gt;? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Scott Alexander&amp;quot;&amp;gt;“A Very Unlikely Chess Game”&amp;lt;/a&amp;gt;; for the Reddit comment generator, see &amp;lt;a href=&amp;quot;/docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots&amp;quot; data-popup-author=&amp;quot;disumbrationist&amp;quot; data-popup-date=&amp;quot;2020-01-12&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI &amp;amp;lt;a href=&amp;amp;quot;https://openai.com/blog/gpt-2-1-5b-release/&amp;amp;quot;&amp;amp;gt;finally released the full 1.5 billion parameter model&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by &amp;amp;lt;a href=&amp;amp;quot;https://old.reddit.com/u/gwern&amp;amp;quot;&amp;amp;gt;/u/gwern&amp;amp;lt;/a&amp;amp;gt; (&amp;amp;lt;a href=&amp;amp;quot;https://www.patreon.com/gwern&amp;amp;quot;&amp;amp;gt;here’s his Patreon&amp;amp;lt;/a&amp;amp;gt;) and Shawn Presser (&amp;amp;lt;a href=&amp;amp;quot;https://old.reddit.com/u/shawwwn&amp;amp;quot;&amp;amp;gt;/u/shawwwn&amp;amp;lt;/a&amp;amp;gt;), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required &amp;amp;lt;a href=&amp;amp;quot;https://twitter.com/gwern/status/1215005375407112193&amp;amp;quot;&amp;amp;gt;around $70K worth of TPU credits&amp;amp;lt;/a&amp;amp;gt;, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from &amp;amp;lt;em&amp;amp;gt;all&amp;amp;lt;/em&amp;amp;gt; the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen&amp;amp;lt;/p&amp;amp;gt;&amp;quot; rel=&amp;quot;archived alternate nofollow&amp;quot; data-url-original=&amp;quot;https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/&amp;quot; title=&amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )&amp;quot;&amp;gt;SubSimulator&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;; for fanfiction, the &amp;lt;a href=&amp;quot;#archive-of-our-own-ao3-gpt-2-1.5b&amp;quot;&amp;gt;Ao3&amp;lt;/a&amp;gt;; and for video games, &amp;lt;a href=&amp;quot;#video-game-walkthrough-gpt-2-1.5b&amp;quot;&amp;gt;the walkthrough model&amp;lt;/a&amp;gt;. For OpenAI’s &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; followup, see &amp;lt;a href=&amp;quot;https://arxiv.org/abs/2005.14165#openai&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Language Models are Few-Shot Learners&amp;quot; data-popup-author=&amp;quot;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al [...]&amp;quot; data-popup-date=&amp;quot;2020-05-28&amp;quot; data-popup-abstract=&amp;quot;Recent work has demonstrated substantial gains on many &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 achieves strong performance on many &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3&amp;amp;#39;s few-shot learning still struggles, as well as some datasets where &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 faces methodological issues related to training on large web corpora. Finally, we find that &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 in general.&amp;quot; title=&amp;quot;Brown et al 2020&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;: Language Models are Few-Shot Learners”&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&quot;><span class=&quot;smallcaps-auto&quot;>GPT-2</span> for poetry</a> &amp;amp; <a href=&quot;/GPT-2-music&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Folk Music&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;1 Nov 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In November 2019, I experimented with training a &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; neural net model to generate folk music in the high-level &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; music text format, following previous work in 2016 which used a char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; trained on a ‘The Session’ dataset. A &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; hypothetically can improve on an &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; by better global coherence &amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I encountered problems with the standard &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; model’s encoding of text which damaged results, but after &amp;lt;a href=&amp;quot;#spaceless-model&amp;quot;&amp;gt;fixing that&amp;lt;/a&amp;gt;, I successfully trained it on &amp;lt;em&amp;gt;n&amp;lt;/em&amp;gt;=205,304 &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; music pieces taken from The Session &amp;amp;amp; &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; folk model &amp;amp;amp; dataset are &amp;lt;a href=&amp;quot;#combined-model-the-session-abcnotation.com&amp;quot;&amp;gt;available for download&amp;lt;/a&amp;gt;, and I provide for listening selected &amp;lt;a href=&amp;quot;#samples&amp;quot;&amp;gt;music samples&amp;lt;/a&amp;gt; as well as medleys of random samples from throughout training.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;We followed the &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; folk model with &amp;lt;a href=&amp;quot;#generating-midi-with-10k30k-context-windows&amp;quot;&amp;gt;an &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; model&amp;lt;/a&amp;gt;: a &amp;lt;a href=&amp;quot;#midi-dataset&amp;quot;&amp;gt;dataset of 453k &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; pieces&amp;lt;/a&amp;gt; decompiled from &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; pieces, which fit into &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; with an expanded context window when trained on &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TPU&amp;lt;/span&amp;gt;s. The &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; pieces are far more diverse and challenging, and &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;lt;a href=&amp;quot;/GPT-2-music#midi-samples&amp;quot;&amp;gt;even better musical samples&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&quot;>music generation</a>, I experimented with <span class=&quot;smallcaps-auto&quot;>GPT-2</span> preference learning for unconditional music and poetry generation.</p><p>I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at <em>n</em>≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using <em>n</em>≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).</p><p>This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp; succumb to adversarial examples easily.</p><p>Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model <a href=&quot;/GPT-2-preference-learning#bradley-terry-preference-learning&quot;>directly model the implied ranking</a> of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which <a href=&quot;/GPT-2-preference-learning#optimization-by-backprop-not-blackbox&quot;>optimizes sequences to maximize the reward model’s output</a>.</p>">“<span class="smallcaps-auto">GPT-2</span> Preference Learning for Music and Poetry Generation”</a>; for playing chess, see <a href="https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/" class="docMetadata" data-popup-title="A Very Unlikely Chess Game" data-popup-author="Scott Alexander" data-popup-date="2020-01-06" data-popup-abstract="<p>Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running <em>within</em> a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.</p><p>…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this <a href=&quot;https://slatestarcodex.com/2019/02/28/meaningful/&quot;>meaningful</a>? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.</p><p>[See also the much later <a href=&quot;https://arxiv.org/abs/2007.03500&quot; &quot;The Go Transformer: Natural Language Modeling for Game Play&quot;>Noever et al 2020a</a>/<a href=&quot;https://arxiv.org/abs/2008.04057&quot; title=&quot;The Chess Transformer: Mastering Play using Generative Language Models&quot;>Noever et al 2020b</a> who do the exact same thing in applying GPT-2 to Go SGF/chess PGN games.]</p>" title="Scott Alexander">“A Very Unlikely Chess Game”</a>; for the Reddit comment generator, see <a href="/docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html" class="docMetadata" data-popup-title="Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots" data-popup-author="disumbrationist" data-popup-date="2020-01-12" data-popup-abstract="<p>When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI <a href=&quot;https://openai.com/blog/gpt-2-1-5b-release/&quot;>finally released the full 1.5 billion parameter model</a>.</p><p>The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by <a href=&quot;https://old.reddit.com/u/gwern&quot;>/u/gwern</a> (<a href=&quot;https://www.patreon.com/gwern&quot;>here’s his Patreon</a>) and Shawn Presser (<a href=&quot;https://old.reddit.com/u/shawwwn&quot;>/u/shawwwn</a>), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required around $70K worth of TPU credits, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.</p><p>Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.</p><p>One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from <em>all</em> the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.</p><p>…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen</p>" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/" title="Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )">SubSimulator<span class="smallcaps-auto">GPT-2</span></a>; for fanfiction, the <a href="/GPT-2#archive-of-our-own-ao3-gpt-2-1.5b">Ao3</a>; and for video games, <a href="/GPT-2#video-game-walkthrough-gpt-2-1.5b">the walkthrough model</a>. For OpenAI’s <span class="smallcaps-auto">GPT-3</span> followup, see <a href="https://arxiv.org/abs/2005.14165#openai" class="docMetadata" data-popup-title="Language Models are Few-Shot Learners" data-popup-author="Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al" data-popup-date="2020-05-28" data-popup-abstract="Recent work has demonstrated substantial gains on many <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current <span class=&quot;smallcaps-auto&quot;>NLP</span> systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train <span class=&quot;smallcaps-auto&quot;>GPT</span>-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 achieves strong performance on many <span class=&quot;smallcaps-auto&quot;>NLP</span> datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where <span class=&quot;smallcaps-auto&quot;>GPT</span>-3's few-shot learning still struggles, as well as some datasets where <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 faces methodological issues related to training on large web corpora. Finally, we find that <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 in general." title="Brown et al 2020">“<span class="smallcaps-auto">GPT-3</span>: Language Models are Few-Shot Learners”</a>.</p>
</blockquote>
</div>
<p>OpenAI announced in February 2019 in <a href="https://openai.com/blog/better-language-models/" class="docMetadata" data-popup-title="Better Language Models and Their Implications: We've trained a large-scale unsupervised language model [GPT-2] which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training" data-popup-author="Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles Brundage, Ilya Sutskever (OpenAI)" data-popup-date="2019-02-14" data-popup-abstract="<p>Our model, called <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 (a successor to <span class=&quot;smallcaps-auto&quot;>GPT</span>), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much <a href=&quot;https://github.com/openai/gpt-2&quot;>smaller model</a> for researchers to experiment with, as well as a <a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;>technical paper</a>.</p><p><span class=&quot;smallcaps-auto&quot;>GPT</span>-2 is a large <a href=&quot;https://arxiv.org/abs/1706.03762&quot;>Transformer</a>-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 is a direct scale-up of <span class=&quot;smallcaps-auto&quot;>GPT</span>, with more than 10X the parameters and trained on more than 10X the amount of data.</p><p><span class=&quot;smallcaps-auto&quot;>GPT</span>-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In addition, <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute.</p>">“Better Language Models and Their Implications”</a> their creation of “<span class="smallcaps-auto">GPT-2-1</span>.5b”, a Transformer<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> neural network 10x larger <a href="https://openai.com/blog/language-unsupervised/" class="docMetadata" data-popup-title="Improving Language Understanding with Unsupervised Learning" data-popup-author="OpenAI" data-popup-date="June 11, 2018" data-popup-abstract="We’ve obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we’re also releasing. Our approach is a combination of two existing ideas: <a href=&quot;https://arxiv.org/abs/1706.03762&quot;>transformers</a> and <a href=&quot;https://arxiv.org/abs/1511.01432&quot;>unsupervised pre-training</a>. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse datasets." title="GPT-1: Improving Language Understanding with Unsupervised Learning [June 2018]">than before</a> trained (like a char-<span class="smallcaps-auto">RNN</span> with a predictive loss) by unsupervised learning on 40GB of high-quality text curated by Redditors. <span class="smallcaps-auto">GPT-2-1</span>.5b led to large improvements over <span class="smallcaps-auto">GPT-1</span>’s natural language generation, is close to or <a href="https://paperswithcode.com/task/language-modelling" class="docMetadata" data-popup-title="Language Modelling State-of-the-art leaderboards" data-popup-author="paperswithcode.com" data-popup-date="2019-08-28" data-popup-abstract="Language modeling is the task of predicting the next word or character in a document. This page lists key recent papers on <span class=&quot;smallcaps-auto&quot;>NLP</span> language modeling and records reported research performance on the following tasks: WikiText-103, Penn Treebank (Word Level), enwiki8, Text8, One Billion Word, WikiText-2, Hutter Prize, Penn Treebank (Character Level)."><span class="smallcaps-auto">SOTA</span> on natural language modeling</a>, and demonstrated high performance on untrained <span class="smallcaps-auto">NLP</span> tasks (see the paper for more details: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf#openai" class="docMetadata" data-popup-title="Language Models are Unsupervised Multitask Learners" data-popup-author="Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever" data-popup-date="2019" data-popup-abstract="Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset—matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, <span class=&quot;smallcaps-auto&quot;>GPT</span>-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.">“Language Models are Unsupervised Multitask Learners”</a>, Radford et al 2019). By large improvements, one means that the best samples like the ones included in the OA announcement have started to reach an uncanny valley of text, capable of telling entire semi-coherent stories which can <a href="/docs/www/srconstantin.wordpress.com/486dba34fb7c61678ed10541ef4b71efc0c56918.html" rel="archived alternate nofollow" data-url-original="https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/" title="Humans Who Are Not Concentrating Are Not General Intelligences (Original URL: https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/ )">almost fool a sloppy reader</a>—certainly, the verisimilitude is better than any char-<span class="smallcaps-auto">RNN</span> output I’ve seen. (A dump of many more samples <a href="https://github.com/openai/gpt-2/blob/master/gpt2-samples.txt">is available on GitHub</a>. There is also an interactive word-by-word <a href="https://gpt2.apps.allenai.org/?text=Joel%20is" class="docMetadata" data-popup-title="LM Explorer (alpha)" data-popup-author="Allen Institute For Artificial Intelligence" data-popup-date="2019-02-26" data-popup-abstract="<p>This demonstration uses the public 345M 117M parameter OpenAI <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 language model to generate sentences.</p><p>Enter some initial text and the model will generate the most likely next words. You can click on one of those words to choose it and continue or just keep typing. Click the left arrow at the bottom to undo your last choice.</p>">“<span class="smallcaps-auto">GPT-2</span>-Explorer”</a>.) The full <span class="smallcaps-auto">GPT-2-1</span>.5b model was not released, but a much smaller one a tenth the size, <span class="smallcaps-auto">GPT-2-117M</span> was released in February 2019, which I call “<span class="smallcaps-auto">GPT-2-117M</span>” to avoid confusion.</p>
<p><span class="smallcaps-auto">GPT-2-117M</span> was used in most initial experiments with <span class="smallcaps-auto">GPT-2</span>-based text generation. OA’s next largest models, <span class="smallcaps-auto">GPT-2-355M</span> &amp; <a href="https://openai.com/blog/gpt-2-6-month-follow-up/" class="docMetadata" data-popup-title="GPT-2: 6-Month Follow-Up" data-popup-author="OpenAI" data-popup-date="2019-08-20" data-popup-abstract="<p>We’re releasing the 774 million parameter <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 language model after the release of our small 124M model in February, staged release of our medium 355M model in May, and subsequent research with partners and the AI community into the model’s potential for misuse and societal benefit. We’re also releasing an open-source legal agreement to make it easier for organizations to initiate model-sharing partnerships with each other, and are publishing a technical report about our experience in coordinating with the wider AI research community on publication norms.</p><p>…Research from these partners will factor into our future release decisions, as will observing how the 774M model is used, and discussing language models with researchers and policymakers to understand the considerations around larger models. As part of our staged release strategy, our current plan is to release the 1558M parameter model in a few months, but it’s plausible that findings from a partner, or malicious usage of our 774M model, could change this.</p>" title="GPT-2: 6-Month Follow-Up"><span class="smallcaps-auto">GPT-2-774M</span></a> were released in May &amp; August 2019, and the final, largest, <span class="smallcaps-auto">GPT-2-1</span>.5b model was released in <a href="https://openai.com/blog/gpt-2-1-5b-release/" class="docMetadata" data-popup-title="GPT-2: 1.5B Release" data-popup-author="Irene Solaiman, Jack Clark, Miles Brundage" data-popup-date="2019-11-05" data-popup-abstract="<p>As the final model release of GPT-2’s staged release, we’re releasing the largest version (1.5B parameters) of GPT-2 along with code and model weights to facilitate detection of outputs of GPT-2 models. While there have been larger language models released since August, we’ve continued with our original staged release plan in order to provide the community with a test case of a full staged release process. We hope that this test case will be useful to developers of future powerful models, and we’re actively continuing the conversation with the AI community on responsible publication.</p><p><em>Our findings</em>:</p><ol type=&quot;1&quot;><li>Humans find GPT-2 outputs convincing.</li><li>GPT-2 can be fine-tuned for misuse.</li><li>Detection is challenging.</li><li>We’ve seen no strong evidence of misuse so far.</li><li>We need standards for studying bias.</li></ol><p>…<em>Next steps</em>: Our experience with GPT-2 over the past 9 months has given us valuable insight into the challenges and opportunities for creating responsible publication norms in AI. We’re continuing our work on this issue via participation in the Partnership on AI’s “Responsible Publication Norms for Machine Learning” project and discussions with our colleagues in the research community.</p>">November 2019</a> (too late to be used in most of these experiments); 355M–774M turn out to just <em>barely</em> be trainable on commodity <span class="smallcaps-auto">GPU</span>s.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Also worth noting is the release of Gokaslan &amp; Cohen 2019’s <a href="https://medium.com/@vanya_cohen/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc" class="docMetadata" data-popup-title="OpenGPT-2: We Replicated GPT-2-1.5b Because You Can Too" data-popup-author="Aaron Gokaslan, Vanya Cohen" data-popup-date="2019-08-22" data-popup-abstract="<p>Recently, large language models like <span class=&quot;smallcaps-auto&quot;>BERT</span>¹, <span class=&quot;smallcaps-auto&quot;>XLN</span>et², <span class=&quot;smallcaps-auto&quot;>GPT</span>-2³, and Grover⁴ have demonstrated impressive results generating new content and multiple tasks. Since Open-AI has not released their largest model [<span class=&quot;smallcaps-auto&quot;>GPT</span>-2-1.5b] at this time, we seek to replicate the model to allow others to build on our pretrained model and further improve it. You can access the model and generate text using our Google Colab.</p><p>…We demonstrate that many of the results of the paper can be replicated by two masters students…Because our replication efforts are not unique, and large language models are the current most effective means of countering generated text, we believe releasing our model is a reasonable first step towards countering the potential future abuse of these kinds of models.</p><p>We base our implementation off of the Grover model⁴ and modify their codebase to match the language modeling training objective of <span class=&quot;smallcaps-auto&quot;>GPT</span>-2. Since their model was trained on a similarly large corpus, much of the code and hyperparameters proved readily reusable. We did not substantially change the hyperparameters from Grover.</p><p>From start to finish, we estimate that we use under $500,000 in cloud compute for all of our experiments including searching for hyper-parameters and testing various cleaning methods on our datasets. The cost of training the model from scratch using our code is about $50,000.</p><p>…Despite the differences in our training distribution, we do report similar perplexities over most datasets.</p>" title="OpenGPT-2: We Replicated GPT-2 Because You Can Too">independently-trained <span class="smallcaps-auto">GPT-2-1</span>.5b model</a>, which produces good samples if perhaps not quite as good as the OpenAI <span class="smallcaps-auto">GPT-2-1</span>.5b, but which was not trainable at the time on desktop <span class="smallcaps-auto">GPU</span>s<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> although it does still at least run (allowing for sampling/prompting if not training). OpenAI notes that “Since February, we’ve spoken with more than five groups who have replicated <span class="smallcaps-auto">GPT-2</span>”, and some have or gone further than <span class="smallcaps-auto">GPT-2-1</span>.5b: replications/extensions include Gokaslan &amp; Cohen 2019, <a href="https://arxiv.org/abs/1905.12616" class="docMetadata" data-popup-title="Defending Against Neural Fake News" data-popup-author="Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi" data-popup-date="2019-08-26" data-popup-abstract="Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.   Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.   Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news." title="'Grover: Defending Against Neural Fake News', Zellers et al 2019">Grover</a>, Hugging Face (a <span class="smallcaps-auto">NLP</span> startup), <a href="https://arxiv.org/abs/1906.08237" class="docMetadata" data-popup-title="XLNet: Generalized Autoregressive Pretraining for Language Understanding" data-popup-author="Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le" data-popup-date="2019-08-26" data-popup-abstract="With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like <span class=&quot;smallcaps-auto&quot;>BERT</span> achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, <span class=&quot;smallcaps-auto&quot;>BERT</span> neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose <span class=&quot;smallcaps-auto&quot;>XLN</span>et, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of <span class=&quot;smallcaps-auto&quot;>BERT</span> thanks to its autoregressive formulation. Furthermore, <span class=&quot;smallcaps-auto&quot;>XLN</span>et integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, <span class=&quot;smallcaps-auto&quot;>XLN</span>et outperforms <span class=&quot;smallcaps-auto&quot;>BERT</span> on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking." title="'XLNet: Generalized Autoregressive Pretraining for Language Understanding', Yang et al 2019"><span class="smallcaps-auto">XLN</span>et</a>, Nvidia’s <a href="https://nv-adlr.github.io/MegatronLM" class="docMetadata" data-popup-title="MegatronLM: Training Billion+ Parameter Language Models Using GPU Model Parallelism" data-popup-author="<span class=&quot;smallcaps-auto&quot;>NVIDIA</span> <span class=&quot;smallcaps-auto&quot;>ADLR</span>" data-popup-date="2019-08-13" data-popup-abstract="Larger language models are dramatically more useful for <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks such as article completion, question answering, and dialog systems. Training the largest neural language model has recently been the best way to advance the state of the art in <span class=&quot;smallcaps-auto&quot;>NLP</span> applications. Two recent papers, <a href=&quot;https://arxiv.org/abs/1810.04805&quot;><span class=&quot;smallcaps-auto&quot;>BERT</span></a> and <a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;><span class=&quot;smallcaps-auto&quot;>GPT</span>-2</a>, demonstrate the benefits of large scale language modeling. Both papers leverage advances in compute and available text corpora to significantly surpass state of the art performance in natural language understanding, modeling, and generation. Training these models requires hundreds of exaflops of compute and <a href=&quot;https://arxiv.org/abs/1604.06174&quot;>clever memory management</a> to trade recomputation for a reduced memory footprint. However, for very large models beyond a billion parameters, the memory on a single <span class=&quot;smallcaps-auto&quot;>GPU</span> is not enough to fit the model along with the parameters needed for training, requiring model parallelism to split the parameters across multiple <span class=&quot;smallcaps-auto&quot;>GPU</span>s. Several approaches to model parallelism exist, but they are difficult to use, either because they rely on custom compilers, or because they scale poorly or require changes to the optimizer.</p></div><div><p> In this work, we implement a simple and efficient model parallel approach by making only a few targeted modifications to existing <a href=&quot;https://openreview.net/pdf?id=BJJsrmfCZ&quot;>PyTorch</a> transformer implementations. <a href=&quot;https://github.com/nvidia/megatron-lm&quot;>Our code</a> is written in native Python, leverages mixed precision training, and utilizes the <a href=&quot;https://developer.nvidia.com/nccl&quot;><span class=&quot;smallcaps-auto&quot;>NCCL</span> library</a> for communication between <span class=&quot;smallcaps-auto&quot;>GPU</span>s. We showcase this approach by training an 8.3 billion parameter transformer language model with 8-way model parallelism and 64-way data parallelism on 512 <span class=&quot;smallcaps-auto&quot;>GPU</span>s, making it the <b>largest transformer based language model ever trained at 24× the size of <span class=&quot;smallcaps-auto&quot;>BERT</span> and 5.6× the size of <span class=&quot;smallcaps-auto&quot;>GPT</span>-2</b>. We have published the code that implements this approach at <a href=&quot;https://github.com/NVIDIA/Megatron-LM&quot;>our GitHub repository</a>.</p></div><div><p> Our experiments are conducted on <span class=&quot;smallcaps-auto&quot;>NVIDIA</span>’s <a href=&quot;https://devblogs.nvidia.com/dgx-superpod-world-record-supercomputing-enterprise&quot;><span class=&quot;smallcaps-auto&quot;>DGX</span> Super<span class=&quot;smallcaps-auto&quot;>POD</span></a>. Without model parallelism, we can fit a baseline model of 1.2B parameters on a single V100 32GB <span class=&quot;smallcaps-auto&quot;>GPU</span>, and sustain 39 Tera<span class=&quot;smallcaps-auto&quot;>FLOPS</span> during the overall training process, which is 30% of the theoretical peak <span class=&quot;smallcaps-auto&quot;>FLOPS</span> for a single <span class=&quot;smallcaps-auto&quot;>GPU</span> in a <span class=&quot;smallcaps-auto&quot;>DGX</span>2-H server. Scaling the model to 8.3 billion parameters on 512 <span class=&quot;smallcaps-auto&quot;>GPU</span>s with 8-way model parallelism, we achieved up to <b>15.1 Peta<span class=&quot;smallcaps-auto&quot;>FLOPS</span> sustained performance</b> over the entire application and reached <b>76% scaling efficiency</b> compared to the single <span class=&quot;smallcaps-auto&quot;>GPU</span> case." title="MegatronLM: Training Billion+ Parameter Language Models Using GPU Model Parallelism">MegatronLM</a>, Google’s <a href="https://arxiv.org/abs/1910.10683#google" class="docMetadata" data-popup-title="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" data-popup-author="Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu" data-popup-date="2020-02-23" data-popup-abstract="Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (<span class=&quot;smallcaps-auto&quot;>NLP</span>). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for <span class=&quot;smallcaps-auto&quot;>NLP</span> by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new &quot;Colossal Clean Crawled Corpus&quot;, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for <span class=&quot;smallcaps-auto&quot;>NLP</span>, we release our dataset, pre-trained models, and code." title="'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', Raffel et al 2019">T5</a> (<a href="https://colab.research.google.com/drive/1-ROO7L09EupLFLQM-TWgDHa5-FIOdLLh">finetuning Colab</a>), and MS’s <a href="https://arxiv.org/abs/1911.00536" class="docMetadata" data-popup-title="DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation" data-popup-author="Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan" data-popup-date="2019-11-05" data-popup-abstract="We present a large, tunable neural conversational response generation model, Dialo<span class=&quot;smallcaps-auto&quot;>GPT</span> (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, Dialo<span class=&quot;smallcaps-auto&quot;>GPT</span> extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage Dialo<span class=&quot;smallcaps-auto&quot;>GPT</span> generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems." title="'DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation', Zhang et al 2019">Dialo<span class="smallcaps-auto">GPT</span></a>.</p>
<section id="gpt-2-117m-generating-poetry" class="level1">
<h1><a href="#gpt-2-117m-generating-poetry" title="Link to section: §'GPT-2-117M: Generating Poetry'"><span class="smallcaps-auto">GPT-2-117M</span>: Generating Poetry</a></h1>
<div class="epigraph">
<blockquote>
<p>“‘I don’t speak’, Bijaz said. ‘I operate a machine called language. It creaks and groans, but is mine own.’”</p>
<p><a href="https://en.wikipedia.org/wiki/Frank_Herbert" class="docMetadata" data-popup-title="Frank Herbert" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Franklin Patrick Herbert Jr.</b> was an American science fiction author best known for the 1965 novel <i>Dune</i> and its five sequels. Though he became famous for his novels, he also wrote short stories and worked as a newspaper journalist, photographer, book reviewer, ecological consultant, and lecturer.</p>" title="Wikipedia: Frank Herbert">Frank Herbert</a>, <em><a href="https://en.wikipedia.org/wiki/Dune_Messiah" class="docMetadata" data-popup-title="Dune Messiah" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>Dune Messiah</b></i> is a science fiction novel by American writer Frank Herbert, the second in his <span><i>Dune</i> series</span> of six novels. It was originally serialized in <i>Galaxy</i> magazine in 1969. The American and British editions have different prologues summarizing events in the previous novel. <i>Dune Messiah</i> and its sequel <i>Children of Dune</i> were collectively adapted by the Sci-Fi Channel in 2003 into a miniseries entitled <i>Frank Herbert's Children of Dune</i>. In 2002, the Science Fiction Book Club also published the two novels in one volume.</p>" title="Wikipedia: Dune Messiah">Dune Messiah</a></em></p>
</blockquote>
</div>
<p>Naturally, people immediately used <span class="smallcaps-auto">GPT-2-117M</span> for all sorts of things, and I applied it myself to generate surreal anime plot summaries &amp; dialogue for <a href="/TWDNE" class="docMetadata" data-popup-title="This Waifu Does Not Exist" data-popup-author="Gwern Branwen" data-popup-date="19 Feb 2019" data-popup-abstract="<p>Generating high-quality anime faces has long been a task neural networks struggled with. The invention of Style<span class=&quot;smallcaps-auto&quot;>GAN</span> in 2018 has effectively solved this task and I have trained a Style<span class=&quot;smallcaps-auto&quot;>GAN</span> model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, <a href=&quot;https://www.thiswaifudoesnotexist.net/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;ThisWaifuDoesNotExist.net&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2019-02-19&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;&amp;lt;code&amp;gt;ThisWaifuDoesNotExist.net&amp;lt;/code&amp;gt;&amp;lt;/a&amp;gt; (&amp;lt;a href=&amp;quot;/TWDNE&amp;quot;&amp;gt;&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TWDNE&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;) is a static website which uses JS to display random &amp;lt;a href=&amp;quot;/Faces&amp;quot;&amp;gt;anime faces generated by Style&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GAN&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt; neural networks, along with &amp;lt;a href=&amp;quot;/GPT-2&amp;quot;&amp;gt;&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT&amp;lt;/span&amp;gt;-2&amp;lt;/a&amp;gt;-generated &amp;#39;anime plot summaries&amp;#39;.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;&amp;lt;figure&amp;gt;&amp;lt;img src=&amp;quot;/images/gan/thiswaifudoesnotexist.png&amp;quot; alt=&amp;quot;A screenshot of “This Waifu Does Not Exist” (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;quot; /&amp;gt;&amp;lt;figcaption&amp;gt;A screenshot of “This Waifu Does Not Exist” (&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TWDNE&amp;lt;/span&amp;gt;) showing a random Style&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GAN&amp;lt;/span&amp;gt;-generated anime face and a random &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT&amp;lt;/span&amp;gt;-2-117M text sample conditioned on anime keywords/phrases.&amp;lt;/figcaption&amp;gt;&amp;lt;/figure&amp;gt;&amp;lt;/p&amp;gt;&quot;>“This Waifu Does Not Exist”</a> for displaying random Style<span class=&quot;smallcaps-auto&quot;>GAN</span> 2 faces. <span class=&quot;smallcaps-auto&quot;>TWDNE</span> displays a different neural-net-generated face &amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. The model can also be used interactively for exploration &amp;amp; editing in the <a href=&quot;https://artbreeder.com/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Artbreeder&quot; data-popup-author=&quot;Joel Simon&quot; data-popup-date=&quot;2019-09-09&quot; data-popup-abstract=&quot;[Artbreeder is an interactive &amp;lt;span class=\&amp;quot;smallcaps-auto\&amp;quot;&amp;gt;GAN&amp;lt;/span&amp;gt; generator website. Originally named &amp;quot;Ganbreeder&amp;quot; and providing only the 256px Big&amp;lt;span class=\&amp;quot;smallcaps-auto\&amp;quot;&amp;gt;GAN&amp;lt;/span&amp;gt; generator, it now provides a variety of Big&amp;lt;span class=\&amp;quot;smallcaps-auto\&amp;quot;&amp;gt;GAN&amp;lt;/span&amp;gt; &amp;amp; Style&amp;lt;span class=\&amp;quot;smallcaps-auto\&amp;quot;&amp;gt;GAN&amp;lt;/span&amp;gt; models, including the anime portrait Style&amp;lt;span class=\&amp;quot;smallcaps-auto\&amp;quot;&amp;gt;GAN&amp;lt;/span&amp;gt; model. (It is more general than the similar Waifu Labs, but my anime model is not as good.) Users can generate random samples and explore slight variants of them to gradually explore the &amp;quot;latent space&amp;quot; and find interesting images, but they can also edit images more directly, upload existing images to find the most similar image produced by the model, etc. A popular website, it has generated &amp;gt;56m images from September 2019 to January 2020.]&quot;>Artbreeder online service</a>.</p><p><span class=&quot;smallcaps-auto&quot;>TWDNE</span> faces have been used as screensavers, user avatars, character art for game packs or <a href=&quot;https://klimaleksus.github.io/FindTwin/&quot; title=&quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&quot;>online</a> <a href=&quot;https://github.com/darabos/high-five-trading&quot; title=&quot;Action stock exchange game for Repl.it Game Jam 2019&quot;>games</a>, uploaded to Pixiv, and used in a research paper (<a href=&quot;https://arxiv.org/abs/1904.01774&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot; data-popup-author=&quot;Atsuhiro Noguchi, Tatsuya Harada&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&quot; title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot;>Noguchi &amp;amp; Harada 2019</a>). <span class=&quot;smallcaps-auto&quot;>TWDNE</span> results also helped inspired Sizigi Studio’s online interactive waifu <span class=&quot;smallcaps-auto&quot;>GAN</span>, <a href=&quot;https://waifulabs.com/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Waifu Labs&quot; data-popup-author=&quot;Sizigi Studios&quot; data-popup-date=&quot;2019-07-23&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;[Waifu Labs is an interactive website for generating (1024px?) anime faces using a customized StyleGAN trained on Danbooru2018. Similar to Artbreeder, it supports face exploration and face editing, and at the end, a user can purchase prints of a particular face.]&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;We taught a world-class artificial intelligence how to draw anime. All the drawings you see were made by a non-human artist! Wild, right? It turns out machines love waifus almost as much as humans do. We proudly present the next chapter of human history: lit waifu commissions from the world&amp;#39;s smartest AI artist. In less than 5 minutes, the artist learns your preferences to make the perfect waifu just for you.&amp;lt;/p&amp;gt;&quot;>Waifu Labs</a>, which generates even better anime faces than my Style<span class=&quot;smallcaps-auto&quot;>GAN</span> results.</p>">“This Waifu Does Not Exist”</a>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> Even more naturally, just as with char-<span class="smallcaps-auto">RNN</span>s, <span class="smallcaps-auto">GPT-2</span> models, even unfinetuned, work well for poetry:</p>
<ul>
<li><p><span class="smallcaps-auto">GPT-2-117M</span> completions of Allen Ginsberg’s <a href="https://www.poetryfoundation.org/poems/49303/howl" class="docMetadata" data-popup-title="Howl" data-popup-author="Allen Ginsberg" data-popup-date="1955" data-popup-abstract="<p>[Poem]</p><p>I saw the best minds of my generation destroyed by madness, starving hysterical naked,<br>dragging themselves through the negro streets at dawn looking for an angry fix,<br>angelheaded hipsters burning for the ancient heavenly connection to the starry dynamo in the machinery of night,<br>who poverty and tatters and hollow-eyed and high sat up smoking in the supernatural darkness of cold-water flats floating across the tops of cities contemplating jazz,<br>who bared their brains to Heaven under the El and saw Mohammedan angels staggering on tenement roofs illuminated,<br>who passed through universities with radiant cool eyes hallucinating Arkansas and Blake-light tragedy among the scholars of war,<br>who were expelled from the academies for crazy &amp; publishing obscene odes on the windows of the skull, …</p>">“Howl”</a>: <a href="/docs/www/antinegationism.tumblr.com/b8b33f6363d7605222ef0254e9f9ae0fc9d775e7.html" class="docMetadata" data-popup-title="An Eternal Howl" data-popup-author="antinegationism" data-popup-date="2019-02-18" data-popup-abstract="<p>[<span class=&quot;smallcaps-auto&quot;>GPT</span>-2] Prompt: </p><p>“Moloch! Solitude! Filth! Ugliness! Ashcans and unobtainable dollars! Children screaming under the stairways! Boys sobbing in armies! Old men weeping in the parks! <br>Moloch! Moloch! Nightmare of Moloch! Moloch the loveless! Mental Moloch! Moloch the heavy judger of men! <br>Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of sorrows!<br>Moloch whose buildings are judgment! Moloch the vast stone of war! Moloch the stunned governments! <br>Moloch whose mind is pure machinery! Moloch whose blood is running money! Moloch whose fingers are ten armies! Moloch whose breast is a cannibal dynamo! <br>Moloch whose ear is a smoking tomb! <br>Moloch whose eyes are a thousand blind windows! <br>Moloch whose skyscrapers stand in the long streets like endless Jehovahs! <br>Moloch whose factories dream and croak in the fog! <br>Moloch whose smoke-stacks and antennae crown the cities …”<br>Response: <br>like a pile of wood! <br>Moloch whose huge flawless towers cry like song in the daytime! <br>Moloch whose buildings implode like a living Iometer of History! <br>Moloch whose bridges sag violently like a lattice apple falling on the stream! <br>Moloch whose court are the host of billions of millions of nerves of passion and anger still raging up and down every dollar in the world! <br>Moloch the shadow world long, Americanized and red in red! <br>Moloch is cruel always and everywhere. <br>Moloch is perfect to human purposes! <br>Moloch is kept beyond height blind. Moloch the incorporeal skull of man! <br>Moloch the flag of equality! <br>Moloch the endless ever-splitting price of hell! <br>Moloch the worldly bathing anchor in fungicide money! <br>Moloch the hideous empire in feeble lines! Moloch the death of the dreamless things! Moloch the glorious command of justice! Moloch the bloody trail!</p>" rel="archived alternate nofollow" data-url-original="http://antinegationism.tumblr.com/post/182901133106/an-eternal-howl" title="(Original URL: http://antinegationism.tumblr.com/post/182901133106/an-eternal-howl )">“An Eternal Howl”</a> (comments: <a href="/docs/www/old.reddit.com/3ccff7f931d485393ae843e2c96d96964034d41a.html" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/slatestarcodex/comments/as8ke7/an_eternal_howl/" title="(Original URL: https://old.reddit.com/r/slatestarcodex/comments/as8ke7/an_eternal_howl/ )">1</a>, <a href="https://old.reddit.com/r/gwern/comments/asb1va/an_eternal_howl_gpt2_completions_of_allen/">2</a>); <a href="https://twitter.com/robertskmiles/status/1104008764997419009" class="docMetadata" data-popup-title="Rob Miles on Twitter" data-popup-author="Robert Miles" data-popup-date="2019-03-08" data-popup-abstract="Has anyone done this yet? I told my <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 program to write new verses for Allen Ginsberg&amp;#39;s Howl, and forgot to tell it to stop:<a href=&quot;https://t.co/TpNnR5V2Ex&quot;>https://t.co/TpNnR5V2Ex</a> <a href=&quot;https://t.co/Q4fUQY8czU&quot;>pic.twitter.com/Q4fUQY8czU</a>">Rob Miles</a></p></li>
<li><p>Shelley’s “<a href="https://en.wikipedia.org/wiki/Ozymandias" class="docMetadata" data-popup-title="Ozymandias" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>&quot;Ozymandias&quot;</b> is the title of two related sonnets published in 1818.</p>" title="Wikipedia: Ozymandias">Ozymandias</a>”: <a href="https://medium.com/@ysaw/gpt-2-writes-a-shelley-poem-bc0c19fe4ee3" class="docMetadata" data-popup-title="GPT-2 Writes a Shelley Poem" data-popup-author="Stephen Woods" data-popup-date="2019-02-20" data-popup-abstract="<p>(seed text was Ozymandias)</p><p>The sea hath never parted from the rocky shores,<br>That doth not rise again the sea, and its long, tumbled,<br>As if the waters have broken by its side.<br>And as the ocean came up to sea,<br>And the sea was broken by the wreck,<br>And the sea sank to the Sea of the Dead<br>By the wad of blood. It would have slain the Sea of the Dead<br>But it would not: its last breath, And it did.<br>This sea in the sea hath a Sea of waggon,<br>And a Sea of waggon of blood; For all what hath not come<br>With it hath fallen, And as a waggon of blood the waggon<br>And the blood is flowing out of that waggon.<br>So, then, are the works of Zeus,<br>And the work of Zeus, O Great Creator, whose designs<br>With the work of the great King are he wrought<br>In their midst. . . . .<br>In the desert have the mighty Gods<br>And the gods themselves come.”<br>A serpent of darkness rises among the hills<br>In a forest on the land of the gods.<br>A mighty bird that flies out a bird-hoast,<br>And flies out a fly, from behind its wing.<br>And there the sky is filled with white, and it is broken through.<br>And like this, we know the mighty Man<br>Of the clouds from which he fled upon the land of the gods,<br>The mighty serpent that the king sent for them,<br>And he brought them to that man; and he said:<br>“I know you, O mighty Man from the earth!”<br>And what do you say? You knew him when you saw in the dream,<br>And you saw from under one of his arms a mighty serpent<br>That a mighty man can lift up, and drive<br>He that was in the way and lifted up; and he led him to be a god,<br>And he he led him down the great mountain which I have prepared.<br>Let his tongue speak, tell thee, O man: “I know thee.<br>Here thou dost see, O great King, that is the very power of the Gods”<br>Let His voice say, to thee, “that is the very power of the Gods”<br>Let His voice say to thee, “that is the voice of the whole creation”</p>">“<span class="smallcaps-auto">GPT-2</span> Writes a Shelley Poem”</a></p></li>
<li><p>Alexander Pope’s <em><a href="https://en.wikipedia.org/wiki/Essay_On_Criticism" class="docMetadata" data-popup-title="An Essay on Criticism" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>An Essay on Criticism</b></i> is one of the first major poems written by the English writer Alexander Pope (1688–1744). It is the source of the famous quotations &quot;To err is human, to forgive divine,&quot; &quot;A little learning is a dang'rous thing&quot;, and &quot;Fools rush in where angels fear to tread.&quot; It first appeared in 1711 after having been written in 1709, and it is clear from Pope's correspondence that many of the poem's ideas had existed in prose form since at least 1706. Composed in heroic couplets and written in the Horatian mode of satire, it is a verse essay primarily concerned with how writers and critics behave in the new literary commerce of Pope's contemporary age. The poem covers a range of good criticism and advice, and represents many of the chief literary ideals of Pope's age.</p>" title="Wikipedia: Essay On Criticism">Essay On Criticism</a></em>: <a href="https://slatestarcodex.com/2019/02/19/gpt-2-as-step-toward-general-intelligence/" class="docMetadata" data-popup-title="GPT-2 As Step Toward General Intelligence" data-popup-author="Scott Alexander" data-popup-date="2019-02-19" data-popup-abstract="A machine learning researcher writes me in response to yesterday’s post, saying:<p><blockquote>I still think <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 is a brute-force statistical pattern matcher which blends up the internet and gives you back a slightly unappetizing slurry of it when asked.</blockquote></p><p>I resisted the urge to answer “Yeah, well, your mom is a brute-force statistical pattern matcher which blends up the internet and gives you back a slightly unappetizing slurry of it when asked.”</p><p>But I think it would have been true.</p><p>A very careless plagiarist takes someone else’s work and copies it verbatim: “The mitochondria is the powerhouse of the cell”. A more careful plagiarist takes the work and changes a few words around: “The mitochondria is the energy dynamo of the cell”. A plagiarist who is more careful still changes the entire sentence structure: “In cells, mitochondria are the energy dynamos”. The most careful plagiarists change everything except the underlying concept, which they grasp at so deep a level that they can put it in whatever words they want—at which point it is no longer called plagiarism.</p><p><span class=&quot;smallcaps-auto&quot;>GPT</span>-2 writes fantasy battle scenes by reading a million human-written fantasy battle scenes, distilling them down to the concept of a fantasy battle scene, and then building it back up from there. I think this is how your mom (and everyone else) does it too. <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 is worse at this, because it’s not as powerful as your mom’s brain. But I don’t think it’s doing a different thing. We’re all blending experience into a slurry; the difference is how finely we blend it…</p>">“<span class="smallcaps-auto">GPT-2</span> As Step Toward General Intelligence”</a></p></li>
<li><p>8 famous opening lines from Tennyson, Yeats, Shakespeare, Henley, Whitman, T.S. Eliot: <a href="https://twitter.com/peterkz_swe/status/1098668851640848384" class="docMetadata" data-popup-title="Peter Krantz on Twitter" data-popup-author="Peter Krantz" data-popup-date="2019-02-21" data-popup-abstract="<p>First line of famous poems continued by the <a href=&quot;https://twitter.com/OpenAI&quot;>@openAI</a> <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 example model from &amp;quot;Language Models are Unsupervised Multitask Learners&amp;quot; <a href=&quot;https://twitter.com/hashtag/gpt2poetrytwsrcpt2poetry</a> <a href=&quot;https://twitter.com/hashtag/GPT2&quot;>#GPT2</a><br>\U0001F447\U0001F3FC</p>&amp;mdash; Peter Krantz (@peterkz_swe) <a href=&quot;https://twitter.com/peterkz_swe/status/1098668851640848384&quot;>February 21, 2019</a></blockquote><br><script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;></script><br><blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;><p lang=&quot;en&quot; dir=&quot;ltr&quot;>It little profits that an idle king,<br>who loves his throne for a moment to enjoy a good meal, <br>might, if he was not in the right position, <br>become the subject of a great banquet.<br>But as the royal household will do, <br>so too shall their subjects. <a href=&quot;https://twitter.com/hashtag/gpt2poetry&quot;>#gpt2poetry</a> <a href=&quot;https://twitter.com/hashtag/tennyson&quot;>#tennyson</a></p>">Peter Krantz</a></p></li>
<li><p>Kyle McDonald <a href="https://github.com/kylemcdonald/gpt-2-poetry" class="docMetadata" data-popup-title="gpt-2-poetry" data-popup-author="Kyle McDonald" data-popup-date="2019-03-04" data-popup-abstract="<p>I used <code>download-urls.py</code> to quickly download the <span class=&quot;smallcaps-auto&quot;>HTML</span> from poetryfoundation.org based on the <span class=&quot;smallcaps-auto&quot;>URL</span>s in <code>romantic-urls.txt</code>.</p><p>Then I used <code>Parse Poetry.ipynb</code> to parse the <span class=&quot;smallcaps-auto&quot;>HTML</span> and extract the title, author, and poem. There are some glitches here with newlines being rendered in some places they shouldn't, and not being rendered in places where they should. This notebook saves a bunch of text files to output/ that include metadata as the first few lines.</p><p>Then I used <code>Generate GPT-2.ipynb</code> to generate poems based on random chunks from the poems and the seed words. This notebook saves files to <code>poems.json</code> and <code>generated.json</code>. To run this notebook, first get <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 running, and drop the notebook in the <code>gpt-2/src/</code> directory.</p><p>Both Python notebooks import from <code>utils</code> which I have separately pushed here.</p><p>Finally, I load <code>generated.json</code> and <code>poems.json</code> with JavaScript in <code>index.html</code> and display the results.</p>">provided a tool around <span class="smallcaps-auto">GPT-2-117M</span></a> demonstrating ~154 prompts</p></li>
<li><p><a href="/docs/www/ask-gpt.tumblr.com/3533dd6615ae971b1ad91964f32b6bdf2f2d3758.html" rel="archived alternate nofollow" data-url-original="https://ask-gpt.tumblr.com/" title="(Original URL: https://ask-gpt.tumblr.com/ )">“Ask <span class="smallcaps-auto">GPT</span>2: Helpful Advice From A Confused Robot”</a>: T.S. Eliot’s <a href="/docs/www/ask-gpt.tumblr.com/237df35081c284274cceb097c8afd99a5b2f3fb8.html" rel="archived alternate nofollow" data-url-original="https://ask-gpt.tumblr.com/post/183402346117/april-is-the-cruelest-month-breeding-n-lilacs" title="(Original URL: https://ask-gpt.tumblr.com/post/183402346117/april-is-the-cruelest-month-breeding-n-lilacs )">“Wasteland”</a></p></li>
<li><p>Samuel Taylor Coleridge’s “<a href="https://en.wikipedia.org/wiki/The_Rime_of_the_Ancient_Mariner" class="docMetadata" data-popup-title="The Rime of the Ancient Mariner" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>The Rime of the Ancient Mariner</b></i> is the longest major poem by the English poet Samuel Taylor Coleridge, written in 1797–98 and published in 1798 in the first edition of <i>Lyrical Ballads</i>. Some modern editions use a revised version printed in 1817 that featured a gloss. Along with other poems in <i>Lyrical Ballads</i>, it is often considered a signal shift to modern poetry and the beginning of British Romantic literature.</p>" title="Wikipedia: The Rime of the Ancient Mariner">The Rime of the Ancient Mariner</a>”: <a href="https://medium.com/merzazine/fridai-water-water-everywhere-as-read-by-artificial-intelligence-d02bb3d2b156" class="docMetadata" data-popup-title="FridAI: ‘Water, water, everywhere’, as read by Artificial Intelligence" data-popup-author="Merzmensch Kosmopol" data-popup-date="2019-03-15" data-popup-abstract="<p>Samuel Taylor Coleridge once introduced to the world that desolated ship journey in his “The Rime of the Ancient Mariner”:<br><blockquote>Water, water, everywhere,<br>And all the boards did shrink;<br>Water, water, everywhere,<br>Nor any drop to drink.</blockquote>Ulysses, Noah and Job, so many allusions are hidden in this masterpiece about “a sadder and a wiser man”. And so I wanted to know, which journey will be chosen by <span class=&quot;smallcaps-auto&quot;>GPT</span>-2, a Deep Learning entity, developed by OpenAI to write texts.</p><p>And it wrote me a poem.</p><p>My input was the four lined stanza, as cited above. And here is the poetic vision by Artificial Intelligence:</p><p><blockquote>Water, water, everywhere, <br>And all the boards did shrink; <br>Water, water, everywhere, <br>Nor any drop to drink. <br>Water, water, everywhere, <br>And all the boards did shrink; <br>Water, water, everywhere, <br>No balm in his hair: <br>Water, water, everywhere, <br>And all the boards did shrink. <br>Water, water, everywhere, <br>Yet we like its rarity. <br>Water, water, everywhere, <br>And all the boards did shrink. <br>Water, water, everywhere.</blockquote></p><p>Yes, the rarity of the water (everywhere) is striking. Even if the hair is suffering without balm. Probably because you cannot drink a drop.</p><p>But the poem was not over. AI was embraced by inspiration…</p>">“FridAI: ‘Water, water, everywhere’, as read by Artificial Intelligence”</a></p></li>
<li><p><a href="/docs/www/yudhanjaya.com/b4be0e0fba30c25b1c66b3773286d9695eb6b778.html" class="docMetadata" data-popup-title="The Poetry Machine" data-popup-author="Yudhanjaya Wijeratne" data-popup-date="2019-04-12" data-popup-abstract="<p>I’ve always admired the translations of Chinese poetry–I’m no expert on the field, but there are two poets named Du Fu and Li Bai that I really like. They were legendary masters from the Great Tang Dynasty, and (if the translations are accurate), they had a phenomenal talent for freezing a moment and capturing that particular slice of time with their words; their poems read like a string of Polaroids stretched across a riverbank.</p><p>Here, for example, is a Du Fu poem. Among other things, there’s a certain simplicity here: one strong emotion resonates through, and unlike much of the English verse I grew up with, it’s firmly in the present tense:<blockquote>A <span class=&quot;smallcaps-auto&quot;>GPT</span> <span class=&quot;smallcaps-auto&quot;>GPT</span><br>In a sharp gale from the wide sky apes are whimpering,<br>Birds are flying homeward over the clear lake and white sand,<br>Leaves are dropping down like the spray of a waterfall,<br>While I watch the long river always rolling on.<br>I have come three thousand miles away. Sad now with autumn<br>And with my hundred years of woe, I climb this height alone.<br>Ill fortune has laid a bitter frost on my temples,<br>Heart-ache and weariness are a thick dust in my wine.</blockquote> <p>Which I suppose is why this appeals to me—there’s a rare clarity here, even if the translation might be inaccurate.</p><p>So the Tang poets seemed like the right place to start with for my experiment with machine-generated art (and besides, the excellent GWERN already did the usual English[1]). Right now, I’ve snuck away for a few hours from a my statistical models to peek at the code I set to run this morning.<p><p>Among those of us who work with machine learning, the work I’ve put into this whole project is trivial: a tiny dataset, a cup of coffee, a few lines of Python code, and a single cigarette while I waited for OpenAI’s transformer-based generation model [2] to download.</p>" rel="archived alternate nofollow" data-url-original="http://yudhanjaya.com/2019/04/the-poetry-machine/" title="(Original URL: http://yudhanjaya.com/2019/04/the-poetry-machine/ )">retraining on Tang-era Chinese poetry (in English)</a></p>
<ul>
<li><a href="https://arxiv.org/abs/1907.00151" class="docMetadata" data-popup-title="GPT-based Generation for Classical Chinese Poetry" data-popup-author="Yi Liao, Yasheng Wang, Qun Liu, Xin Jiang" data-popup-date="2019-09-10" data-popup-abstract="We present a simple yet effective method for generating high quality classical Chinese poetry with Generative Pre-trained Language Model (<span class=&quot;smallcaps-auto&quot;>GPT</span>). The method adopts a simple <span class=&quot;smallcaps-auto&quot;>GPT</span> model, without using any human crafted rules or features, or designing any additional neural components. While the proposed model learns to generate various forms of classical Chinese poems, including Jueju, L\&quot;{u}shi, various Cipai and Couples, the generated poems are of very high quality. We also propose and implement a method to fine-tune the model to generate acrostic poetry. To the best of our knowledge, this is the first to employ <span class=&quot;smallcaps-auto&quot;>GPT</span> in developing a poetry generation system. We have released an online mini demonstration program on Wechat to show the generation capability of the proposed method for classical Chinese poetry.">“<span class="smallcaps-auto">GPT</span>-based Generation for Classical Chinese Poetry”</a>, Liao et al 2019</li>
</ul></li>
<li><p><a href="https://twitter.com/rossgoodwin/status/1124901310677913600" class="docMetadata" data-popup-title="Ross Goodwin on Twitter" data-popup-author="@rossgoodwin" data-popup-date="2019-05-04" data-popup-abstract="<p>THREE MORE GPT-2 POEMS</p><p>I've been training this poetry model on a corpus structured to encourage thematic integration of individual keyword prompts, and the word &quot;ghost&quot; produced these results.</p><p>{ 1 / 3 } <a href=&quot;https://t.co/GehRqfQGq8&quot;>pic.twitter.com/GehRqfQGq8</a>…</p>">retraining on a thematic-keyword-organized poem corpus</a></p></li>
<li><p><a href="/docs/www/old.reddit.com/14b0aa1719ce16b97ddd47ba7e3020485fd64292.html" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/coc09l/p_these_lyrics_do_not_exist/" title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/coc09l/p_these_lyrics_do_not_exist/ )">song lyrics</a></p></li>
<li><p><a href="/docs/www/iforcedabot.com/eb2bca0e56e959f9628e536274c407edb71813f9.html" class="docMetadata" data-popup-title="Testing The Limits of Grover The Neural Fake News Detector. Can It Write Fiction? Can It Write Riddles?" data-popup-author="Jonathan Fly" data-popup-date="2019-05-31" data-popup-abstract="<p>Grover is a neural network modeled after <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 as a state-of-the-art detector for Neural Network fake news. Grover is also a state-of-the-art generator of fake news and they provide a web interface. Since Grover is modeled after the full-size 1.5B <span class=&quot;smallcaps-auto&quot;>GPT</span>-2—not the smaller version the public has access to—this is a bit like getting a back door to the full-size <span class=&quot;smallcaps-auto&quot;>GPT</span>-2. This is very exciting! (Update: They just reduced the size of the public model to 345M—everything in this post come from the full size model. Glad I stayed up way too late hammering the real thing for samples… Update 2: It’s back up!)</p><p>Grover was trained on 5000 news domains indexed by Google News, not the internet generally like <span class=&quot;smallcaps-auto&quot;>GPT</span>-2. As you would expect Grover excels at writing fake news: <blockquote><p>Scientists Recommend Against Vaccination: &quot;The Evidence Is Now Clear: Vaccines Cause Autism and Cancer&quot;</p><p>After a 15-year study, scientists in Britain are recommending that the British government refrain from administering vaccines. The study, written by Mike Gunton, professor of epidemiology and professor of genetic medicine at the University of Liverpool, claims that the chances of the <span class=&quot;smallcaps-auto&quot;>GPT</span> vaccine, a vaccine for measles, mumps and rubella, causing autism in children are “strongly possible.” And while Gunton and his team say their findings only apply to the <span class=&quot;smallcaps-auto&quot;>GPT</span> vaccine, they believe there are “many similar disorders” linked to vaccines and there is “strong evidence” vaccines cause autism and leukemia. Gunton told the Telegraph that no effort should be made to give vaccines to children younger than 3 months of age. The vaccine is highly controversial, and parents have been choosing to opt out of the <span class=&quot;smallcaps-auto&quot;>GPT</span> vaccination in recent years. This year, the British government’s vaccination program chose not to distribute the <span class=&quot;smallcaps-auto&quot;>GPT</span> vaccine, citing the study as the reason.</p></blockquote> You don’t have to fish to get coherent fake news of Grover, it’s absolutely great at it.</p>" rel="archived alternate nofollow" data-url-original="https://iforcedabot.com/what-can-a-fake-news-detector-do/" title="Testing The Limits of Grover The Neural Fake News Detector. Can It Write Fiction? Can It Write Riddles? (Original URL: https://iforcedabot.com/what-can-a-fake-news-detector-do/ )">verse from a <span class="smallcaps-auto">GPT-2-1</span>.5b trained on a Google News corpus (‽)</a> (using <a href="https://arxiv.org/abs/1905.12616" class="docMetadata" data-popup-title="Defending Against Neural Fake News" data-popup-author="Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi" data-popup-date="2019-08-26" data-popup-abstract="Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.   Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.   Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news." title="'Grover: Defending Against Neural Fake News', Zellers et al 2019">Grover</a>)</p></li>
<li><p><a href="/docs/www/reddit.com/2ba72007b23422f532f93511f38a2ba1c5afe3c7.html" rel="archived alternate nofollow" data-url-original="https://reddit.com/r/SubSimulatorGPT2Meta/comments/ccvspt/update_experimenting_with_generating_hybrid/" title="(Original URL: https://reddit.com/r/SubSimulatorGPT2Meta/comments/ccvspt/update_experimenting_with_generating_hybrid/ )">Writer-specific style/content mixing of models</a></p></li>
<li><p><a href="/docs/www/einstein.ai/54d6580672a72898c0e2fde484455f0c243ee0b7.pdf" class="docMetadata" data-popup-title="CTRL: A Conditional Transformer Language Model For Controllable Generation" data-popup-author="Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher (Salesforce)" data-popup-date="2019-09-11" data-popup-abstract="Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release <span class=&quot;smallcaps-auto&quot;>CTRL</span>, a 1.6 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow <span class=&quot;smallcaps-auto&quot;>CTRL</span> to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of <span class=&quot;smallcaps-auto&quot;>CTRL</span> at <a href=&quot;https://github.com/salesforce/ctrl&quot;><code>github.com/salesforce/ctrl</code></a>." rel="archived alternate nofollow" data-url-original="https://einstein.ai/presentations/ctrl.pdf" title="'CTRL: A Conditional Transformer Language Model For Controllable Generation', Keskar et al 2019 (Original URL: https://einstein.ai/presentations/ctrl.pdf )"><span class="smallcaps-auto">CTRL</span></a> appears capable of generating verse when prompted with the “books” genre, see the <a href="https://github.com/salesforce/ctrl#Generations">Github repository’s “Weary with toil…” example</a> (<span class="smallcaps-auto">CTRL</span> uses a ‘prefix’ approach similar to mine, and the “books” prefix corresponds to Project Gutenberg text, so it is not surprising that its samples would resemble my <span class="smallcaps-auto">GPT-2</span>-poetry samples)</p></li>
<li><p><a href="/docs/www/papergains.co/7ee092dafe3615f738c32405c2b9e7c543847330.pdf#page=3" rel="archived alternate nofollow" data-url-original="https://papergains.co/pdfs/Transformer_Poetry-978-1-7341647-0-1.pdf#page=3" title="(Original URL: https://papergains.co/pdfs/Transformer_Poetry-978-1-7341647-0-1.pdf#page=3 )"><em>Transformer Poetry: Poetry classics reimagined by artificial intelligence</em></a>, Kane Hsieh 2019<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p></li>
<li><p>Kenyon College class projects: <a href="/docs/ai/poetry/2020-zitelli.pdf" title="'345M-GPT-2 After James Wright: Can AI Generate Convincing Contemporary Poetry? Contemporary Poetry?', Zittelli 2020">James Wright</a>/<a href="/docs/ai/poetry/2020-case.pdf" title="'GPT-2 AI Poetry Generation: Writing like Donne', Case 2020">John Donne</a>/<a href="/docs/ai/poetry/2020-barrio.pdf" title="'Writing the Next American Hit: Using GPT-2 to Explore the Possibility of Creating Successful AI-Generated Song Lyrics Possibility of Creating Successful AI-Generated Song Lyric', Barrio 2020">Taylor Swift</a></p></li>
</ul>
<p>Poetry is a natural fit for machine generation because we don’t necessarily expect it to make sense or have standard syntax/grammar/vocabulary, and because it is often as much about the sound as the sense. Humans may find even mediocre poetry quite hard to write, but <a href="https://en.wikipedia.org/wiki/Moravec%27s_paradox" class="docMetadata" data-popup-title="Moravec's paradox" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Moravec's paradox</b> is the discovery by artificial intelligence and robotics researchers that, contrary to traditional assumptions, high-level reasoning requires very little computation, but low-level sensorimotor skills require enormous computational resources. The principle was articulated by Hans Moravec, Rodney Brooks, Marvin Minsky and others in the 1980s. As Moravec writes, &quot;it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility&quot;.</p>" title="Wikipedia: Moravec's paradox">machines are indefatigable</a> and can generate many samples to select from, so the final results can be pretty decent.</p>
<p>The quality of the results is limited by sometimes only having access to smaller models and difficulty in running larger models at all; that can’t be fixed (yet). But quality is also reduced by <span class="smallcaps-auto">GPT-2-117M</span> being trained on all kinds of text, not just poetry, which means sampling may quickly diverge into prose (as seems to happen particularly easily if given only a single opening line, which presumably makes it hard for it to infer that it’s supposed to generate poetry rather than much more common prose), and it may not have learned poetry as well as it could have, as poetry presumably made up a minute fraction of its corpus (Redditors not being particularly fond of as unpopular a genre these days as poetry). Finetuning or retraining the released <span class="smallcaps-auto">GPT-2-117M</span> model on a large poetry corpus would solve the latter two problems.</p>
<p>The poetry samples above did not exploit finetuning because OpenAI did not provide any code to do so and <a href="https://github.com/openai/gpt-2/issues/19">declined to provide any when asked</a>. Fortunate, <a href="https://github.com/nshepperd/gpt-2">nshepperd wrote a simple finetuning training implementation</a>, which I could use for adding more interesting samples to my <span class="smallcaps-auto">TWDNE</span> and for retraining on poetry corpuses to compare with my previous char-<span class="smallcaps-auto">RNN</span> poetry attempts back in 2015–2016 (see the top of this page). An <a href="https://github.com/ConnorJL/GPT2">alternative <span class="smallcaps-auto">GPT-2</span> training implementation</a> with support for training on <span class="smallcaps-auto">GCP</span> <span class="smallcaps-auto">TPU</span>s has been created by Connor Leahy (<a href="https://medium.com/@NPCollapse/replicating-gpt2-1-5b-86454a7f26af" class="docMetadata" data-popup-title="Replicating GPT2–1.5B" data-popup-author="Connor Leahy" data-popup-date="2019-06-06" data-popup-abstract="<p>In this post, I want to quickly talk about the technical and organizational questions around my recent replication of <span class=&quot;smallcaps-auto&quot;>GPT</span>2–1.5B. Please read my main post for the full story. I will try to keep this post brief.</p>><b>The important facts</b></p><p>Code: <a href=&quot;https://github.com/ConnorJL/GPT2&quot;>https://github.com/ConnorJL/GPT2</a></p><p>Samples: <a href=&quot;https://github.com/ConnorJL/GPT2/tree/master/samples&quot;>https://github.com/ConnorJL/GPT2/tree/master/samples</a></p><p>The code should run out of the box on <span class=&quot;smallcaps-auto&quot;>GPT</span>s and <span class=&quot;smallcaps-auto&quot;>GPT</span>s (and <span class=&quot;smallcaps-auto&quot;>GPT</span>s, if you’re really desperate). I used the parameters specified in 1.5B.json and trained it on a preemptible v3–512 <span class=&quot;smallcaps-auto&quot;>GPT</span> pod (which is actually more powerful than the machine OpenAI used) for around a week (with interruptions). Code and instructions for generating the dataset are also included in the repo.</p><p>You can download my models with the script in the repo. Currently I have a weaker version of 117M, and a model I call PrettyBig which is slightly larger than OpenAI’s 345M, which means it is technically the largest <span class=&quot;smallcaps-auto&quot;>GPT</span>2 model currently publicly available.</p><p>I will be releasing 1.5B to the public on July 1<sup>st</sup>, if, and only if, no one shows me a convincing reason not to. When I do, it will be downloadable just like my other models.</p>" title="Replicating GPT2-1.5b">technical details</a>), who <a href="https://medium.com/@NPCollapse/addendum-evaluation-of-my-model-e6734b51a830" class="docMetadata" data-popup-title="Addendum: Evaluation of My Model" data-popup-author="Connor Leahy" data-popup-date="2019-06-12" data-popup-abstract="<p>As a mercifully short addendum, I’d like to quickly address a few questions about my model. Please read my update post to hear my important updated beliefs on this situation, because I believe the details of how powerful my model is or not are not actually very important to the overall situation.</p><p>As described in my technical post, my model is not identical to OpenAI’s, because I simply didn’t have all the details of what they did. The truth is also that the samples and metrics I have shown aren’t 100% accurate. For one, my metric code is flawed, I made several rookie mistakes in setting up accurate evaluation (let train and eval data mix, used metrics whose math I didn’t understand etc), and the model I used to generate the samples is in fact not the final trained model, but one about halfway through the training. I didn’t take my time to evaluate the strength of my model, I simply saw I had the same amount of hardware as OpenAI and code as close to the paper as possible and went with it. The reason for this is a simple human flaw: I got cold feet once I realized what I was sitting on and acted rashly. I made a mistake, I did something stupid, that’s all there is to it.</p><p>Thanks to help from OpenAI it is now safe to say that my model is not as powerful as OpenAI’s. The metric results for WikiText2, <span class=&quot;smallcaps-auto&quot;>GPT</span> and <span class=&quot;smallcaps-auto&quot;>GPT</span> are (lower is better):</p><p><span class=&quot;smallcaps-auto&quot;>GPT</span>2: 18.67 / 8.63 / 36.51</p><p>Mine: 43.79 / 109.47 / 202.29</p><p>Although I used the same amount of hardware (or more), the differences in my training setup and hyperparameters made a significant difference. Which is an unfortunate reality to anyone familiar with reproducing deep learning papers. I don’t think my model in its current state is even as dangerous as 117M in its text generating abilities. But I believe to have found the quirks in my setup that have held the model back, and they are easy to fix. I am very tempted to continue tinkering with the model and seeing if I can improve it…but I will be holding back for now.</p>" title="Addendum: Evaluation of My Model">trained a <span class="smallcaps-auto">GPT-2-1</span>.5b (albeit to substantially worse performance)</a>.</p>
</section>
<section id="training-gpt-2-117m-to-generate-poetry" class="level1">
<h1><a href="#training-gpt-2-117m-to-generate-poetry" title="Link to section: §'Training GPT-2-117M To Generate Poetry'">Training <span class="smallcaps-auto">GPT-2-117M</span> To Generate Poetry</a></h1>
<section id="data-the-project-gutenberg-poetry-corpus" class="level2">
<h2><a href="#data-the-project-gutenberg-poetry-corpus" title="Link to section: §'Data: The Project Gutenberg Poetry Corpus'">Data: The Project Gutenberg Poetry Corpus</a></h2>
<div class="epigraph">
<blockquote>
<p>“My heart, why come you here alone?<br>The wild thing of my heart is grown<br>To be a thing,<br>Fairy, and wild, and fair, and whole”</p>
<p><span class="smallcaps-auto">GPT-2</span></p>
</blockquote>
</div>
<p>For the poetry corpus, Allison Parrish’s public domain <a href="https://github.com/aparrish/gutenberg-poetry-corpus">“A Gutenberg Poetry Corpus”</a> (“approximately three million lines of poetry extracted from hundreds of books from Project Gutenberg”) will serve admirably. A few other possibilities surface in <a href="https://toolbox.google.com/datasetsearch/search?query=poem%20OR%20poetry&amp;docid=T3haTlmLU9Dl6xqYAAAAAA%3D%3D">Google Dataset Search</a>, like <a href="https://www.kaggle.com/ultrajack/modern-renaissance-poetry">“Poems from <code>poetryfoundation.org</code>”</a>, but nothing particularly compelling.</p>
<p>As far as the text formatting goes, <span class="smallcaps-auto">GPT-2-117M</span> is flexible, you can dump in pretty much any text into a text file to use as the corpus, but some text formats are better than others. You want something which is as regular as possible (in both syntax &amp; semantics), but also one which is as close to the kind of text you want generated, but also which wastes as few symbols as possible. Regularity makes learning easier, and you don’t want to have to massage the output too much, but on the other hand, <span class="smallcaps-auto">GPT-2-117M</span> has a narrow ‘window’ and no memory whatsoever, so if each line is padded out with a lot of formatting or even just whitespace, one would expect that to considerably damage output coherence—as most of the fixed ‘window’ is wasted on meaningless repetitive whitespace, while other changes like replacing newlines with the poetic convention of ’ / ’ are worse than nothing (since newline is 1 character vs 3 and maximally dense). Minimizing formatting also makes the cross-entropy loss easier to interpret or compare across datasets/runs: if there is a lot of formatting which is easy to predict or little formatting, the loss can look misleadingly good (or bad) as it easily predicts the formatting but struggles on more meaningful content.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>The PG corpus has a strange format: each line is a separate <span class="smallcaps-auto">JSON</span> object, consisting of one line of poetry and a numeric ID for the work it’s from. Fortunately, the file as a whole is in order (if the lines were out of order, training on them would destroy the long-range language modeling which is the Transformer’s <em>raison d’être</em>!), so to turn it into a clean text file for training on, we can simply query it with <code>jq</code> and strip out the remaining formatting. This provides a pretty good format over all: the newlines are meaningful, no symbols are wasted on leading or trailing whitespace, and it looks like what we want. It is imperfect in that metadata/formatting we would like to be there, such as author or poem title, is not there, and things we would prefer not to be there, like the prose prefaces of books or annotations, are, but hard to see how to fix those easily. Another flaw I learned about only afterwards is that the PG corpus has been censored <em>ad usum Delphini</em> <a href="https://github.com/dariusk/wordfilter">to remove</a> “egregiously offensive content” such as “racist/sexist/ableist” words like “Pakistan” or “homogenous” (while, of course, permitting words like “shit” or “fuck”). This corpus is unsuited for any serious academic work, but it should be fine for playing around with generating poems.</p>
<p>Setting up the <span class="smallcaps-auto">GPT-2-117M</span> training environment &amp; obtaining the poetry corpus:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode Bash collapse"><code class="sourceCode bash"><span class="fu">git</span> clone <span class="st">'https://github.com/nshepperd/gpt-2.git'</span>
<span class="bu">cd</span> gpt-2
<span class="bu">source</span> activate <span class="va">$MY_TENSORFLOW_ENVIRONMENT</span> <span class="co"># set up your particular virtualenv</span>
<span class="fu">sh</span> download_model.sh 117M <span class="co"># download original OA model</span>

<span class="fu">wget</span> <span class="st">'http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz'</span>
<span class="fu">gunzip</span> gutenberg-poetry-v001.ndjson.gz
<span class="fu">cat</span> gutenberg-poetry-v001.ndjson <span class="kw">|</span> <span class="ex">jq</span> .s <span class="kw">|</span> <span class="fu">sed</span> -e <span class="st">'s/^.//'</span> -e <span class="st">'s/.$//'</span> -e <span class="st">'s/\\//g'</span> <span class="kw">\</span>
    <span class="op">&gt;&gt;</span> <span class="ex">gutenberg-poetry-v001.txt</span> ## delete JSON quoting

<span class="ex">shuf</span> gutenberg-poetry-v001.txt <span class="kw">|</span> <span class="fu">head</span> <span class="co">## random poetry lines:</span>
<span class="co"># For the black bat, night, has flown,</span>
<span class="co"># The other's fate, Gaville, still dost rue.</span>
<span class="co"># That make me mad. Oh, save me from those eyes!</span>
<span class="co"># eyes, "w'y don't you talk straight out from the</span>
<span class="co"># Make all there is in love so true.</span>
<span class="co"># But p'r'aps I couldn't.</span>
<span class="co"># And the soft ground turned to gravel,</span>
<span class="co"># "'We will fight in bloody scuffle.'"</span>
<span class="co"># For the fever'd dreams on thy rest that throng!"</span>
<span class="co"># It is strange--my heart is heavy,</span>
<span class="fu">du</span> -h gutenberg-poetry-v001.txt<span class="kw">;</span> <span class="fu">wc</span> gutenberg-poetry-v001.txt
<span class="co"># 117M  gutenberg-poetry-v001.txt</span>
<span class="co"># 3085117  21959786 121730091 gutenberg-poetry-v001.txt</span></code></pre></div>
<p>There is an additional step before beginning training. <span class="smallcaps-auto">GPT-2-117M</span> works with text in a “byte-pair encoding”, which is somewhere in between a character embedding &amp; a word embedding. The point of this <span class="smallcaps-auto">BPE</span> encoding is that it is somewhat more efficient than raw characters, because it can chunk more common sub-words or phrases &amp; this gets more complete words or phrases into the Transformer’s fixed ‘window’ of <em>n</em> symbols, but <span class="smallcaps-auto">BPE</span> still assigns symbols to individual letters, and thus arbitrary outputs can be generated, unlike word-level NNs which are more compact but trade this off by having a restricted vocabulary of <em>m</em> words seen in the training corpus and must treat everything else as the unknown token <code>&lt;UNK&gt;</code> (especially bad for rare words like proper names or variants of words like pluralization or tenses). The training code will encode the text corpus at startup if necessary, but for 117MB of text this is so slow that it is worth the extra work to run the encoding process in advance &amp; store the results before training on it:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="va">PYTHONPATH=</span>src <span class="ex">./encode.py</span> gutenberg-poetry-v001.txt gutenberg-poetry-v001.txt.npz</code></pre></div>
</section>
</section>
<section id="training-gpt-2-poetry" class="level1">
<h1><a href="#training-gpt-2-poetry" title="Link to section: §'Training GPT-2-poetry'">Training <code>GPT-2-poetry</code></a></h1>
<figure>
<img alt="" class="float-right" height="1400" loading="lazy" sizes="(max-width: 30em) 80vw, 1030px" src="/images/2019-11-26-gwern-deeplearning-kaibadeafeatmeme-tensorflowupgrading.png" srcset="/images/2019-11-26-gwern-deeplearning-kaibadeafeatmeme-tensorflowupgrading.png-768px.png 768w, /images/2019-11-26-gwern-deeplearning-kaibadeafeatmeme-tensorflowupgrading.png 1030w" title="Yu-Gi-Oh meme: Kaiba vs Yugi. Kaiba attempts to trick Yugi into upgrading his Tensorflow installation, thereby risking breaking it permanently, as CUDA install problems are incomprehensible. Yugi upgrades, only for Kaiba to reveal his plan. However, it fails, as Yugi trains on CPU, accepting the extreme slowdown as the price for reliable software." width="1030"><figcaption>The temptation of <span class="smallcaps-auto">CPU</span> training.</figcaption>
</figure>
<p>I assume you have a fully-working Nvidia <span class="smallcaps-auto">CUDA</span> &amp; <span class="smallcaps-auto">GPU</span>-enabled Tensorflow installation, and have either run other DL code successfully or run the TF installation checklist’s <span class="smallcaps-auto">MNIST</span> toy example to verify that you have working <span class="smallcaps-auto">GPU</span> training. If you do not, I can give you no advice other than “good luck”. Debugging <span class="smallcaps-auto">CUDA</span> problems are the worst, and once you get a working setup, you should stick with it. If you just can’t solve the inscrutable crashes, you should look into using the free Google Colab <span class="smallcaps-auto">GPU</span>/<span class="smallcaps-auto">TPU</span> notebooks, or renting a cloud VM. (Training in the cloud is not as hard or complicated as it may seem, since you can pick a VM OS image which comes with <span class="smallcaps-auto">CUDA</span>/Tensorflow preinstalled and which will always work with the available <span class="smallcaps-auto">GPU</span>s.) You may be tempted to train on <span class="smallcaps-auto">CPU</span> to avoid the <span class="smallcaps-auto">GPU</span>-support mess entirely, but I advise against this: it will be at least 20x slower even if you have a many-core <span class="smallcaps-auto">CPU</span> like Threadripper, and it’ll save you time in the short run to switch to Colab or cloud or some alternative.</p>
<p>Then training proper can begin; my Nvidia 1080ti<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> can fit a minibatch size of 2 (<span class="smallcaps-auto">GPT-2-117M</span> is still a large model), and I’d rather not see too much output so I reduce the frequency of checkpointing &amp; random text generation:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="va">PYTHONPATH=</span>src <span class="ex">./train.py</span> --model_name 117M --dataset gutenberg-poetry-v001.txt.npz <span class="kw">\</span>
    <span class="ex">--batch_size</span> 2 --save_every 10000 --sample_every 1000</code></pre></div>
<div class="admonition warning">
<div class="admonition-title">
Check your <span class="smallcaps-auto">CLI</span> options
</div>
<p>The Python library “fire” used in the OA <span class="smallcaps-auto">GPT-2</span> code is treacherous—<a href="https://github.com/google/python-fire/issues/168">it will <em>not</em> error out or even warn you</a> if you typo a command-line option! Double or triple-check any new options you set against the available arguments defined by <code>train_main</code> in <code>train.py</code>, and keep this gotcha in mind if setting an option doesn’t appear to be doing anything.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> While nshepperd has removed use of “fire” in favor of saner <span class="smallcaps-auto">CLI</span> options, watch out for this if you are using the original OA code or other derivatives.</p>
</div>
<p>Some hyperparameters could use tweaking:</p>
<ol type="1">
<li><p>runtime, <span class="smallcaps">Temperature</span>:</p>
<p>‘Temperature’ (0–∞) is used in sampling: the top-<em>k</em> most likely words are generated, and then selected randomly from; at 0, the most likely word is always chosen, while 1 means each is selected according to its likelihood, and it degenerates to a uniform 1 in <em>k</em> probability with higher values. In other words, the higher the temperature, the more chaotic or unlikely the generated sequences will be.</p>
<p>In the original nshepperd code release, the default temperature setting for the samples during training, 1.0, is not the usual 0.7 everyone uses for <span class="smallcaps-auto">GPT-2</span> prose sampling—although it turns out for poetry we <em>don’t</em> want it at 0.7 as that forces too many repeated lines &amp; 0.9–1 turns out to be much better, so use temperature in that range when generating samples. (Higher still may be better but I have not experimented with &gt;1.)</p>
<p>If you are sampling after 15 May 2019, it may be a better idea to use a new sampling strategy, <a href="https://arxiv.org/abs/1904.09751" class="docMetadata" data-popup-title="The Curious Case of Neural Text Degeneration" data-popup-author="Ari Holtzman, Jan Buys, Maxwell Forbes, Yejin Choi" data-popup-date="2019-08-25" data-popup-abstract="Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.   In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model.   Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence." title="'The Curious Case of Neural Text Degeneration', Holtzman et al 2019">“nucleus sampling”</a> (which essentially sets a different <em>k</em> at each step to avoid sampling extremely unlikely words and greatly reduces the repetition problem), which can be enabled like <code>--top_p 0.9</code>. (An interesting but untested sampling strategy is <a href="https://trentbrick.github.io/Tail-Free-Sampling/">“tail free sampling”</a>.)</p></li>
<li><p>train time, <span class="smallcaps">Learning Rate</span> (LR):</p>
<p>A key NN hyperparameter as always.</p>
<p>In nshepperd’s code, the Adam <span class="smallcaps-auto">SGD</span> learning rate is left at its TensorFlow default of 0.001, which works initially, but appears to be much too high for this purpose (perhaps because the minibatch is so tiny on 1 <span class="smallcaps-auto">GPU</span>). After training overnight, the loss was not decreasing below 2.5, so I decayed it manually to 0.0001 &amp; resumed training (editing line 136 of <code>train.py</code> to read <code>tf.train.AdamOptimizer(learning_rate=0.001*0.10)</code>), eventually decaying it again (to <code>0.001*0.0001</code>) to get it down to a loss of ~1.95. (nshepperd has since added a <code>--learning_rate</code> option so manual editing of the source is no longer necessary.)</p></li>
</ol>
<section id="gpt-2-poetry-samples" class="level2">
<h2><a href="#gpt-2-poetry-samples" title="Link to section: §'GPT-2-poetry Samples'"><code>GPT-2-poetry</code> Samples</a></h2>
<p>After training <span class="smallcaps-auto">GPT-2-117M</span> an hour or two, a sample</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode Bash collapse"><code class="sourceCode bash"><span class="co"># ...[14999 | 3855.62] loss=3.77 avg=3.62</span>
<span class="co"># ======== SAMPLE 1 ========</span>
<span class="co">#  in her place before</span>
<span class="co"># We could not see her smile.</span>
<span class="co"># If, the dear one there are,</span>
<span class="co"># That thou dost hide thy face--</span>
<span class="co"># In thy own eyes, too, I shall see--</span>
<span class="co"># A gentle smile, and smile.</span>
<span class="co"># And I shall see thee, even as in that place,</span>
<span class="co"># There be whom death shall be.   By some, at least in some,</span>
<span class="co"># Not to them I shall say; wherefore</span>
<span class="co"># I have, to whom I will not say.</span>
<span class="co"># For though I should be sad,</span>
<span class="co"># 'Tis not to see them--no,</span>
<span class="co"># No voice I can convey</span>
<span class="co"># Or voice to me or voice--no,</span>
<span class="co"># In a single case my own.</span>
<span class="co"># They who, in all this state,</span>
<span class="co"># Celestial are.  The world</span>
<span class="co"># Of heaven must have its own:</span>
<span class="co"># And when that thou canst tell--</span>
<span class="co"># That which I cannot tell,</span>
<span class="co"># No matter whose.</span>
<span class="co"># If I should tell thee then--</span>
<span class="co"># That I should tell thee then--</span>
<span class="co"># This man--but this man!</span>
<span class="co"># That I should say to thee,</span>
<span class="co"># That thou shouldst say to thee,</span>
<span class="co"># It cannot be without,</span>
<span class="co"># But that I shall speak to thee--</span>
<span class="co"># Thou knowest not the way.</span>
<span class="co"># Whence no speech!  No tongue!</span>
<span class="co"># In that I am not weak in thy power.</span>
<span class="co"># No, no, I will speak to thee--</span>
<span class="co"># I will speak for thee:</span>
<span class="co"># For if it still do need</span>
<span class="co"># I shall speak a word or an act.</span>
<span class="co"># In that place, far away,</span>
<span class="co"># On this sea, and that place!</span>
<span class="co"># I will tell thee, and I will speak to thee.</span>
<span class="co"># ...</span></code></pre></div>
<p>Overnight samples during training:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode Bash collapse"><code class="sourceCode bash"><span class="co"># ...[115999 | 2375.96] loss=1.10 avg=2.31</span>
<span class="co"># ======== SAMPLE 1 ========</span>
<span class="co"># The sun is gone, and the night is late,</span>
<span class="co"># The lights are few, and the chimneys close,</span>
<span class="co"># And yet the moon hangs lonely</span>
<span class="co"># Above the town, where the tides divide,</span>
<span class="co"># On the shores of the silent tide.</span>
<span class="co"># The tide is still, and the tide is at rest,</span>
<span class="co"># And the sands are many, and many a breast</span>
<span class="co"># And breast of the tide is weary,</span>
<span class="co"># And far away the tide's last foam</span>
<span class="co"># Stands silent under the waves;</span>
<span class="co"># But the restless tide creeps slowly,</span>
<span class="co"># And the tide is strong, and the tide is deep,</span>
<span class="co"># And the sea, like a white-faced phantom,</span>
<span class="co"># Floats motionless over the deep.</span>
<span class="co"># The tide is past, and the tide is at rest,</span>
<span class="co"># And the sands are many, and far apart,</span>
<span class="co"># And the sands are many, and far apart,</span>
<span class="co"># And the sea, like a white-faced phantom,</span>
<span class="co"># Floats motionless over the sea.</span>
<span class="co"># 'Tis evening, and he comes to me,</span>
<span class="co"># With scent of flowers, from the village lane;</span>
<span class="co"># And the wind is blowing faint and low,</span>
<span class="co"># And the dew, like tears, on the hill-tops pale,</span>
<span class="co"># Is drifting faint and cold from the eastern sky,</span>
<span class="co"># As he floats adown the western wave;</span>
<span class="co"># And the trees that bend their branches sigh,</span>
<span class="co"># And the winds that waft a sigh,</span>
<span class="co"># Are fading dimly away,</span>
<span class="co"># As the tide goes wandering,</span>
<span class="co"># Tossing waves of sand.</span>
<span class="co"># And the cloud that floats and swells</span>
<span class="co"># Has passed, like the wave on the bay,</span>
<span class="co"># And the moon is slowly sinking low,</span>
<span class="co"># As he sinks adown the western bay.</span>
<span class="co"># There are waters blue and bright</span>
<span class="co"># On the breast of the silent sea;</span>
<span class="co"># And the foam is like a bright</span>
<span class="co"># And lingering pearl of the sea.</span>
<span class="co"># Oh, I would my heart might still</span>
<span class="co"># Ever be where the waves are free.</span>
<span class="co"># Where the foam-flakes fall and swell,</span>
<span class="co"># Falling into the silent sea;</span>
<span class="co"># Where the foam-flakes fall and swell,</span>
<span class="co"># Falling into the silent sea;</span>
<span class="co"># Where the foam-flakes fall and swell</span>
<span class="co"># Falling into the silent sea.</span>
<span class="co"># Oh, I would my heart might still</span>
<span class="co"># Ever be where the waves are free.</span>
<span class="co"># Far away from the sea and the tide,</span>
<span class="co"># In a quiet little bay,</span>
<span class="co"># Where the foam-flakes fall and swell,</span>
<span class="co"># And the rocks are telling of a day</span>
<span class="co"># When I would my heart could stay;</span>
<span class="co"># Where the foam-flakes fall and swell</span>
<span class="co"># Falling into the silent sea;</span>
<span class="co"># Where the foam-flakes fall and swell</span>
<span class="co"># Falling into the silent sea.</span>
<span class="co"># Oh, I would my heart might still</span>
<span class="co"># Ever be where the waves are free.</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a passion never still,</span>
<span class="co"># With a memory never still</span>
<span class="co"># For a love that never will,</span>
<span class="co"># With a sorrow never still</span>
<span class="co"># For a love that never will,</span>
<span class="co"># With a passion never still,</span>
<span class="co"># With a passion never still,</span>
<span class="co"># With a passionate love that never will,</span>
<span class="co"># With a sorrow never still</span>
<span class="co"># For a love that never will.</span>
<span class="co"># Oh, I would my heart might still</span>
<span class="co"># Ever be where the waves are free.</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a passion never still,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a passion never will,</span>
<span class="co"># With a passion never will,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a love that never will.</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a passion never will,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a grief that never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a love that never will.</span>
<span class="co"># Oh, I would my heart might still</span>
<span class="co"># Ever be where the waves are free.</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never will.</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a love that never will.</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a sorrow never will.</span>
<span class="co"># With a love that never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never still,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a sorrow never will,</span>
<span class="co"># With a grief that never</span>
<span class="co"># [116000 | 2385.67] loss=3.33 avg=2.32</span>
<span class="co"># ...</span>
<span class="co"># [133999 | 5919.61] loss=2.89 avg=2.21</span>
<span class="co"># ======== SAMPLE 1 ========</span>
<span class="co">#</span>
<span class="co"># "When the night is gone and the sun goes out,</span>
<span class="co"># "There goes my bride and my lover:</span>
<span class="co"># "Can she never come to her wedding-ring</span>
<span class="co"># "With the light of the seven candles?</span>
<span class="co"># "Oh! to bed with the bridegroom and bride,</span>
<span class="co"># "Or to lie with my lover!</span>
<span class="co"># "He shall not come to my bed at night</span>
<span class="co"># "For a little while longer.</span>
<span class="co"># "She shall not come to my chamber</span>
<span class="co"># "For a little longer."</span>
<span class="co"># The night is spent, the sun has gone;</span>
<span class="co"># The bridal of the bride hath done;</span>
<span class="co"># The bridal of the bridegroom comes</span>
<span class="co"># To wed the bridegroom's attour.</span>
<span class="co"># I come to see the wedding-feast.</span>
<span class="co"># He cries to me--"Wilt seek the bride,</span>
<span class="co"># "Wilt seek the bridal?"</span>
<span class="co"># "Oh! I have sought her mother's bower</span>
<span class="co"># "And never found her! oh! my flower!</span>
<span class="co"># "Would we should love as brides should do!</span>
<span class="co"># "I shall not find her!"</span>
<span class="co"># The bridegroom at the bridegroom's door</span>
<span class="co"># Gave his bride a ring and a prayer.</span>
<span class="co"># "Now, bridegroom, sing a bridegroom loud!</span>
<span class="co"># "I shall not find her!"</span>
<span class="co"># The bridegroom at the bridegroom's door</span>
<span class="co"># Gave his bride a ring and a prayer.</span>
<span class="co"># What means the bridegroom or the bride?</span>
<span class="co"># The bridegroom's bridegroom waits to ride.</span>
<span class="co"># And looks with wonder at the bride;</span>
<span class="co"># "And does she dream?"</span>
<span class="co"># "Oh! I have dreamed!</span>
<span class="co"># "She dreams of my youth,</span>
<span class="co"># "As one that hears--it cannot be -</span>
<span class="co"># "The story of a marriage vow!"</span>
<span class="co"># And a voice answers:</span>
<span class="co"># "The story of a marriage vow!"</span>
<span class="co"># And the words reach the bridegroom's door</span>
<span class="co"># As the bridegroom at the bridegroom's door</span>
<span class="co"># Kisses the ring with a bridegroom's kiss on his cheek.</span>
<span class="co"># They are wed! They are wedded!</span>
<span class="co"># Each is in his bridegroom's bower;</span>
<span class="co"># Each hath his bride in his bosom now!</span>
<span class="co"># And each hath his bride in his heart.</span>
<span class="co"># She is wedded!</span>
<span class="co"># With each is the bridegroom's bride!</span>
<span class="co"># And love is a bridegroom's bride!</span>
<span class="co"># She is wedded!</span>
<span class="co"># With each is the bridegroom's bride!</span>
<span class="co"># "They are wedded!</span>
<span class="co"># They are wedded!</span>
<span class="co"># They are wedded!"</span>
<span class="co"># A man from a fair tower, where the birds of the air</span>
<span class="co"># Fairer and fainter and fairer the flowers and the trees,</span>
<span class="co"># When the sweet light fades from the gardens of day he found</span>
<span class="co"># Came he in quest of a maiden, whose form was wildly wild,</span>
<span class="co"># So she came at the summons. He came, and stood by her side;</span>
<span class="co"># And he gazed on the dream of her marvellous face, and smiled;</span>
<span class="co"># And he said:  "I came by the river's side, when the day was still</span>
<span class="co"># And I think of my bridegroom--he--whom ye all have a will!</span>
<span class="co"># She is wedded!</span>
<span class="co"># And he loves me not, my bride!</span>
<span class="co"># And he looks at her eyes with a love that seems to him divine,</span>
<span class="co"># Then he pines, and thinks of her eyes with an inward passionate shine</span>
<span class="co"># They are wed! They are wedded!</span>
<span class="co"># By the river's side they are, bridegroom, led!"</span>
<span class="co"># He hath summoned her maiden with the will, and she answers</span>
<span class="co"># That he needs her for his bridegroom still.</span>
<span class="co"># A man from a fair tower, where the birds of the air</span>
<span class="co"># Fairer and fainter and fairer the flowers and the trees,</span>
<span class="co"># What is the meaning of that--no--is it, Lord, where ye bear</span>
<span class="co"># All the wonders of the world in this fair maiden's hair?</span>
<span class="co"># What doth the meaning of that--no--nor what doth she know,</span>
<span class="co"># She is wedded!</span>
<span class="co"># What doth the meaning of that--no--she hath chosen so?</span>
<span class="co"># She is wedded!</span>
<span class="co"># She is wedded!</span>
<span class="co"># She is wedded!</span>
<span class="co"># She is wedded!</span>
<span class="co"># She is wedded!</span>
<span class="co"># She is wedded!</span>
<span class="co"># She is wedded!</span>
<span class="co"># And the will is good,</span>
<span class="co"># In the light of days,</span>
<span class="co"># In the heat and stress,</span>
<span class="co"># It is wise and wise</span>
<span class="co"># To be wedded!</span>
<span class="co"># She is wedded!</span>
<span class="co"># And the love is good,</span>
<span class="co"># In the light of days,</span>
<span class="co"># It is hard to live</span>
<span class="co"># And bear the wreath!</span>
<span class="co"># And he girdeth her to his bosom, and thinks of her</span>
<span class="co"># As she stands, all white, in the open air,</span>
<span class="co"># With the full moon facing her face; and her breath</span>
<span class="co"># Cries and whispers and wails and cries</span>
<span class="co">#</span>
<span class="co"># ...[134000 | 5929.46] loss=2.12 avg=2.21</span>
<span class="co">#</span>
<span class="co"># ...[135998 | 6934.07] loss=1.96 avg=2.16</span>
<span class="co"># [135999 | 6934.58] loss=3.06 avg=2.17</span>
<span class="co"># ======== SAMPLE 1 ========</span>
<span class="co"># re</span>
<span class="co"># Their love and beauty are as one in dream,</span>
<span class="co"># A visible sign of the things that were.</span>
<span class="co"># But I have seen these things by all men's eyes,</span>
<span class="co"># Felt them as kindred of man's earthly life,</span>
<span class="co"># And with the instinct of the unseen spirit</span>
<span class="co"># Been caught by likeness of the thing he is.</span>
<span class="co"># My love is but the wind, and I have blown</span>
<span class="co"># From earth to where I am, and I have seen</span>
<span class="co"># The things that no man dreamed of; yet at last</span>
<span class="co"># I know by my soul's sense of a sense of things</span>
<span class="co"># That are not, and may be, but the things that were;</span>
<span class="co"># And yet I know these things are not, but are</span>
<span class="co"># As earth and heaven, if earth and heaven and hell</span>
<span class="co"># Are but the same things that it seems. Yea, then</span>
<span class="co"># I am the wind. God knows the ways of men,</span>
<span class="co"># He knows the insensate secrets of delight,</span>
<span class="co"># And they are mysteries, if there be any to be seen."</span>
<span class="co"># But with that word the wind in wonder strode.</span>
<span class="co"># He heard the rustle of the leaves, and saw</span>
<span class="co"># The shadows move about him, and he leaned</span>
<span class="co"># Against the doorway like a god, and knew</span>
<span class="co"># The inner meanings of the leaves and streams.</span>
<span class="co"># There where the trees lie down at their root-holes,</span>
<span class="co"># There where the wind smells of the blossoming boughs,</span>
<span class="co"># He saw the grass, and felt the green blades come,</span>
<span class="co"># As if it were the buds and boughs upon air,</span>
<span class="co"># And heard the green birds sing. He saw the fields,</span>
<span class="co"># The trees, the rivers, and the flowers within,</span>
<span class="co"># The birds, the grasses, and the living things,</span>
<span class="co"># And the strange river on the shore that rolls</span>
<span class="co"># Through all its quiet marge into the sky.</span>
<span class="co"># There let him live till time should come, and then</span>
<span class="co"># Let love be like the heaven, and we be one</span>
<span class="co"># To love, and not be one, being all in all.</span>
<span class="co"># And if he had not done me the good work</span>
<span class="co"># Had it been well not I. The things that he said</span>
<span class="co"># Should never be fulfilled by simple sense;</span>
<span class="co"># For all must have a meaning in themselves.</span>
<span class="co"># But he that works out of his mind is one</span>
<span class="co"># With whom the things that are and are not are,</span>
<span class="co"># And makes them meet and good. 'T were a good thing</span>
<span class="co"># For him to work and win for me, and so</span>
<span class="co"># If he were not I would have it all.'</span>
<span class="co"># But he that lives and not lives in the world</span>
<span class="co"># Was not more worthy of the hand of Fate,</span>
<span class="co"># And knows life's meaning, and would seek for it</span>
<span class="co"># Through failure, and in death's despite. For him,</span>
<span class="co"># Who hath been stricken with me through the brain,</span>
<span class="co"># Forget to tell me how his brother, he</span>
<span class="co"># Whom he had saved and murdered--so let it be</span>
<span class="co"># By some great memory left.</span>
<span class="co"># But at last,</span>
<span class="co"># As I said this, he saw me, and he said</span>
<span class="co"># To one, whose face was grey with tears in me,</span>
<span class="co"># "What is it? let me tell you who I am.</span>
<span class="co"># Do you see the things that you have seen before?</span>
<span class="co"># What is it?"</span>
<span class="co"># "They are more wise</span>
<span class="co"># Than wise men think of wisdom and good will,"</span>
<span class="co"># Replied the other. "What I deem is good.</span>
<span class="co"># The gods are good to mortals as they are,</span>
<span class="co"># And they know well whereby we are born: but they</span>
<span class="co"># Who have loved God and died to him the most</span>
<span class="co"># Of all the gods are fallen into ill things:</span>
<span class="co"># For God we know is good, and hath not been,</span>
<span class="co"># And therefore must be, so it be, with men</span>
<span class="co"># Who love, and love because we loved them not.</span>
<span class="co"># Alas, I do not think that God alone</span>
<span class="co"># Hath power over the earth to let the gods</span>
<span class="co"># Face to face with the world. I hate at times</span>
<span class="co"># The gods that made them: the gods that knew</span>
<span class="co"># Their names are our own gods, and would not know</span>
<span class="co"># One other reason, for I have the power,</span>
<span class="co"># And all the gods are fallen into ill things."</span>
<span class="co"># Then she said to me, "What may have been</span>
<span class="co"># To have known, before I came into this land</span>
<span class="co"># To find you in some other place and knew you,</span>
<span class="co"># And know you, seeing so many and strange,</span>
<span class="co"># And knowing such a godlike way to go</span>
<span class="co"># Among the gods and suffer such long-sought.</span>
<span class="co"># I can take my crown of gold and wear a garland,</span>
<span class="co"># Take some crown for my sake, and the happy crown</span>
<span class="co"># And let it be for all the years long held</span>
<span class="co"># That I have known, and felt so like a god</span>
<span class="co"># Some few suns live. My heart is all in all</span>
<span class="co"># To live again, my life upon earth dead."</span>
<span class="co"># So I said to the god that loved me well</span>
<span class="co"># And longed to have him come back into my prayers,</span>
<span class="co">#</span>
<span class="co"># [136000 | 6944.39] loss=3.15 avg=2.18</span>
<span class="co"># ...</span></code></pre></div>
<p>The loss here is the usual cross-entropy we often see in architectures like a char-<span class="smallcaps-auto">RNN</span>. Typically, the best text generation results come when the model has trained down to a cross-entropy of &lt;1, and 2–4 tend to be incoherent gibberish. (For example, in <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="docMetadata" data-popup-title="The Unreasonable Effectiveness of Recurrent Neural Networks" data-popup-author="Andrej Karpathy" data-popup-date="2015-05-21" data-popup-abstract="<p>[Exploration of char-<span class=&quot;smallcaps-auto&quot;>RNN</span> neural nets for generating text. Karpathy codes a simple recurrent NN which generates character-by-character, and discovers that it is able to generate remarkably plausible text (at the syntactic level) for Paul Graham, Shakespeare, Wikipedia, LaTeX, Linux C code, and baby names&amp;mdash;all using the same generic architecture. Visualizing the internal activity of the char-<span class=&quot;smallcaps-auto&quot;>RNN</span>s, they seem to be genuinely understanding some of the recursive syntactic structure of the text in a way that other text-generation methods like n-grams cannot. Inspired by this post, I began <a href=&quot;/RNN-metadata&quot;>tinkering with char-<span class=&quot;smallcaps-auto&quot;>RNN</span>s for poetry</a> myself; as of 2019, char-<span class=&quot;smallcaps-auto&quot;>RNN</span>s have been largely obsoleted by the new <a href=&quot;/GPT-2&quot;><em>Transformer architecture</em></a>, but recurrency will make a comeback and Karpathy's post is still a valuable and fun read.]</p><p>There’s something magical about Recurrent Neural Networks (<span class=&quot;smallcaps-auto&quot;>RNN</span>s). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that <span class=&quot;smallcaps-auto&quot;>RNN</span>s were supposed to be difficult to train (with more experience I’ve in fact reached the opposite conclusion). Fast forward about a year: I’m training <span class=&quot;smallcaps-auto&quot;>RNN</span>s all the time and I’ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.<em>We’ll train <span class=&quot;smallcaps-auto&quot;>RNN</span>s to generate text character by character and ponder the question “how is that even possible?”</em></p>" title="The Unreasonable Effectiveness of Recurrent Neural Networks">Andrej Karpathy’s Tiny Shakespeare</a>.) That loss is per character, while <span class="smallcaps-auto">GPT-2</span> operates on <span class="smallcaps-auto">BPE</span>s, which usually encode multiple characters, so are harder to predict; it seems to me that the conversion factor is ~2–3, so a <span class="smallcaps-auto">GPT-2</span> model should aim for a loss of &lt;2 if a good char-<span class="smallcaps-auto">RNN</span> would reach losses like &lt;1. In this case, <span class="smallcaps-auto">GPT-2-117M</span>’s original poetry modeling capability is not too shabby (as demonstrated by the various prompted samples), and it shows decent poetry samples starting ~3.5. (Gibberish seems to set in at losses &gt;6.) Given how large &amp; powerful <span class="smallcaps-auto">GPT-2-117M</span> is, even with this much poetry to work with, overfitting remains a concern—memorizing poetry is not amusing, we want creative extrapolation or mashups.</p>
<p>For this model &amp; dataset, I trained for 519,407 steps to a final loss of ~2 in 72 <span class="smallcaps-auto">GPU</span>-hours; almost all of the learning was achieved in the first ~16 <span class="smallcaps-auto">GPU</span>-hours, and training it additional days did not do any apparent good in terms of the loss itself.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> This suggests that <span class="smallcaps-auto">GPT-2</span>-poetry was underfitting the poetry corpus &amp; would benefit from an even larger model size.</p>
<p>Results:</p>
<ul>
<li><a href="https://mega.nz/#!HXhRwS7R!yl4qZM-gMWdn4Qc3scavOBKqdLNAcZ_WYd2gVPqabPg"><span class="smallcaps-auto">GPT-2</span>-poetry model download</a> (430MB)</li>
<li><a href="/docs/ai/poetry/2019-03-06-gpt2-poetry-1000samples.txt"><span class="smallcaps-auto">GPT-2</span>-poetry, 1000 unconditional generation samples</a> (3.6MB)</li>
</ul>
<p>Before sampling from any new finetuned version of <span class="smallcaps-auto">GPT-2-117M</span>, remember to copy <code>encoder.json</code>/<code>hparams.json</code>/<code>vocab.bpe</code> from the 117MB model directory into the new model’s directory. I find higher temperature settings work better for poetry (perhaps because poetry is inherently more repetitive than prose), and top-<em>k</em> appears to work fine at OA’s top-40. So unconditional sampling can be done like this to generate 2 samples:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode Bash collapse"><code class="sourceCode bash"><span class="ex">python</span> src/generate_unconditional_samples.py --top_k 40 --temperature 0.9 --nsamples 2 --seed 0 <span class="kw">\</span>
    <span class="ex">--model_name</span> 2019-03-06-gwern-gpt2-poetry-projectgutenberg-network519407
<span class="co"># ======================================== SAMPLE 1 ========================================</span>
<span class="co"># --</span>
<span class="co"># And it must be decided.</span>
<span class="co"># It must be decided,</span>
<span class="co"># And it must be decided.</span>
<span class="co"># It must be decided,</span>
<span class="co"># And it must be considered.</span>
<span class="co"># It will be decided,</span>
<span class="co"># Though the hill be steep,</span>
<span class="co"># And the dale and forest</span>
<span class="co"># Hold the land of sheep.</span>
<span class="co"># And it must be decided,</span>
<span class="co"># There's a jolt above,</span>
<span class="co"># And its paths are narrow,</span>
<span class="co"># And its paths are long.</span>
<span class="co"># Yes, it is decided,</span>
<span class="co"># And it is completely.</span>
<span class="co"># All the hills are covered</span>
<span class="co"># With grey snowdrifts,</span>
<span class="co"># Shaded with a shimmer of misty veils,</span>
<span class="co"># And the hills have a shimmer of hills between,</span>
<span class="co"># And the valleys are covered with misty veils,</span>
<span class="co"># And there lie a vast, grey land, like a queen,</span>
<span class="co"># And they are not, in truth, but many and many streams,</span>
<span class="co"># O'er the purple-grey sea whose waves are white</span>
<span class="co"># As the limbs of a child of ten.  And there</span>
<span class="co"># The river stands, like a garden-fair</span>
<span class="co"># In the valleys of the north, the valleys of the west,</span>
<span class="co"># Blue and green in the summer, and runneth softly forth</span>
<span class="co"># To the blue far upland beyond the sea;</span>
<span class="co"># And over the high white upland far away</span>
<span class="co"># Floats a white and tender water, and wearily</span>
<span class="co"># Through the trees the rosiest water-lilies play</span>
<span class="co"># In the sun, and rise and fall--the purple and red</span>
<span class="co"># Of the streams.  The waters are hidden in their bed</span>
<span class="co"># By the stone o'er the darkling hills.  The waters run</span>
<span class="co"># Like a ringlet under the stone.  The water flows</span>
<span class="co"># Through the rocks like a river, and the stream</span>
<span class="co"># Is a ribbon of gold spun by the sun.  It gleams</span>
<span class="co"># Like a gold sunbeam shining through the gleam</span>
<span class="co"># Of a sudden silver, and silently falls</span>
<span class="co"># On the pool, and is lost in the darkling deeps--</span>
<span class="co"># Sink, sink in the shadows, ere it flee</span>
<span class="co"># Into the darkling depths.  And the waters sleep</span>
<span class="co"># In the light of the moon and the silver of dawn,</span>
<span class="co"># And silently float past the mountains of heaven.</span>
<span class="co"># As we gazed the city fades into the clouds</span>
<span class="co"># Of the sky, and we are above the roofs.</span>
<span class="co"># And suddenly as the moon, flurrying,</span>
<span class="co"># Dazzles the sea with her swan-throated song,</span>
<span class="co"># And there is a faint far singing of birds,</span>
<span class="co"># And a sound from the land, as of swarming seas,</span>
<span class="co"># The grey sea, and the land that hideth rest,</span>
<span class="co"># And the sky that hides the lovely green of God.</span>
<span class="co"># So we are caught, like the moving sea,</span>
<span class="co"># That calleth unto its sleeping</span>
<span class="co"># Soft and still, like the moon that calleth</span>
<span class="co"># In the twilight depths vast and hoary--</span>
<span class="co"># Till we see the City changing toward the dark,</span>
<span class="co"># And its changing towers in the distance darken.</span>
<span class="co"># In the city is a calm and quiet street,</span>
<span class="co"># Full of sunlight, and a smell of rain,</span>
<span class="co"># That falls from unseen towers like soft white feet</span>
<span class="co"># On sleeping city's rue and misty pane.</span>
<span class="co"># There is peace, and a vague peace over death,</span>
<span class="co"># And a far-off singing in the city's breath.</span>
<span class="co"># And all fair cities must go to dust,</span>
<span class="co"># And every body be one tomb--</span>
<span class="co"># And all white houses dwindle and grow dull,</span>
<span class="co"># And the city's breath is a dull death-blow.</span>
<span class="co"># But this place is a place of peace and trust,</span>
<span class="co"># And it is but a little street,</span>
<span class="co"># Whose idle heads and sunken faces</span>
<span class="co"># Are bright with light that makes them bright.</span>
<span class="co"># Then it is not alone fair Town that lies,</span>
<span class="co"># With open pillared streets beneath a sun,</span>
<span class="co"># And many a weary world and dusty town,</span>
<span class="co"># And a sunflowers and a great tide onward run</span>
<span class="co"># In the blue of the heavens that are not gray,</span>
<span class="co"># But only blue and pale, like tender wings</span>
<span class="co"># Sailing with wide-spread, languid, luminous eyes.</span>
<span class="co"># This place is the very heart of it,</span>
<span class="co"># Whose quiet hours with its peace throng</span>
<span class="co"># The silent nights and the perpetual sea.</span>
<span class="co"># The City slept with her silent towers,</span>
<span class="co"># A stream that ran in an idle stream,</span>
<span class="co"># And a mist hung at the windows of the tower.</span>
<span class="co"># And it was a street--a sunlit dream,</span>
<span class="co"># A dream of a world that lay</span>
<span class="co"># Open in the summer morning,</span>
<span class="co"># And in its heart a joy all gay.</span>
<span class="co"># For its sunshines and palaces were there,</span>
<span class="co"># Till a wind came softly here.</span>
<span class="co"># And it was a new, new city,</span>
<span class="co"># A city that arose in the early morning;</span>
<span class="co"># That opened its gates on June morning,</span>
<span class="co"># With a sunset and a moonrise sweet.</span>
<span class="co"># The city was a cathedral;</span>
<span class="co"># And out of the sound of the bells and t</span>
<span class="co"># ======================================== SAMPLE 2 ========================================</span>
<span class="co">#  of the world</span>
<span class="co"># The best, that, when once dead, is found again.</span>
<span class="co"># And what is this?  Where can we find a place,</span>
<span class="co"># Save in the solitude, where he may be</span>
<span class="co"># The friend of all beneath the sun, and be</span>
<span class="co"># An unseen presence, if the traveller's eye</span>
<span class="co"># Can follow where he cannot:  there he stands</span>
<span class="co"># Dark in majestic pomp, like those whom owls</span>
<span class="co"># Could once have told down with a lion's maw.</span>
<span class="co"># His form is like his fathers, and the crown</span>
<span class="co"># Of all his race:  the very colours are</span>
<span class="co"># As his to-day, which we must see and bear;</span>
<span class="co"># The only parent is the creature's he.</span>
<span class="co"># His face, where we have marked it, is but veiled</span>
<span class="co"># In twilight, when we see, and he appears</span>
<span class="co"># Himself in all his nature--where, if man</span>
<span class="co"># Can recollect, he saw it in the frame:</span>
<span class="co"># 'Tis clay wherever found--and so is called,</span>
<span class="co"># When nature gives him back her clay.  It means</span>
<span class="co"># That clay was form'd; but clay is form'd elsewhere;</span>
<span class="co"># He needs must feel through all this frame, and, lo,</span>
<span class="co"># The horse he rears, is human in his mind.</span>
<span class="co"># So too, his nature is a thing apart</span>
<span class="co"># From the great Nature, which has made him thus</span>
<span class="co"># A likeness of himself:  and he beholds</span>
<span class="co"># The creatures that he knows, and not intends</span>
<span class="co"># To visit them, and only in their hearts</span>
<span class="co"># Deserts them; and if they come indeed,</span>
<span class="co"># And if the sea doth bring them, then the man</span>
<span class="co"># Is still a child of theirs.  He can recall</span>
<span class="co"># His mother's features and the father's look.</span>
<span class="co"># And often he has said that he foresaw</span>
<span class="co"># The sea, the winds, that he may all at will</span>
<span class="co"># Be sea.  In short, the man is all he sees.</span>
<span class="co"># He fears the sea may hurt him.</span>
<span class="co"># Lashed to the helm,</span>
<span class="co"># The ship was in the sea, and, on its moor</span>
<span class="co"># And the sails furled, in silence sat the maid</span>
<span class="co"># Motionless, like a star; no sound was heard</span>
<span class="co"># Save of the distant ocean's fitful hum;</span>
<span class="co"># The sounds of tempest came to him, his ears</span>
<span class="co"># Mercurially listless, and his heart</span>
<span class="co"># Disturbed like a distempered sea; he stood,</span>
<span class="co"># And gazed from heaven in an unblest thought;</span>
<span class="co"># He had not heard his mother's voice; he gazed;</span>
<span class="co"># The mother's look was of a loftier mood;</span>
<span class="co"># He had not heard his own; he had not heard</span>
<span class="co"># What ever was, where his own heart has been;</span>
<span class="co"># He had not understood the very thought</span>
<span class="co"># Of his own heart, where life could find no shore.</span>
<span class="co"># The sea beats on:  the vessel's bell strikes six:</span>
<span class="co"># Dive down, O death! to earth, to heaven! to heaven!</span>
<span class="co"># And it is sweet thus to be two souls alone:</span>
<span class="co"># Dive down for home, and to the air renounce</span>
<span class="co"># The galling bonds of everlasting life</span>
<span class="co"># In some lone bark, that, dying, to the last</span>
<span class="co"># Are still as death without her:  so to him,</span>
<span class="co"># The mother's voice, still sweeter, spoke of home;</span>
<span class="co"># And as the young man fell upon her breast,</span>
<span class="co"># The mother's oracle, the words of death,</span>
<span class="co"># Even as he spoke, a living death arose:</span>
<span class="co"># He feels his heart rise, and ascend the sky.</span>
<span class="co"># The wreck shall surely reach the sea; he dies,</span>
<span class="co"># A mortal change, as earth, in which it was;</span>
<span class="co"># And God, though dead, had still a dying man.</span>
<span class="co"># But when they parted, he can never die.</span>
<span class="co"># There are thousands, yes, there are thousands who,</span>
<span class="co"># Without a mother, could not die unheard</span>
<span class="co"># Of by a hand unseen:  yet some are sad,</span>
<span class="co"># Lonely and wretched here, without a mate;</span>
<span class="co"># Or if the grave touch, the great hearts' light</span>
<span class="co"># Have no soft touch, even of a brother's grief</span>
<span class="co"># Scarce suffered, they shall each a new life yield;</span>
<span class="co"># And one, once more on earth, to heaven, or God,</span>
<span class="co"># Shall meet his father's face, or bless his grave.</span>
<span class="co"># Not vainly on these mocking thoughts he breathes;</span>
<span class="co"># They sink to nothing when he sinks to rise:</span>
<span class="co"># The tears of fatherly compassion reach</span>
<span class="co"># The mother's eyelids, her, but not her eyes.</span>
<span class="co"># And now a voice was heard by the wild bird,</span>
<span class="co"># With words of comfort from the infant boy.</span>
<span class="co"># Oh, had it stayed the angel's birth, and then</span>
<span class="co"># Those tresses streaming, would have felt the strain</span>
<span class="co"># For the bright star, and for a glorious man.</span>
<span class="co"># It is a noble deed:  and, through the world,</span>
<span class="co"># Doth woman triumph, though she suffer loss</span>
<span class="co"># And poverty and pain, and,</span></code></pre></div>
<p>Not bad.</p>
</section>
<section id="cleaning-project-gutenberg-contemporary-poetry" class="level2">
<h2><a href="#cleaning-project-gutenberg-contemporary-poetry" title="Link to section: §'Cleaning Project Gutenberg &amp; Contemporary Poetry'">Cleaning Project Gutenberg &amp; Contemporary Poetry</a></h2>
<div class="epigraph">
<blockquote>
<p>“In the dark the sun doth gleam,<br>And in the dark the moon doth seem<br>But now the evening is begun—<br>Gone is the sun upon the earth!<br>The silver moon doth like a cup<br>Of blood-red wine, and as that cup<br>Is drained of life, doth quench no drop.<br>What man will drink such wine?”</p>
<p><span class="smallcaps-auto">GPT-2</span></p>
</blockquote>
</div>
<p><a href="https://twitter.com/theshawwn">Shawn Presser</a> noted the issues with the Project Gutenberg corpus and, as book-level transitions are solved, suggested a heuristic for reconstructing the blank lines denoting (presumably) stanzas: use the <em>line numbers</em> in the original <span class="smallcaps-auto">JSON</span> and look for gaps in the transitions. Since stanzas are still connected, <code>&lt;|endoftext|&gt;</code> is used for the book-level transitions, and a blank line is used for the stanza-level, preserving as much of the semantics as possible.</p>
<p>Presser’s Python script:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode Python collapse"><code class="sourceCode python"><span class="im">import</span> json
<span class="im">import</span> os
<span class="im">import</span> sys

<span class="kw">def</span> split_by_book(poems):
  prev_gid <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>
  r <span class="op">=</span> []
  <span class="cf">for</span> entry <span class="kw">in</span> poems:
    gid <span class="op">=</span> entry[<span class="st">'gid'</span>]
    <span class="cf">if</span> gid <span class="op">!=</span> prev_gid:
      <span class="cf">if</span> prev_gid <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span> <span class="kw">and</span> <span class="bu">len</span>(r) <span class="op">&gt;</span> <span class="dv">0</span>:
        <span class="cf">yield</span> r
        r <span class="op">=</span> []
      prev_gid <span class="op">=</span> gid
    r <span class="op">+=</span> [entry]
  <span class="cf">if</span> <span class="bu">len</span>(r) <span class="op">&gt;</span> <span class="dv">0</span>:
    <span class="cf">yield</span> r

<span class="kw">def</span> split_by_transition(poems):
  prev_line <span class="op">=</span> <span class="op">-</span><span class="dv">2</span>
  r <span class="op">=</span> []
  <span class="cf">for</span> entry <span class="kw">in</span> poems:
    line <span class="op">=</span> entry[<span class="st">'line'</span>]
    <span class="cf">if</span> line <span class="op">!=</span> prev_line <span class="op">+</span> <span class="dv">1</span>:
      <span class="cf">if</span> prev_line <span class="op">!=</span> <span class="op">-</span><span class="dv">2</span> <span class="kw">and</span> <span class="bu">len</span>(r) <span class="op">&gt;</span> <span class="dv">0</span>:
        <span class="cf">yield</span> r
        r <span class="op">=</span> []
    prev_line <span class="op">=</span> line
    r <span class="op">+=</span> [entry]
  <span class="cf">if</span> <span class="bu">len</span>(r) <span class="op">&gt;</span> <span class="dv">0</span>:
    <span class="cf">yield</span> r

<span class="kw">def</span> extract(entries):
  <span class="cf">return</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(<span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: x[<span class="st">'s'</span>], entries)))

<span class="kw">def</span> final(poems):
  <span class="cf">for</span> book <span class="kw">in</span> split_by_book(poems):
    <span class="cf">yield</span> <span class="st">'&lt;|endoftext|&gt;'</span>
    <span class="cf">for</span> stanza <span class="kw">in</span> split_by_transition(book):
      <span class="cf">if</span> <span class="bu">len</span>(stanza) <span class="op">&gt;</span> <span class="dv">5</span>:
        <span class="cf">yield</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>
        <span class="cf">for</span> entry <span class="kw">in</span> stanza:
          line <span class="op">=</span> entry[<span class="st">'s'</span>] <span class="co"># {</span>
          line <span class="op">=</span> line.rstrip(<span class="st">'} '</span>)
          <span class="cf">yield</span> line

<span class="kw">def</span> output(poems, fname):
  <span class="cf">with</span> <span class="bu">open</span>(fname, <span class="st">"w"</span>) <span class="im">as</span> f:
    <span class="cf">for</span> line <span class="kw">in</span> final(poems):
      f.write(line <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)

<span class="kw">def</span> main():
  <span class="bu">print</span>(<span class="st">'Loading poems...'</span>)
  <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"gutenberg-poetry-v2.ndjson"</span>) <span class="im">as</span> f:
    poems<span class="op">=</span>[json.loads(line) <span class="cf">for</span> line <span class="kw">in</span> f]
  <span class="bu">print</span>(<span class="st">'Loaded.'</span>)
  output(poems, <span class="st">"corpus.txt"</span>)

<span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:
  main()</code></pre></div>
<p>This is converted to the <span class="smallcaps-auto">NPZ</span> and trained as usual. I retrained the previous non-prefix 117M PG poetry model for ~30k steps (&gt;16h?) down to a loss of ~1.73. (I used 117M instead of 345M for compatibility with my concurrent experiment in preference-learning training.)</p>
<p>The results are quite nice, and competitive even with <a href="#gpt-2-345m">345M</a>. Some selected samples:</p>
<pre><code>The gods are they who came to earth
And set the seas ablaze with gold.
There is a breeze upon the sea,
A sea of summer in its folds,
A salt, enchanted breeze that mocks
The scents of life, from far away
Comes slumbrous, sad, and quaint, and quaint.
The mother of the gods, that day,
With mortal feet and sweet voice speaks,
And smiles, and speaks to men:  "My Sweet,
I shall not weary of thy pain."

...Let me drink of the wine of pain
And think upon the agonies of hope,
And of the blessed Giver of all good things;
For, man for man, mine is the deepest love
That sorrow takes upon the humblest soul;
But who hath learned how sorrow turns to gall
The places where my feet have trod before.

...And 'stead of light, o'er earth, o'er rocky mountains,
A slowly falling star,
Its pointed pointed splendor far uplifting,
Heaven's flowery path bore down;
Each cranny of the air a gracious feeling,
It waved divinely round,
It called us hence, "Come what wouldst thou here?"--
Sweet mountain, that I love,
With that bright tint of heaven above,
'Twould make me still to see
One like to thee,
As fades the light that seeks the wandering eye.

...The skies are smiling sweetly on,
And summer's fairest hours are gone.
Oh, blessed Mercy! how the blest
Taste life itself can truly taste.
Thy morn of days, with all its past,
May on life's tempest paint the last.

...When you come to die,
Every nerve and bone
Soon lulled in sleep,
Secure and free,
Sleep will seize on you.
When you come to die,
Every nerve and bone
Soon lulled in sleep,
Sleep will seize on you.
When you come to die,
Every nerve and bone
Soon lulled in sleep,
We'll still be free,
And you'll never escape from our woe!

...I would be all that I can do
And this to carry with me
Along with me, O brother,
And bid my lagging days relent
For every worthy deed done,
And glorious though the world be,
They never will repent me,
But in God's name endureth ever,
Whose blessed hope my soul abides
For refuge through the awful doors of death.

...We are old men, who pass
On the sands with gaze
Out of the narrow world of fashion;
We are old men, who stay
On a river's flow
And a common day
Where the life of youth is waiting,
And a longing grows
For the world of youth and beauty
Where the old man goes.

...When I am dead, my dearest,
Sing no sad songs for me;
Plant thou no roses at my head,
That by that token may grow cold.
My dirge shall be a muffled noise,
My trentals stiff with dread,
For he who once his faith hath won
Will never know it read.

...O beautiful, golden-bosomed ships!
O sunburned ships on the sea; O ship which breams
Above the waves and beams; O songs of love
Sent from the wide West, that shall sing us songs
In our hearts afar, as a summer star.</code></pre>
<p>While training that, I recalled that my other major beef with the PG corpus was its absence of more contemporary poetry. I didn’t really feel like trying to scrape the Poetry Foundation (PF) website myself, but I gave Google Dataset Search another try and to my surprise, discovered <a href="https://www.kaggle.com/tgdivy/poetry-foundation-poems/version/1">a scrape had surfaced on Kaggle</a>. Aside from being large, it comes with interesting metadata: the title and author, but also a somewhat eccentric set of ‘tags’ describing the poem. They would be nice to add inline, allowing some degree of controllability (like my use of author or book ID prefixes, or <span class="smallcaps-auto">CTRL</span>’s use of subreddits).</p>
<p>The Kaggle Poetry Foundation scrape has numerous issues I had to fix before its format was acceptable for <span class="smallcaps-auto">GPT-2</span>:</p>
<ul>
<li>I replaced prefixed whitespace, trimming leading/trailing whitespace in all fields</li>
<li>replace 3+ spaces with newlines</li>
<li>deleted all 2+ spaces</li>
<li>dropped poems with &lt;100 characters (generally a scrape error)</li>
<li>remove Unicode junk</li>
<li>serialize it as title+author+tags (if any) / poem / <code>&lt;|endoftext|&gt;</code> (ie the inline metadata trick, allowing for potentially better learning and a small degree of control in conditional generation)</li>
<li>the final formatted corpus <a href="https://mega.nz/#!jW52zYIJ!GtLtjBAxFKmK0zm7gfVG_J4o0cq9uaLN_43FtYDNtis" title="2019-10-18-poetryfoundation-formatted.txt">is available for download</a></li>
</ul>
<p>Once the cleaned PG was done, I then swapped out the PG for PF dataset and began finetuning. (I could train on the combined dataset, but the PF dataset is only 20MB and at 1⁄6 the size of PG, training on that would take a long time to pick up on PF’s details.) Surprisingly, the PF dataset trained down to ~0.60 loss after ~10k steps, as compared to PG’s ~1.73, a decrease much greater than I had expected from providing some metadata, suggesting that modern poetry, being so prose-like, is much easier for <span class="smallcaps-auto">GPT-2</span> to model—which doesn’t strike me as a compliment.</p>
<p>Some selected samples from the PF-finetuning:</p>
<pre><code>"The New York Movie", by William Collins

for Billie Holiday

Billie Holiday is dead. I am unhappy,
standing there like a gray stone,
standing there in my sweater,
thinking I can't kill you: I would like to be a big, red dog
with snow. I wish you a good night
with your face, your cheek and your legs, your legs.
I want to throw away this falling Sunday
with your face, my ugly body, my ugly body.
Nothing to give my body to,
nothing to give my heart to.

..."To a History Store", by Yehuda Amichai [Living, Death, Time &amp; Brevity, Nature, Fall]

How hard it is for the living to die
holding only memory, and then to study the minutes
how like a trumpet of the future it sounds.

Gentlest of birds, patient as a body, cautious,
clear as the tongues of previous vervain birds.

Perhaps you are close to it, beautiful world.

..."Zagajas", by Robertizing [Religion, Christianity, Faith &amp; Doubt, God &amp; the Divine]

Our Lord who bore a child upon his knee.
We'll ask no more.
As sandpaper for what it wears.

Autumn, a new day.
Remember November, a new day.
Remember November, a new day.
Think of the silver
smiling into the barracud
glass

Already at the edge of day.
Think of the hardson of earth
crumbling into something new

Into every tingling ring
to the full edge. Think of what

New, bright, revolutionary clouds

scattered out of the blue:
the clouds that dissolved them,
soundless, self-edesfilled.
Think of the cycles in and around of this

crescent myriads of ants starting
to collect beneath each other,

their seeds suddenly

burning each to the other, each

moving and flashing.

..."The Bean Eaters", by Rudyard Kipling [Relationships, Home Life, Pets, Nature, Animals, Landscapes &amp; Pastorals, Winter]

The fairies were wonderful.
They trod the snow, chasing
the catkins to the north.

Frosty violin-skins were flying
and they began to sing,
leaving an echo of singing.
Then, as the she-torches rang,
a second spring
flowed up from the fur brush.
It was the strangest sight
all through the wintry night.

It was the woods, falling in long grass,
and I was thinking of you, Little Brother,

in the sweet marsh,
that I might recognize, Little Brother,

as I think of you.

..."In Golden Gate Park", by James Jenny Xie [Living, Coming of Age, Time &amp; Brevity, Activities, Jobs &amp; Working, Philosophy]

In Golden Gate Park's the day is breaking, only
the timeless moments of the night sketch the sky's
high promenade of flying goldenness now
and never a late, dissolving splinter of black glass.
But in Golden Gate Park's the morning breaks. The sidewalks
bask to me like cars at a funeral or the stars
like blind lights waiting on cars long since gone.
There, to the streaming windowpane, the little birds
scarve to get ready to swoop, and the sky's yellow
and gold. It is the end of hunger that slays the bird.

..."To Theodore", by Kenneth Slessor

Death may forgive, but love is better.

He that loves the rose
Whose pale cheek glows
With one hand swift and close,
Whose fingers move
The gold hair of the rose,
Gone to pass.
Where his lips draw breath
The bitter thong
Sigh as if Death had
No part with them,
He hears the song,
Hears the shout,
Saying me,
As I must.
Love is better, they say,
Than the loss they know;
Dreaming is worse, they say,
Love must hate so.
As his torch I carry the air;
He shakes my wings;
He speaks no word;
Saying me,
As I reach,
As he calls me,
Call him, O dear,
Call him, oh dear.
Love has been my constant care.</code></pre>
<p>The contemporary PF samples properly mimic all the metadata and formatting, and are good, for what they are. (If you doubt this, read through a random selection of PF poems.) The ones I liked seemed like they benefited greatly from the PG pretraining. There are still a lot of flaws in the unclean PF data: run-on lines are particularly irritating, and appear to be flaws in the Kaggle scrape dataset rather than the original PF website. I have brought up the problems on Kaggle, but I doubt they’ll be fixed soon.</p>
<p>With PF done, I combined it with PG and trained on the combination dataset for another ~20,000 steps, yielding a final loss of 1.61. The combined model appears able to do both datasets well (the weighted average of a dataset with a loss of 0.6 and another dataset 6 times larger and a loss of 1.7 would be ~1.55, close to the model’s ~1.6), and the samples don’t appear to differ much, so I don’t excerpt any. But the combined model should make a great starting point for RL preference-learning training.</p>
<p>Random samples &amp; model downloads:</p>
<ul>
<li><a href="/docs/ai/2019-10-17-117m-poetry-cleanprojectgutenberg-samples.txt">cleaned Project Gutenberg samples</a></li>
<li><a href="/docs/ai/2019-10-19-117m-poetryfoundation-samples.txt">Poetry Foundation-finetuned samples</a></li>
<li>final (combined) model: <a href="https://mega.nz/#!2PhghaZD!_IJPpErXIRIDwRI0ktq2UKUZClDEoY7z8UpF28_qme8">117M-clean</a></li>
</ul>
</section>
</section>
<section id="training-gpt-2-poetry-prefix" class="level1">
<h1><a href="#training-gpt-2-poetry-prefix" title="Link to section: §'Training GPT-2-poetry-prefix'">Training <code>GPT-2-poetry-prefix</code></a></h1>
<p>The first version of the PG data for <span class="smallcaps-auto">GPT-2</span>-poetry just runs all the lines together, erasing the metadata about what book each line comes from. A good model should nevertheless gradually learn about the transitions between poems &amp; whole books, but that is hard and there may not be enough transitions in the data to learn effectively.</p>
<p>Much like the char-<span class="smallcaps-auto">RNN</span> experiments on this page, there is no reason one can’t inject that metadata in a structured way to see if the model can learn to exploit the metadata; even if it cannot, the added metadata shouldn’t hurt <em>that</em> much because it is so regular &amp; repetitive. Inserting the metadata also allows for some degree of control in conditional generation; one should be able to put in the book ID for, say, Homer’s <em>Iliad</em> as a prompt and get out a long block of consistent Homeric pastiche.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Ideally, there would be unique IDs for every author, poem, and book and these would appear at the beginning of every poem and the end of the poem would be delimited with the <code>&lt;|endoftext|&gt;</code> symbol that OA’s <span class="smallcaps-auto">GPT-2</span> models were trained with, but unfortunately only the book ID is available in this particular dataset. (Project Gutenberg ebooks do not include any metadata or formatting which would cleanly split each discrete poem from each other.) Like before with authors, the book ID metadata can be formatted as a prefix on every line with a delimiter like the pipe character.</p>
<p>Rather than start over with <span class="smallcaps-auto">GPT-2-117M</span> again, <span class="smallcaps-auto">GPT-2</span>-poetry can just be further finetuned on this new prefixed version of the PG corpus to produce what I call “<span class="smallcaps-auto">GPT-2</span>-poetry-prefix”:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">cat</span> gutenberg-poetry-v001.ndjson <span class="kw">|</span> <span class="ex">jq</span> .gid <span class="kw">|</span> <span class="fu">tr</span> -d <span class="st">'"'</span>                                 <span class="op">&gt;</span> id.txt <span class="co"># "</span>
<span class="fu">cat</span> gutenberg-poetry-v001.ndjson <span class="kw">|</span> <span class="ex">jq</span> .s   <span class="kw">|</span> <span class="fu">sed</span> -e <span class="st">'s/^.//'</span> -e <span class="st">'s/.$//'</span> -e <span class="st">'s/\\//g'</span> <span class="op">&gt;&gt;</span> poetry.txt
<span class="ex">paste</span> --delimiters=<span class="st">'|'</span> id.txt poetry.txt <span class="op">&gt;</span> gutenberg.txt

<span class="ex">shuf</span> gutenberg.txt <span class="kw">|</span> <span class="fu">head</span>
<span class="co"># 14869|Beware of the brand of the fiery Frank!</span>
<span class="co"># 1727|and they have great power among the Argives. I am flying to</span>
<span class="co"># 38550|Shows heaven in page of living book;</span>
<span class="co"># 22421|First, for effusions due unto the dead, I. 26.</span>
<span class="co"># 26275|blossomed beneath their temples, and covered their chins with</span>
<span class="co"># 1745|What happiness, who can enjoy alone,</span>
<span class="co"># 1645|When first he won the fairy clime.</span>
<span class="co"># 4332|And out of these molten flowers,</span>
<span class="co"># 36916|What!  Never more go gladly back?</span>
<span class="co"># 2507|Raged for hours the heady fight,</span>

<span class="va">PYTHONPATH=</span>src <span class="ex">./encode.py</span> gutenberg-poetry-v001-delimited.txt gutenberg-poetry-v001-delimited.txt.npz</code></pre></div>
<p>The loss of <span class="smallcaps-auto">GPT-2</span>-poetry-prefix will be much lower than <span class="smallcaps-auto">GPT-2</span>-poetry because the prefix is so predictable, but it will hopefully learn interesting things beyond that.</p>
<p>In other samples, the generated IDs switch in the first two lines, and while that’s not much to judge from, <span class="smallcaps-auto">GPT-2</span>-poetry-prefix <em>seems</em> to ignore keywords from the first line when the IDs change, and doesn’t repeat them in the rest of the sample or attempt to rhyme off them, which is further evidence it is successfully associating &amp; learning to mode-switch.</p>
<p>Like <span class="smallcaps-auto">GPT-2</span>-poetry, <span class="smallcaps-auto">GPT-2</span>-poetry-prefix converged quickly to a final loss of ~1.6 after 224,474 steps taking 31 <span class="smallcaps-auto">GPU</span>-hours, not improving much after the first ~8 <span class="smallcaps-auto">GPU</span>-hours despite decreasing the learning rate. (Diminishing returns appear to set in quickly for finetuning <span class="smallcaps-auto">GPT-2-117M</span> even if one has a relatively large new corpus.)</p>
<ul>
<li><a href="https://mega.nz/#!zX4lzCzK!TNo_1uDlvszGkBUEdd5R_cQ-7Dfv0gyaaaq8BVzw1jA"><span class="smallcaps-auto">GPT-2</span>-poetry-prefix, model download</a> (430MB)</li>
<li><a href="/docs/ai/poetry/2019-03-06-gpt2-poetry-prefix-1000samples.txt"><span class="smallcaps-auto">GPT-2</span>-poetry-prefix, 1000 unconditional samples</a> (3.2MB)</li>
</ul>
<section id="gpt-2-poetry-prefix-samples" class="level2">
<h2><a href="#gpt-2-poetry-prefix-samples" title="Link to section: §'GPT-2-poetry-prefix Samples'"><code>GPT-2-poetry-prefix</code> Samples</a></h2>
<section id="training-samples" class="level3">
<h3><a href="#training-samples" title="Link to section: §'Training samples'">Training samples</a></h3>
<div class="sourceCode" id="cb12"><pre class="sourceCode Bash collapse"><code class="sourceCode bash"><span class="co"># ...[33999 | 4308.76] loss=2.45 avg=1.86</span>
<span class="co"># ======== SAMPLE 1 ========</span>
<span class="co"># 11|But I have had a day that was gone when I saw you in a dream,</span>
<span class="co"># 1322|A year ago, a year ago.... I'm going back</span>
<span class="co"># 1322|Oh, the years are many and the years begin!</span>
<span class="co"># 1322|You and I have traveled far and wide,</span>
<span class="co"># 1322|You and I have wandered far and wide</span>
<span class="co"># 1322|Through the land I love so well,</span>
<span class="co"># 1322|Where the beautiful land of long ago</span>
<span class="co"># 1322|Died and ebbed and glowed,</span>
<span class="co"># 1322|In the little green land of long ago,</span>
<span class="co"># 1322|In the little green land of long ago,</span>
<span class="co"># 1322|In the little green land of long ago.</span>
<span class="co"># 1322|You and I have traveled far and wide</span>
<span class="co"># 1322|Through the land of long ago;</span>
<span class="co"># 1322|You and we have traveled far and wide</span>
<span class="co"># 1322|O'er the land we love so well,</span>
<span class="co"># 1322|In the little green land of long ago,</span>
<span class="co"># 1322|In the little green land of long ago.</span>
<span class="co"># 1322|You and I have traveled far and wide</span>
<span class="co"># 1322|Through the land of long ago;</span>
<span class="co"># 1322|And the years have slowly swept us on</span>
<span class="co"># 1322|Till the dust has left the sod,</span>
<span class="co"># 1322|And we all must rue the little day</span>
<span class="co"># 1322|When we all are gone from God....</span>
<span class="co"># 1322|And we all must rue the little day</span>
<span class="co"># 1322|When we all are gone from God....</span>
<span class="co"># 1322|I know a little garden very old,</span>
<span class="co"># 1322|Built in a nook near the western sea,</span>
<span class="co"># 1322|With doors and windows closely locked and barred</span>
<span class="co"># 1322|And steps that made no sound but the world outside,</span>
<span class="co"># 1322|And a wall of white-walled flowers all in a row</span>
<span class="co"># 1322|Of violet bloom that just began to blow,</span>
<span class="co"># 1322|And a dial that looked like a golden globe,</span>
<span class="co"># 1322|Cool, square, and green, with the leaves and the grass,</span>
<span class="co"># 1322|And a dial that looked like a needle of bronze;</span>
<span class="co"># 1322|And I wonder if these petals are lost to-day,</span>
<span class="co"># 1322|These petals will be forgotten to-morrow.</span>
<span class="co"># 1322|"A rose would be a rose,"</span>
<span class="co"># 1322|I wonder much at it;</span>
<span class="co"># 1322|It does not matter what or who,</span>
<span class="co"># 1322|So very, very much it matters:</span>
<span class="co"># 1322|It is so nice to think</span>
<span class="co"># 1322|The world is such a place for petals,</span>
<span class="co"># 1322|And so much like a pink.</span>
<span class="co"># 1322|"A purple fox-hawk lived down by the sea</span>
<span class="co"># 1322|And danced and slept in the white-walled flowers,</span>
<span class="co"># 1322|And I thought about a tree,</span>
<span class="co"># 1322|A rose that dropped as an apple falls</span>
<span class="co"># 1322|Into the water, and fell in showers."</span>
<span class="co"># 1322|And I wondered what people said,</span>
<span class="co"># 1322|And wherefore took the trouble,</span>
<span class="co"># 1322|And wherefore came my little red bird</span>
<span class="co"># 1322|To look at me and wave its wings</span>
<span class="co"># 1322|For joy of self and for its red,</span>
<span class="co"># 1322|And why I cannot tell;</span>
<span class="co"># 1322|For it is so nice to think</span>
<span class="co"># 1322|The world is such a place for petals.</span>
<span class="co"># 1322|And so I sat and played,</span>
<span class="co"># 1322|And the old clock set the chimney-top</span>
<span class="co"># 1322|To ring time to itself and stop</span>
<span class="co"># 1322|The time with a slow, frost-ridden drone,</span>
<span class="co"># 1322|And a rustle as of bees.</span>
<span class="co"># 1322|The house was old, and had grown lean</span>
<span class="co"># 1322|As if it were but a leaf,</span>
<span class="co"># 1322|But I found there was only one room</span>
<span class="co"># 1322|And I slept on my stem at the end of the porch</span>
<span class="co"># 1322|Like a child asleep.</span>
<span class="co"># 1322|The door was old, but very young,</span>
<span class="co"># 1322|And very, very young;</span>
<span class="co"># 1322|And a weary smile on its satin lips</span>
<span class="co"># 1322|Hung like a crown upon the night,</span>
<span class="co"># 1322|And the roses crept into the room</span>
<span class="co"># 1322|By the little red door in the wall,</span>
<span class="co"># 1322|And I woke, and felt as a king is,</span>
<span class="co"># 1322|And thought of all the trouble and pain</span>
<span class="co"># 1322|Of a little life and a little love</span>
<span class="co"># 1322|When only a boy is born.</span>
<span class="co"># 1322|For my heart was old, and restless with the years,</span>
<span class="co"># 1322|And my face with no trace of the things I had</span>
<span class="co"># 1322|And no remembrance when I was twenty-five,</span>
<span class="co"># 1322|And</span>
<span class="co">#</span>
<span class="co"># [34000 | 4318.77] loss=2.15 avg=1.86</span>
<span class="co"># ...</span>
<span class="co"># [47998 | 11504.75] loss=1.91 avg=1.84</span>
<span class="co"># [47999 | 11505.25] loss=2.37 avg=1.85</span>
<span class="co"># ======== SAMPLE 1 ========</span>
<span class="co"># |So when the sun has touched the hill</span>
<span class="co"># 30599|And made the earth a little heave</span>
<span class="co"># 30599|Of gold-dust on the firmament.</span>
<span class="co"># 30599|The sea is a molten pearl</span>
<span class="co"># 30599|Silvering the yellow moon.</span>
<span class="co"># 30599|Gold is the poppies of purple plush,</span>
<span class="co"># 30599|Which make a star within a pool</span>
<span class="co"># 30599|Of purple, blue, and green and blue.</span>
<span class="co"># 30599|The sea is a molten pearl,</span>
<span class="co"># 30599|Whose petals burn as light</span>
<span class="co"># 30599|In the rose-yarn of the sea.</span>
<span class="co"># 30599|The gold star beckons, sparkles, shines.</span>
<span class="co"># 30599|The water gleams and glows.</span>
<span class="co"># 30599|The poppies turn to a golden rim,</span>
<span class="co"># 30599|And a thousand petals, one by one,</span>
<span class="co"># 30599|Come dancing through the purple waves</span>
<span class="co"># 30599|Of purple.</span>
<span class="co"># 30599|My soul goes up to the sun.</span>
<span class="co"># 30599|The sun, all gold, is gone.</span>
<span class="co"># 30599|The sun is gone, the sun</span>
<span class="co"># 30599|Is not more gold than my soul.</span>
<span class="co"># 30599|What is the rainbow, my soul?</span>
<span class="co"># 30599|The rain is falling in the tree</span>
<span class="co"># 30599|In dewdrops falling, that are wet</span>
<span class="co"># 30599|With dewdrops falling.</span>
<span class="co"># 30599|I have heard music in the woods</span>
<span class="co"># 30599|Under a great sky.</span>
<span class="co"># 30599|Their notes, on a hundred harps,</span>
<span class="co"># 30599|Dance by a liquid, falling star.</span>
<span class="co"># 30599|The song of the whole universe</span>
<span class="co"># 30599|Rings like a loon's tune.</span>
<span class="co"># 30599|The rain is falling, my soul,</span>
<span class="co"># 30599|In the tree-tops, the rain is falling,</span>
<span class="co"># 30599|And the rain is not more musical.</span>
<span class="co"># 30599|The trees, like great globed fruit in a garden of heaven,</span>
<span class="co"># 30599|Are as full of little shining blossoms</span>
<span class="co"># 30599|As the face of a child of ten minutes.</span>
<span class="co"># 30599|The wind is playing a soft tune</span>
<span class="co"># 30599|Like the silver notes of a bell.</span>
<span class="co"># 30599|The grass is a dance for a child,</span>
<span class="co"># 30599|And the sun is going down.</span>
<span class="co"># 30599|The rain is playing a soft tune</span>
<span class="co"># 30599|Like the golden notes of a bell.</span>
<span class="co"># 30599|The rain is playing asleep in the meadows</span>
<span class="co"># 30599|Like a sea of dreams,</span>
<span class="co"># 30599|And the wind is playing a soft tune.</span>
<span class="co"># 30599|The leaves are wet with the tears:</span>
<span class="co"># 30599|There is only the wind that comes.</span>
<span class="co"># 30599|The leaves are wet with the tears.</span>
<span class="co"># 30599|The trees have bent to the wind,</span>
<span class="co"># 30599|Like heavy blossoms, and nothing stays.</span>
<span class="co"># 30599|The earth is flooded with tears</span>
<span class="co"># 30599|Like small white stars in the sky.</span>
<span class="co"># 30599|The wind's song is marching by</span>
<span class="co"># 30599|Like a song in a wind from a string.</span>
<span class="co"># 30599|It comes from nothing, it comes not from the years;</span>
<span class="co"># 30599|An atom of dust and a tide,</span>
<span class="co"># 30599|The ceaseless rain falls heavily over the world,</span>
<span class="co"># 30599|The summer flowers are red</span>
<span class="co"># 30599|And one like a gold cup, in a crimson cup,</span>
<span class="co"># 30599|Flows out from nothing, and goes</span>
<span class="co"># 30599|Without speech or motion nor sound.</span>
<span class="co"># 30599|The yellow flowers all slip,</span>
<span class="co"># 30599|All their leaves are wet,</span>
<span class="co"># 30599|And their crimson petals are wet.</span>
<span class="co"># 30599|The rain has driven the sun outside,</span>
<span class="co"># 30599|The wind has driven the rain outside,</span>
<span class="co"># 30599|The moon is going out and in,</span>
<span class="co"># 30599|With the stars on the roof like snow.</span>
<span class="co"># 30599|The rain has driven the rain outside,</span>
<span class="co"># 30599|The moon is driving the clouds in the air</span>
<span class="co"># 30599|Like white, wool flakes in a snow.</span>
<span class="co"># 30599|The trees are wet with the tears,</span>
<span class="co"># 30599|They weep in their night-black tatters,</span>
<span class="co"># 30599|They moan like mortals who lost their way</span>
<span class="co"># 30599|Because they were glad of the rain.</span>
<span class="co"># 30599|The rain is falling, my soul,</span>
<span class="co"># 30599|It has torn away the flowers,</span>
<span class="co"># 30599|They are wet with the tears of the sun.</span>
<span class="co"># 30599|In the black ooze and the snow,</span>
<span class="co"># 30599|The cold winds, shuddering, are blowing,</span>
<span class="co"># 30599|The leaves are blown like blown corn,</span>
<span class="co"># 30599|There is only the wind that comes.</span>
<span class="co"># 30599|Black and long!  The night is</span>
<span class="co">#</span>
<span class="co"># [48000 | 11515.52] loss=2.14 avg=1.85</span>
<span class="co">#</span>
<span class="co"># ...</span>
<span class="co"># [166998 | 5752.38] loss=2.17 avg=1.52</span>
<span class="co"># [166999 | 5752.88] loss=2.10 avg=1.53</span>
<span class="co"># ======== SAMPLE 1 ========</span>
<span class="co"># 26|His golden radiance is no more."</span>
<span class="co"># 16452|"I shall not think of men in Argos more</span>
<span class="co"># 16452|Than they are now, who many a bloody deed</span>
<span class="co"># 16452|Wrought on the Greeks, nor yet of Ilium's king</span>
<span class="co"># 16452|In arts and arts like these can speak the rest.</span>
<span class="co"># 16452|But they--their kings--the Trojans and their sons</span>
<span class="co"># 16452|Have fallen. Their deaths, the Grecians and their friends</span>
<span class="co"># 16452|Have fallen in battle, from whom little hope</span>
<span class="co"># 16452|To escape the battle, but the steadfast hearts</span>
<span class="co"># 16452|Of heroes and of Trojans have become</span>
<span class="co"># 16452|Inglorious still. The immemorial strife</span>
<span class="co"># 16452|Shall rise for ever in a glorious day,</span>
<span class="co"># 16452|When wars are waged between us and the Greeks.</span>
<span class="co"># 16452|The battle shall be theirs, the mirth, the song;</span>
<span class="co"># 16452|The mirth which all the Olympian people share,</span>
<span class="co"># 16452|Shall bless the younger warriors with a joy</span>
<span class="co"># 16452|So great, so glorious, and a greater fame,</span>
<span class="co"># 16452|That all the Greeks shall learn, that in the van</span>
<span class="co"># 16452|Ye stand yourselves, and they will praise your deeds.</span>
<span class="co"># 16452|But I beseech you, if indeed by mine</span>
<span class="co"># 16452|Unknown dishonour you be wrested hence,</span>
<span class="co"># 16452|That with your lusts, illustrious and august,</span>
<span class="co"># 16452|All others ye may vanquish. Now, my friend,</span>
<span class="co"># 16452|Behold this prize to crown your father's pride.</span>
<span class="co"># 16452|He said, and shaking both his palms, assent</span>
<span class="co"># 16452|That I should also wish it. Thou art brave;</span>
<span class="co"># 16452|Thou know'st how Menoetiades the swift</span>
<span class="co"># 16452|Was dragged, of Hector and the fierce compeers</span>
<span class="co"># 16452|And Phrygian warriors. So, we will dispatch</span>
<span class="co"># 16452|Your bodies, then, yourselves to burn the ships</span>
<span class="co"># 16452|In sacrifice; with torches and with bells</span>
<span class="co"># 16452|To burn them, and with oxen to replace</span>
<span class="co"># 16452|Your gallant friends for ever. But I wish</span>
<span class="co"># 16452|That no man living has so long endured</span>
<span class="co"># 16452|The onset of his foes, as I have power</span>
<span class="co"># 16452|To burn or storm; for mighty Hector erst</span>
<span class="co"># 16452|Was slain, and now returns his safe return</span>
<span class="co"># 16452|To the Thesprotians, and each other's wives</span>
<span class="co"># 16452|And tender children, and all other babes</span>
<span class="co"># 16452|Assemble round me now, for ye have more</span>
<span class="co"># 16452|To suffer than they know. Go then--the rest</span>
<span class="co"># 16452|Will bear you safely; if ye dare to use</span>
<span class="co"># 16452|The Grecians with your ships, leave not the flames</span>
<span class="co"># 16452|Or fire, or slay yourselves and leave the rest,</span>
<span class="co"># 16452|Ye may, at least, escape, but Hector's hands</span>
<span class="co"># 16452|Will set you free; the vultures on the hills</span>
<span class="co"># 16452|Held forth, and I will pierce those hollow shades</span>
<span class="co"># 16452|Adown the trench, and ye shall find them there</span>
<span class="co"># 16452|Scaped unaware. If, then, the Gods above</span>
<span class="co"># 16452|Should bring thee to the fane of Peleus' son,</span>
<span class="co"># 16452|In his despite, thou wouldst be left alone,</span>
<span class="co"># 16452|Beside the yoke unwearied, there to stand,</span>
<span class="co"># 16452|And from the Trojans scatter all thy dead.</span>
<span class="co"># 16452|So spake the son of Atreus, and retired.</span>
<span class="co"># 16452|Then, leaving Hector's body in the tent</span>
<span class="co"># 16452|Of Periphas, King Agamemnon seized,</span>
<span class="co"># 16452|And in his royal hand his spear adorned</span>
<span class="co"># 16452|With silver, and with gold, studded thick all o'er</span>
<span class="co"># 16452|With glittering tin, the weapon of Mars thus toil'd.</span>
<span class="co"># 16452|Brave as he is, and of unwearied arm,</span>
<span class="co"># 16452|So shall be thy revenge, if thou shalt make</span>
<span class="co"># 16452|Fierce contest for the Danaï. But fly</span>
<span class="co"># 16452|With shout and clamour fill'd throughout the host.</span>
<span class="co"># 16452|So shall ye perish, slain by Hector's hands,</span>
<span class="co"># 16452|Each living soul, and, flying into fire,</span>
<span class="co"># 16452|Sink under Troy's wide gulfs. Be this the work</span>
<span class="co"># 16452|Of your brave sires, and all the parents' hearts</span>
<span class="co"># 16452|Shall prosper, one and all. But ye in haste</span>
<span class="co"># 16452|(For ye are valiant) shall</span>
<span class="co">#</span>
<span class="co"># [167000 | 5762.79] loss=1.32 avg=1.53</span>
<span class="co"># [167001 | 5763.29] loss=1.60 avg=1.53</span></code></pre></div>
<p>One training sample is worth remarking on:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode Bash collapse"><code class="sourceCode bash"><span class="co"># [49999 | 12526.44] loss=1.83 avg=1.77</span>
<span class="co"># ======== SAMPLE 1 ========</span>
<span class="co"># ._</span>
<span class="co"># 19221|The curfew tolls the knell of parting day,</span>
<span class="co"># 19221|The lowing herd winds slowly o'er the lea,</span>
<span class="co"># 19221|The ploughman homeward plods his weary way,</span>
<span class="co"># 19221|And leaves the world to darkness and to me.</span>
<span class="co"># 19221|Now fades the glimmering landscape on the sight,</span>
<span class="co"># 19221|And all the air a solemn stillness holds,</span>
<span class="co"># 19221|Save where the beetle wheels his droning flight,</span>
<span class="co"># 19221|And drowsy tinklings lull the distant folds.</span>
<span class="co"># 19221|Save that from yonder ivy-mantled tower</span>
<span class="co"># 19221|The moping owl does to the moon complain</span>
<span class="co"># 19221|Of such as, wandering near her secret bower,</span>
<span class="co"># 19221|Molest her ancient solitary reign.</span>
<span class="co"># 19221|Methinks I see her in her blissful dreams:</span>
<span class="co"># 19221|--Or, fancy-like, in some mirage she lies,</span>
<span class="co"># 19221|Majestic yet majestic, and of seems</span>
<span class="co"># 19221|The image of the unconquerable skies.</span>
<span class="co"># 19221|Methinks I see her in her blissful dreams:</span>
<span class="co"># 19221|--Or, fancy-like, in some majestic cell,</span>
<span class="co"># 19221|Where lordly seraphs strew their balmy dreams</span>
<span class="co"># 19221|On the still night, or in their golden shell.</span>
<span class="co"># 19221|There, in the calm of some Platonic dream,</span>
<span class="co"># 19221|Sits she, and views the unclouded moon arise</span>
<span class="co"># 19221|Like a fair lady full of realms divine;</span>
<span class="co"># 19221|And, all at once, a stony face and bright</span>
<span class="co"># 19221|Glittering in moonlight, like the noon-tints of a night.</span>
<span class="co"># 19221|Methinks I see her in her blissful dreams:</span>
<span class="co"># 19221|--Or, fancy-like, among the moonlight pale,</span>
<span class="co"># 19221|A heavenly lark twitters with every gleam</span>
<span class="co"># 19221|Of the rapt rapt beauty that she craves to hail.</span>
<span class="co"># 19221|Or in the moonlight gleams she seems to see</span>
<span class="co"># 19221|A virgin glory, and a power divine,</span>
<span class="co"># 19221|And every motion of the looking-glass</span>
<span class="co"># 19221|Is full of love and divine gentleness.</span>
<span class="co"># 19221|Methinks I hear her in the twilight dim:</span>
<span class="co"># 19221|--Or is it fancy-free, and musical,</span>
<span class="co"># 19221|And full of music? Or some holy hymn</span>
<span class="co"># 19221|Over some water-lily's long-lost lute?</span>
<span class="co"># 19221|O no! it is not fancy-free, I see:</span>
<span class="co"># 19221|Nay! fain would I thy spirit-hearts invest,</span>
<span class="co"># 19221|And hear the paean of these dreamy rhymes,</span>
<span class="co"># 19221|Might I but speak of my departed rhymes,</span>
<span class="co"># 19221|And thou wouldst sing of my empoet-swans.</span>
<span class="co"># 19221|O no! it is not fancy-free, I see:</span>
<span class="co"># 19221|Even Poetry murmurs in a pensive dream,</span>
<span class="co"># 19221|And like a breeze sings the blue mists of night.</span>
<span class="co"># 19221|O no! it is not fancy-free, I see:</span>
<span class="co"># 19221|Even Poetry murmurs in a pensive dream.</span>
<span class="co"># 19221|How soft the zephyrs, how the languid hours,</span>
<span class="co"># 19221|As in the noonday shade their limbs recline!</span>
<span class="co"># 19221|How mellow streams the mossy banks assume!</span>
<span class="co"># 19221|How solemn woods, low-chiselled, sumptuous green,</span>
<span class="co"># 19221|Nod to the cadence of their hoarse refrain,</span>
<span class="co"># 19221|As if the deep-toned muses' melodies</span>
<span class="co"># 19221|With half-averted plaints and half-averted plumes</span>
<span class="co"># 19221|In some majestic temple's quietness</span>
<span class="co"># 19221|Had to the silver twilight slowly come.</span>
<span class="co"># 19221|How solemn woods, low-couched, around thee lie:</span>
<span class="co"># 19221|--Or is it fancy-free, and melody</span>
<span class="co"># 19221|That makes the dull night long in worship held?</span>
<span class="co"># 19221|Methinks I hear the harp's harmonious sound</span>
<span class="co"># 19221|In some dim wood, when the deep shadows fall;</span>
<span class="co"># 19221|And the low wind, like one that listens, makes</span>
<span class="co"># 19221|In the still woods the harmony of all.</span>
<span class="co"># 19221|Or in the moon's pale beam, on some hoar rock,</span>
<span class="co"># 19221|Lonely and spectral, mourns her feeble woe;</span>
<span class="co"># 19221|And as the slow waves roll, and, ebbing, break</span>
<span class="co"># 19221|In music</span>
<span class="co">#</span>
<span class="co"># [50000 | 12536.67] loss=1.36 avg=1.76</span></code></pre></div>
<p>The rhyming in this sample is so good as to be suspicious. It might also sound familiar—because many of these lines are being copied from <a href="https://en.wikipedia.org/wiki/Thomas_Gray" class="docMetadata" data-popup-title="Thomas Gray" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Thomas Gray</b> was an English poet, letter-writer, classical scholar, and professor at Pembroke College, Cambridge. He is widely known for his <i>Elegy Written in a Country Churchyard,</i> published in 1751.</p>" title="Wikipedia: Thomas Gray">Thomas Gray’s</a> <em><a href="https://en.wikipedia.org/wiki/Elegy_Written_in_a_Country_Churchyard" class="docMetadata" data-popup-title="Elegy Written in a Country Churchyard" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>Elegy Written in a Country Churchyard</b></i> is a poem by Thomas Gray, completed in 1750 and first published in 1751. The poem's origins are unknown, but it was partly inspired by Gray's thoughts following the death of the poet Richard West in 1742. Originally titled <i>Stanzas Wrote in a Country Church-Yard</i>, the poem was completed when Gray was living near St Giles' parish church at Stoke Poges. It was sent to his friend Horace Walpole, who popularised the poem among London literary circles. Gray was eventually forced to publish the work on 15 February 1751 in order to preempt a magazine publisher from printing an unlicensed copy of the poem.</p>" title="Wikipedia: Elegy Written in a Country Churchyard">Elegy Written in a Country Churchyard</a></em>, which opens:</p>
<blockquote>
<p>The curfew tolls the knell of parting day,<br>
The lowing herd wind slowly o’er the lea<br>
The ploughman homeward plods his weary way,<br>
And leaves the world to darkness and to me.<br>
Now fades the glimm’ring landscape on the sight,<br>
And all the air a solemn stillness holds,<br>
Save where the beetle wheels his droning flight,<br>
And drowsy tinklings lull the distant folds;<br>
Save that from yonder ivy-mantled tow’r<br>
The moping owl does to the moon complain<br>
Of such, as wand’ring near her secret bow’r,<br>
Molest her ancient solitary reign.</p>
</blockquote>
<p>Some spelling differences aside, this intro is almost entirely copied from the 8 copies of Gray’s poem in the corpus; this extensive copying is not something I spotted in the <span class="smallcaps-auto">GPT-2</span>-poetry samples I looked at, suggesting that the scaffolding of the metadata did indeed help with learning.</p>
<p>Also interestingly, the copying only goes so far, as immediately after the final line about the owl, where Gray continues:</p>
<blockquote>
<p>…Beneath those rugged elms, that yew-tree’s shade,<br>
Where heaves the turf in many a mould’ring heap,<br>
Each in his narrow cell for ever laid,<br>
The rude Forefathers of the hamlet sleep …<br>
</p>
</blockquote>
<p><span class="smallcaps-auto">GPT-2</span>-poetry-prefix instead continues:</p>
<blockquote>
<p>…Methinks I see her in her blissful dreams:<br>
—Or, fancy-like, in some mirage she lies,<br>
Majestic yet majestic, and of seems<br>
The image of the unconquerable skies.<br>
Methinks I see her in her blissful dreams:<br>
—Or, fancy-like, in some majestic cell,<br>
Where lordly seraphs strew their balmy dreams<br>
On the still night, or in their golden shell.<br>
There, in the calm of some Platonic dream,<br>
Sits she, and views the unclouded moon arise<br>
Like a fair lady full of realms divine;<br>
And, all at once, a stony face and bright<br>
Glittering in moonlight, like the noon-tints of a night. …</p>
</blockquote>
<p>That is, it focuses on the female figure of the Moon in a way more ode-like than elegiac. These lines also do not seem to be extracted from the rest of <em>Elegy</em> either, as words like “bliss” or “mirage” or “dream” or “seraph” or “Platonic” do not appear in it. Some of the phrases like “blissful dreams” do appear in the rest of the corpus, but others like “some mirage” or “mirage she” do not. Nevertheless, the style is consistent throughout the entire sample and the quality is good, suggesting that while <span class="smallcaps-auto">GPT-2</span>-poetry-prefix has managed to memorize to a limited extent, it is nevertheless fully capable of generating good original text.</p>
<p>An additional example of memorization has been spotted; sample #17 in the 1000 unconditional samples is almost <em>entirely</em> a memorized copy of Percy Bysshe Shelley’s <a href="https://www.poetryfoundation.org/poems/45146/to-a-skylark">“To a Skylark”</a>:</p>
<pre><code>======================================== SAMPLE 17 ========================================
 of our mortal life, and made it known
32373|To be man's also,--as to make us one;
32373|A thing apart, yet of an individual
32373|In frailty and high elemental own.
32373|Hail to thee, blithe Spirit!
32373|Bird thou never wert,
32373|That from heaven, or near it
32373|Pourest thy full heart
32373|In profuse strains of unpremeditated art.
32373|Higher still and higher
32373|From the earth thou springest
32373|Like a cloud of fire;
32373|The blue deep thou wingest,
32373|And singing still dost soar, and soaring ever singest.
32373|In the golden lightning
32373|Of the sunken sun
32373|O'er which clouds are brightening,
32373|Thou dost float and run,
32373|Like an unbodied joy whose race is just begun.
32373|The pale purple even
32373|Melts around thy flight;
32373|Like a star of heaven,
32373|In the broad daylight
32373|Thou art unseen, but yet I hear the music of thy singing.
32373|Keen as are the arrows
32373|Of that silver sphere,
32373|Whose intense lamp narrows
32373|In the white dawn clear,
32373|Until we hardly see, we feel that it is there.
32373|All the earth and air
32373|With thy voice is loud,
32373|As, when night is bare,
32373|From one lonely cloud
32373|The moon rains out her beams, and heaven is overflowed.
32373|What thou art we know not;
32373|What is most like thee?
32373|From rainbow clouds there flow not
32373|Drops so bright to see,
32373|As from thy presence showers a rain of melody.
32373|Like a poet hidden
32373|In the light of thought,
32373|Singing hymns unbidden,
32373|Till the world is wrought
32373|To sympathy with hopes and fears it heeded not;
32373|Like a high-born maiden
32373|In a palace tower,
32373|Soothing her love-laden
32373|Soul in secret hour
32373|With music sweet as love, which overflows her bower;
32373|Like a glow-worm golden,
32373|In the silver dew,
32373|Scattering unbeholden
32373|Its aerial hue
32373|Among the flowers and grass, which screen it from the view;
32373|Like a rose embowered
32373|In its own green leaves,
32373|By warm winds deflowered,
32373|Till the scent it gives
32373|Makes faint with too much sweet these heavy-winged thieves.
32373|Sound of vernal showers
32373|On the twinkling grass,
32373|Rain-awakened flowers,
32373|All that ever was
32373|Joyous and fresh and clear thy music doth surpass.
32373|Teach us, sprite or bird,
32373|What sweet thoughts are thine:
32373|I have never heard
32373|Praise of love or wine
32373|That panted forth a flood of rapture so divine.
32373|Chorus hymeneal,
32373|Or triumphant chant,
32373|Match'd with thine, would be all
32373|But an empty vaunt--
32373|A thing wherein we feel there is some hidden want.
32373|What objects are the fountains
32373|Of thy happy strain?
32373|What fields, or waves, or mountains?
32373|What shapes of sky or plain?
32373|What love of thine own kind? What ignorance of pain?
32373|With thy clear keen joyance
32373|Languor cannot be:
32373|Shadow of annoyance
32373|Never came near thee:
32373|Thou lovest; but ne'er knew love's sad satiety.
32373|Waking or asleep,
32373|Thou of death must deem
32373|Things more true and deep
32373|Than we mortals dream,
32373|Or how could thy notes flow in such a crystal stream?
32373|We look before and after,
32373|And pine for what</code></pre>
<p>The 87 lines beginning with “Hail to thee, blithe Spirit!” are all Shelley (with perhaps slight spelling differences), much surpassing the memorization for Thomas Gray. Considering the top-<em>k</em> sampling method, it’s amazing that the sample could so exactly follow “To A Skylark”. It turns out there are ~12 copies of the poem in the PG corpus (it’s a popular poem), so in retrospect some degree of memorization is not surprising, but that’s still a <em>lot</em> of memorization. The 4 lines beforehand don’t appear to be copied from another Shelley poem, making it even more amazing. It’s a pity that that sample did not continue further because one wonders whether it could have repeated the entire poem and what it would’ve done when the original poem ended.</p>
</section>
<section id="unconditional-samples" class="level3">
<h3><a href="#unconditional-samples" title="Link to section: §'Unconditional samples'">Unconditional samples</a></h3>
<div class="epigraph">
<blockquote>
<p>“How the clouds<br>Seem to me birds, birds in God’s garden! I dare not!<br>The clouds are as a breath, the leaves are flakes of fire,<br>That clash i’ the wind and lift themselves from higher!”</p>
<p><span class="smallcaps-auto">GPT-2</span></p>
</blockquote>
</div>
<p>For both <span class="smallcaps-auto">GPT-2</span>s, I generated 1000 samples as follows:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode Bash collapse"><code class="sourceCode bash"><span class="ex">python</span> src/generate_unconditional_samples.py --top_k 40 --temperature 0.9 --nsamples 1000 --seed 0 <span class="kw">\</span>
 <span class="ex">--model_name</span> 2019-03-06-gwern-gpt2-poetry-prefix-projectgutenberg-network-224474
<span class="co"># ======================================== SAMPLE 1 ========================================</span>
<span class="co"># |But I shall tell thee of the glorious days</span>
<span class="co"># 1008|Of that old strife, wherein the truth of it</span>
<span class="co"># 1008|Atoned, though at the most points of the pulp</span>
<span class="co"># 1008|Be passed through: and about the torment, which</span>
<span class="co"># 1008|A Hermit in his youth thoughte, yet made</span>
<span class="co"># 1008|Still worse by his words, where he said, 'Peace!</span>
<span class="co"># 1008|Keep silence here; because in any place</span>
<span class="co"># 1008|I heard of charity the while I dwelt,</span>
<span class="co"># 1008|And of her tears made e'en my lot a sting:</span>
<span class="co"># 1008|And, but for those, in truth, to heaven were found</span>
<span class="co"># 1008|The spirit of him offered by the sea,</span>
<span class="co"># 1008|So that, out struggling, he made no denial.</span>
<span class="co"># 1008|But the proud spirit, soon as he was silent,</span>
<span class="co"># 1008|Embraced me, even in bidding me speak therefore,</span>
<span class="co"># 1008|And with such gentle voice and such great front</span>
<span class="co"># 1008|Came up unto my Saviour, that he made</span>
<span class="co"># 1008|Full many a sign, how that all virtue he</span>
<span class="co"># 1008|Would at his own discretion hear and mark;</span>
<span class="co"># 1008|Then saw he the chief flower of love delight</span>
<span class="co"># 1008|Within the living light, and made such pact,</span>
<span class="co"># 1008|That it would make bliss vouchsafe to him.</span>
<span class="co"># 1008|Paradiso: Canto XXIX</span>
<span class="co"># 1008|The day, that most of us had sojourning</span>
<span class="co"># 1008|'Mong the sweet spirits, from the highest shaft</span>
<span class="co"># 1008|Of their kind master had not looked on them,</span>
<span class="co"># 1008|Into the eyes forth from the bark they came;</span>
<span class="co"># 1008|Two palms it bore of one and of the letters,</span>
<span class="co"># 1008|And the other two with wings outspread.</span>
<span class="co"># 1008|Not in beatitude do I behold them,</span>
<span class="co"># 1008|But in and out of hope they uttered a</span>
<span class="co"># 1008|Heaven-toned word of comfort, so that reached</span>
<span class="co"># 1008|The upper light, which in attention hung</span>
<span class="co"># 1008|With the low shores, that to the nether shores</span>
<span class="co"># 1008|Were as the utmost thread 'to the high point.</span>
<span class="co"># 1008|And that unceasing after-effiguring</span>
<span class="co"># 1008|Of the eternal Sire, upon the which</span>
<span class="co"># 1008|One live soul only drew its breath mysteriously,</span>
<span class="co"># 1008|Up to the point, so pointed to the source</span>
<span class="co"># 1008|Of what it had been, I never it knew,</span>
<span class="co"># 1008|And hence never would have recognized.</span>
<span class="co"># 1008|What verdure of an undivided heart</span>
<span class="co"># 1008|Resolved, I say; and he complained e'en thus:</span>
<span class="co"># 1008|"What from thy soul unto the Good I send</span>
<span class="co"># 1008|I ever pray to, and by grace of that</span>
<span class="co"># 1008|Pointing myself I pray thee to the world,</span>
<span class="co"># 1008|To point thee the great mystery of love,</span>
<span class="co"># 1008|From this, the bottom to the source of all</span>
<span class="co"># 1008|Concerning thee; and not by its green leaves</span>
<span class="co"># 1008|Of science so unsullied was the thought,</span>
<span class="co"># 1008|As a small-handled cup, acquired by men.</span>
<span class="co"># 1008|The mind's eye, taking from the mortal world</span>
<span class="co"># 1008|All that it asks of bar or of the gold,</span>
<span class="co"># 1008|With the same fury burns as it was wont;</span>
<span class="co"># 1008|Now it may be by lantern or by shining,</span>
<span class="co"># 1008|Since both thy and my love has made me its."</span>
<span class="co"># 1008|The Almighty Father in his thunder made</span>
<span class="co"># 1008|Resenting, and all round about Him round</span>
<span class="co"># 1008|Went down his smitten steps, so that the air</span>
<span class="co"># 1008|Impregnate came not from his visitations,</span>
<span class="co"># 1008|Setting a day of darkness on all sides.</span>
<span class="co"># 1008|Therefore mine eyes I lifted to the ground,</span>
<span class="co"># 1008|And I beheld a river by the ice</span>
<span class="co"># 1008|Chained up and flowing back along the ice,</span>
<span class="co"># 1008|And suddenly before my feet it melted;</span>
<span class="co"># 1008|And what it now behoves me to retrace</span>
<span class="co"># 1008|The cause I had of it in heart I felt.</span>
<span class="co"># 1008|As the Sicilian bull, that rightfully</span>
<span class="co"># 1008|His cries first echoed in the mountains,</span>
<span class="co"># 1008|Did so rebellow, with the sound of which</span>
<span class="co"># 1008|It made my very blood to quicken well,</span>
<span class="co"># 1008|The dolorous accents which envenom'd me,</span>
<span class="co"># 1008|Forthwith I hasten'd unto where reply</span>
<span class="co"># 1008|Was made: "O Ro! Brunhild"</span>
<span class="co"># ======================================== SAMPLE 2 ========================================</span>
<span class="co"># |Hear the tale that the funeral chant is telling,</span>
<span class="co"># 2491|For the sorrows of other's children that dwell</span>
<span class="co"># 2491|Like sweet flowers upon the wold?</span>
<span class="co"># 2491|'Tis the tale of a life which is fled and gone,</span>
<span class="co"># 2491|And the star of a hope which shone</span>
<span class="co"># 2491|Bright above it, though dark may it be,</span>
<span class="co"># 2491|For the hopes of a brighter day are fled</span>
<span class="co"># 2491|And the joys of a happier lot?</span>
<span class="co"># 2491|'Tis the tale of a life with the weary and sad,</span>
<span class="co"># 2491|Where sorrows begin and rest.</span>
<span class="co"># 2491|For only a song can the widow's soul glad</span>
<span class="co"># 2491|Who sits musing 'mid shadows drear.</span>
<span class="co"># 2491|And only a music, sad with its sighs,</span>
<span class="co"># 2491|Till sad to the soul as death draws near</span>
<span class="co"># 2491|As life on her fragile bark!</span>
<span class="co"># 2491|I hear their voices faint in my slumbrous sleep,</span>
<span class="co"># 2491|The music of lives that seem less real</span>
<span class="co"># 2491|Than phantoms are dream-bound in duty's mystic keep,</span>
<span class="co"># 2491|With music that seems to be more real</span>
<span class="co"># 2491|Than phantoms are dream-bound in duty's mystic keep</span>
<span class="co"># 2491|For souls that sin may not see!</span>
<span class="co"># 2491|All round about us seems, in every place,</span>
<span class="co"># 2491|As far off as the eyes of kith and kin,</span>
<span class="co"># 2491|The ever-tremulous busy world's harmonious race,</span>
<span class="co"># 2491|And I hear the mighty ocean tides,</span>
<span class="co"># 2491|Feeling their strength, their might, their rhythmic din,</span>
<span class="co"># 2491|Are calling me all into one wide choral face,</span>
<span class="co"># 2491|And I hear the infinite singing of the winds,</span>
<span class="co"># 2491|That seem to make me simply live!...</span>
<span class="co"># 2491|The world seems a world that is full of sound and motion;</span>
<span class="co"># 2491|A world of beauty and of music, where it lies;</span>
<span class="co"># 2491|Yet all that is and has for me seems one more treasure</span>
<span class="co"># 2491|Than all the world dreams leave in the skies.</span>
<span class="co"># 2491|I hear the mighty tides of life,</span>
<span class="co"># 2491|They're crying to me,</span>
<span class="co"># 2491|They rise and sink in a restless strife</span>
<span class="co"># 2491|Of endless song.</span>
<span class="co"># 2491|Yet every stroke of sorrow's sword</span>
<span class="co"># 2491|Comes surely from afar,</span>
<span class="co"># 2491|That is true peace which is hard on board</span>
<span class="co"># 2491|Though oceans be dark and terrors war.</span>
<span class="co"># 2491|I hear the myriad singing words</span>
<span class="co"># 2491|Of ocean's depths,</span>
<span class="co"># 2491|They come like a song of broken birds,</span>
<span class="co"># 2491|The music floats on the air and stirs</span>
<span class="co"># 2491|My life to bear its measure in calms</span>
<span class="co"># 2491|Of perfect peace, and it is good,</span>
<span class="co"># 2491|But all is false peace only.</span>
<span class="co"># 2491|When first I heard the autumn rain</span>
<span class="co"># 2491|Sink down the hollows on the plain,</span>
<span class="co"># 2491|I held it very near,</span>
<span class="co"># 2491|And as I spoke to March again</span>
<span class="co"># 2491|I felt the long, slow throbbing rain</span>
<span class="co"># 2491|Creep from the earth in sudden flight</span>
<span class="co"># 2491|Through all the veins of earth again,</span>
<span class="co"># 2491|And in the sunlit, silent night</span>
<span class="co"># 2491|The world grew far forlorn.</span>
<span class="co"># 2491|And April came with rushing rains,</span>
<span class="co"># 2491|And leaves about the naked lanes.</span>
<span class="co"># 2491|I saw again the August noon</span>
<span class="co"># 2491|Roll round the world in blazing heaps.</span>
<span class="co"># 2491|And in the sunlight and the dark</span>
<span class="co"># 2491|A thousand germs their pageant crush.</span>
<span class="co"># 2491|And from the earth the maples bloom</span>
<span class="co"># 2491|In odors of the breath of bloom</span>
<span class="co"># 2491|And from the meadows and the hills</span>
<span class="co"># 2491|The rosy clouds drop down their spilled spilled spilled spilled</span>
<span class="co"># 2491|And drunken with the rain it kills.</span>
<span class="co"># 2491|And soon above the hills shall crash</span>
<span class="co"># 2491|The thunder of rain-wings,</span>
<span class="co"># 2491|And all the naked trees and shrubs</span>
<span class="co"># 2491|Shall lie, like naked, naked blades.</span>
<span class="co"># 2491|Out on the hills there shall be rain,</span>
<span class="co"># 2491|And the maples down the windy lane</span>
<span class="co"># 2491|Shall bleed, and flowers shall weep again</span>
<span class="co"># 2491|Through the weary hours of rain.</span>
<span class="co"># 2491|They shall lie where the maples lie</span>
<span class="co"># 2491|Deep in their bosoms, cold and numb,</span>
<span class="co"># 2491|Each with its wound on either arm,</span>
<span class="co"># ...</span></code></pre></div>
<p>Download links again:</p>
<ul>
<li><a href="/docs/ai/poetry/2019-03-06-gpt2-poetry-1000samples.txt"><span class="smallcaps-auto">GPT-2</span>-poetry, 1000 unconditional generation samples</a> (3.6MB)</li>
<li><a href="/docs/ai/poetry/2019-03-06-gpt2-poetry-prefix-1000samples.txt"><span class="smallcaps-auto">GPT-2</span>-poetry-prefix, 1000 unconditional samples</a> (3.2MB)</li>
</ul>
<p>Some fun passages I noticed in the first 100 unconditional samples:</p>
<pre><code>======================================== SAMPLE 2 ========================================
|Hear the tale that the funeral chant is telling,
2491|For the sorrows of other's children that dwell
2491|Like sweet flowers upon the wold?
2491|'Tis the tale of a life which is fled and gone,
2491|And the star of a hope which shone
2491|Bright above it, though dark may it be,
2491|For the hopes of a brighter day are fled
2491|And the joys of a happier lot?
2491|'Tis the tale of a life with the weary and sad,
2491|Where sorrows begin and rest.
2491|For only a song can the widow's soul glad
2491|Who sits musing 'mid shadows drear.
2491|And only a music, sad with its sighs,
2491|Till sad to the soul as death draws near
2491|As life on her fragile bark!
...
## Sample 3:
...
37804|The white-petalled white fox
37804|Opens himself to coolness
37804|In the late evening.
37804|But when the last child started
37804|The white fox to his feet flew,
37804|And the old fox was master
37804|Of all the magic heathen.
37804|Till when the faint huntsman
37804|Had snuffed the fragrant water
37804|Over his plump ears and skin,
37804|In the old way he knew not
37804|Till morn had almost shone;
37804|And then the fox came slowly
37804|And left the place unguessed;
37804|The white fox was not master,
37804|Although he had been master,
37804|Although he had been servant
37804|And now he could be master
37804|Of all the magic powers
37804|That keep the place enchanted
37804|In the wide earth and water.
...
## Sample 9:
...
36661|And the morn breaks, and, all the day,
36661|Red-clover'd birds with silver bill
36661|Flutter from tree to tree in flower,
36661|A quivering dew, a wind that wafts
36661|To haunts among the ancient woods.
36661|The golden-crested ilex, here
36661|Doth vine her purple cup; the deer,
36661|The wild-goose; and, in troops, the sheep,
36661|The goat, the sylvan-haunted elm,
36661|And the green-faced oft-gadding pine
36661|Blossom with purple.
36661|The lark soars up,
36661|And the hare loud answer make!
36661|Doves, willows, dunes, aslant the lake;
36661|Pair after pike sounds warbling;
36661|The reeds a triumph!
...
## Sample 14:
...
37452|I had a vision
37452|Of an old and stubborn old man,
37452|His hair was pale, and thin,
37452|His face was all forlorn,
37452|And the moon was full in the air,
37452|And a spirit passed over his brow,
37452|And its face was all for ever.
37452|And he spoke:
37452|'Have we ever a dream?
37452|Have we ever a vision
37452|Of the ghost's ghost?'
37452|The Master gave the word:
37452|'By the breath I know
37452|The meaning of Death:
37452|Can it be 'hush?
37452|Have we ever a dream?'
37452|The spirit said:
37452|'By the breath I know,
37452|The meaning of Death,
37452|You will see a ghost
37452|Stand by the door
37452|And enter.'
37452|And the spirit said:
37452|'By the breath I know,
37452|The meaning of Death
37452|You may understand:
37452|Can it be 'hush?
37452|Have we ever a dream?'
37452|The Spirit said:
37452|'By the breath I know,
37452|The meaning of Death
37452|You can see a ghost
37452|Stretched toward the door,
37452|And see a spectre
37452|Pass by the chamber door.
...
## Sample 24:
...
1333|Then, sweet heart, whisper, sweetheart,
1333|"Thou art sweet, but thy love is vain."
1333|I do love thee, my love,
1333|In a word, in a song,
1333|With the heart and the will,
1333|And the power of my heart;
1333|The power of my whole
1333|Of the poet's soul,
1333|And the heart and the soul!
1333|As the winds take the leaves
1333|As the flowers take the flowers,
1333|As the floods take the dew,
1333|As the salt runs in floods,
1333|As the salt runs in floods,
1333|As the snow in the seas,
1333|As the rain in the logs,
1333|As the wind comes and goes,
1333|As the sleet in the coppice,
1333|As the snow in the coppice,
1333|As the snow in the bogland,
1333|As the hail in the river,
1333|As the snow in the river,
1333|As the snow in the county,
1333|As the snow in the county,
1333|As the snow in the county,
1333|As the rain in the vale.
1333|As the stars take the dew,
1333|As the sparks fly from eye,
1333|As the sparks fly,
1333|So the hand of my heart
1333|As the heart of my art
1333|As the tongue of my lips,
1333|As the heart of my heart
1333|As the flame in the eye.
...
======================================== SAMPLE 39 ========================================
|And as the summer twilight,
34237|When the golden vinewood
34237|Strikes the silent midnight,
34237|Stands mute beside the brook,
34237|With a ghostly sense of the human heart
34237|Forgotten, yearning, sighing.
34237|I do remember how, long years ago,
34237|At the spring by the vistaed stream,
34237|I stood as 'neath the orchard, in the June,
34237|To the sound of the grass and the dream.
34237|I know the moss where the violets
34237|Quested the dew and the sun;
34237|The air above 'mong the orchards
34237|Murmuring ever of bees;
34237|And the heart that was filled with the music
34237|That came to the listening trees,
34237|While the bluebird's notes, as he piped again,
34237|Awoke the robin's golden throat;
34237|And the sound I heard, long years ago,
34237|Came through the wood and the dells,
34237|Bringing the sound of the violets
34237|And the perfume of dying wells.
34237|And the song I heard in the August dusk,
34237|In the August dusk by the lake,
34237|Was sweeter, from the full-leaved orchard,
34237|Than the sound of a happy brook,
34237|When it came to the school of my childhood,
34237|And to the school of the land,
34237|Oh my home of the woods, where the wild-flower
34237|Loses itself and dies!
34237|They give me back the old-time delight,
34237|The pleasant and the calm,
34237|When still the wind was blowing in the woods,
34237|And the children stood in the warm, glad school,
34237|And smiled as the dear lad asked.
34237|They give me back the pleasant book
34237|That gave my heart its fire,
34237|Those childish words, the constant brook,
34237|Those childish words, the tire;
34237|They made my soul to loiter!--Yes,
34237|They do, they make me blest!--
34237|The rest of the household, and the rest
34237|Of the parents whose hearts were filled with care,
34237|And who were sad in their care!
34237|Their voices!--Yes, and they do--
34237|'T was aye! 'T is aye! 'T is aye!
34237|And the dear friends, so dear to me,
34237|They still will live and die!
34237|I have not a moment now
34237|To forget when the morn is gray--
34237|To be happy, and cherish so
34237|The rose that is on her way.
34237|The evening breezes blow,
34237|And the stars shine out to-day--
34237|But I would not live in to-day,
34237|If I were as happy to stay!
34237|I hope that maybe one day,
34237|When all my work is done,
34237|My darling's coming away,
34237|To meet me in the sun;
34237|I hope that maybe I can see
34237|My Peggy's smile upon me.
34237|The evening wears an old, old gray,
34237|Which softly slants upon the way,
34237|Its shadows on the sunny day,
34237|Its shadows on the sunny day.
34237|O'er life, a sad, unwritten scroll,
34237|The words are like the gentle dove,
34237|That sails upon the nightly soul,
34237|Though none may read or hear reproof.
34237|And drooping o'er life's weary way,
34237|God grant the book may never end,
34237|The gentle words that cheer my way,
34237|The gentle words--they come to blend--
34237|The tender words of comfort and of love,
34237|The kindly words--they come to bring me joy.
34237|I know not if my path shall be
34237|Through the world's wild, woeful wild;
34237|But I know that sometimes, in the night,
34237|The dark will come, with wild delights,
...
======================================== SAMPLE 64 ========================================
 away,
2620|And be glad as the lark
2620|When the skies are clear;
2620|And send forth a breeze of love
2620|As of wings to our bark,
2620|And away with a joyous song
2620|As of streams in our ears,
2620|And away with a joyous tune
2620|As of birds in the spheres,
2620|And away with a joyous tune
2620|As of voices in trees,
2620|As of swans in the summer time
2620|When the grass is green
2620|And the air is keen,
2620|And the leaves are young--
2620|Then away with a song of praise
2620|As of flowers in Maytime
2620|All the sunny days!
2620|O beautiful, gentle, and clear,
2620|Illimitable and strong!
...
======================================== SAMPLE 72 ========================================
, he who had no need to fly;
24869|For in this moment of dismay
24869|The king who held that evil foe
24869|Threw Indra's son as he drew down
24869|The Lord of Life shaft-headed and bow.
24869|Then Indra, lord of every woe,
24869|The Vánar legions, with a shout,
24869|The Vánar legions met and fought,
24869|And straight they broke the tyrant's yoke,
24869|And hurled him at the giant, broke
24869|The mighty bow the giant broke,
24869|Which Indra, King of all the Blest,
24869|Had thrown by Rávaṇ's(924) mighty breast,
24869|The monstrous coil, the brawny hand,
24869|The monstrous mouth, the jaw, the jaw,
24869|The jaw, the jaw and bleeding jaw,
24869|The ungovernable host, the jaw,
24869|And the great bow which never bends,
24869|The arm, the fist, the knee, the ends,
24869|The body laid with mighty stroke,
24869|And the great bow which never bends.
24869|So, when the giants fought, and fell
24869|With murderous strokes, the giant fell,---
24869|So falls the tree with all his trunks
24869|Terrific in its death, that shoots
24869|Wild volley at the mighty trunk,---
24869|So fell the tree with all its boughs
24869|While all the vipers dug and sowed---
24869|So fell the tree with all its boughs.
24869|But Ráma's heart was sad within:
24869|He wept and mourned his captive's sin,
24869|For he had wrought a ruin yet
24869|O'er Raghu's son in his wrath,---
...
======================================== SAMPLE 78 ========================================
 on the bosom of
11014|King Deshav, son of Bhishma, sat in the shade of the trees,
11014|Humbu, the great, strong, beautiful, fortunate Brahmin,
11014|A king, a keeper of the law, a guide of the realm,
11014|His name unfolded through all time and space,
11014|A ruler of the realm, a keeper of the realm,
11014|And was worshipped, as was meet, by the Great Spirit of God.
11014|And all the days of his life he kept on striving with God
11014|For the union of faith; and at last all-wise he spoke to
11014|"Lord, I am the Brahmin's lord--and I hold thee thine inmost
11014|As I cast my life away from thee, my Lord, to-day!
11014|Therefore I cast mine body away from thee, my lord."
11014|And that, by constant penance, I might win thy favour
11014|So in the spirit's depths he plunged it into the sea,
11014|But, as the wave closed over it, the wandering wind
11014|Caught up the ship's chattels, and bore it with it to the beach.
11014|And Bhimasena seeing there the empty space behind,
11014|The wandering ship rocked in the dark and glowing heat.
11014|He sat upon the bosom of the Mother of God,
11014|He sat upon the emerald seas, meditating death
11014|Of the great sea.  He sat and pondered in his mind
11014|Upon the mystery of the sea, what gods the daring man
11014|Must have to tell of,--and this mystery,--when, in the morning,
11014|As, in the after days, the Lord of life should pass away,
11014|And leave the body alone to ride the ocean's force,
11014|To die in solitude, unknown, untroubled,--and unto him
11014|His world was opened; and as yet no living creature.
11014|And all the night he sat there, gazing in the east,
11014|Until the morning sunlight faded from the hills
11014|And dawn came, bringing darkness and the darkness awful,
11014|And to his soul came holy light from God, to cleanse
11014|All doubt and all resistance, till, in the morning of life,
11014|The coming of the Lord beheld his face.
...
## Sample 95:
...
24869|Canto XXI. Lakshman's Speech.
24869|He ceased: then Raghu's son repressed
24869|The sovereign of the giant kind,
24869|And thus with soothing words unsoft
24869|To Ráma Rávaṇ spake:
24869|"Come, with thy brother Lakshmaṇ, friend,
24869|And Lakshmaṇ, and the dame agree.
24869|Thou in the woods shalt soon be found
24869|And bathed in pleasant waters clean;
24869|Where thou shalt sit, and rest, and save,
24869|Well clad in coats of bark and hide,
24869|What days, what nights, what hours will pass
24869|That thou in holy heaven mayst see
24869|Thy darling with her night-made tressed
24869|Far from the forest. Thence will spring
24869|Sweet smells of pleasantness and light
24869|And bliss from the celestial string.
24869|Thence on the ground shalt thou be borne
24869|O'er the bare earth, O Queen Mosteer,
24869|And on the fresh bright earth where thou
24869|Shalt sit in state with Queen Sítá,
24869|In glorious heaven the nights and days
24869|Thou wilt be rapt by the great bliss
24869|E'en as the Lord of Gods is hearkening.
24869|The nights and days are thine, O best
24869|Of giant lords, and I, the best
24869|Of all who love the Lord of Lords,
24869|Whose might can turn the firmament,
24869|Whose might can sway the leafy bowers
24869|And turn each flower and leaf and bower
24869|To holy joy and blissful flowers.
24869|Ah me, the languorous days are come,
24869|And not a moment shall I see
24869|The happy days of Ráma's Queen
24869|Far from the light that round her glows,
24869|And marked with darkening leaves and boughs.
24869|Ah, whither would her steps be turned,
24869|And where the woodman's art had burned?
24869|Ah, whither would her steps be bent
24869|To turn her toil-worn heart once more,
24869|When all her hours were joy and peace,
24869|And all her hopes were set on store?
24869|Ah, let thy soul be comforted,
24869|Let trembling fancy still excuse
24869|The burden of a weary time
24869|That mars a saintlike life and use.
24869|Ah, if thy love were still the same
24869|That now I watch with toil and pain,
24869|That I could be for aid or flame,
24869|Could not my heart and bitterer gain."
24869|And Lakshmaṇ to the forest came
24869|And told his tale with welcoming.
24869|He saw the tree where he was set
24869|With burning buds and leaves beset.
24869|He saw the tree where he was brought
24869|By Sítá of the glittering thought,
24869|And when the leaves were fallen, he
24869|Spoke of his lord the tallest be.
24869|"O Lakshmaṇ, I the deer will slay
24869|From thicket, cave, and mountain gray,
24869|Ere long shall I this forest seek,
24869|And Lakshmaṇ in the covert seek.
24869|O'er hill and wood the Vánar bands
24869|And watch the beasts of wood and sands."
24869|He spoke: and Lakshmaṇ's love obeyed
24869|Nor did he speak as he was prayed.
...
# Sample 100:
...
38475|O Liberty, the patriot's sure defence!
38475|True to the man who fears a tyrant's eye,
38475|Preserve thy rights, and own his glorious cause,
38475|And yield the haughty title to a lie.
38475|No longer now on mean estate depend,
38475|And England owns thy sovereign vital force,
38475|And her best sons succeed to guard her laws,
38475|Or her best sons bestow a deedless course.
38475|Now, from that happy climate freedom's hope had birth,
38475|And made one day a milder country bleed,
38475|To the great cause that gave her aid is given,
38475|And to mankind one sure reward is even,
38475|Whilst I, perhaps, to distant climes must speed.
38475|To the same cause who has the cause to join?
38475|What foes against mankind may rise to arms,
38475|Boldly they fight, in actions of design,
38475|Yet all the same, and every day they charms.
38475|Ah, Washington! who can thy cause design?
38475|What can the nation do, or me, subdue,
38475|But still go on, in humbling folks admire!
38475|That we may praise thy conduct, that we fire,
38475|And for thy conduct many a hero dare,
38475|That we may rise, and cast the tyrants down,
38475|And tyrants fall, and fall the people crown!</code></pre>
<p>Not bad.</p>
<p>These samples represent roughly top decile poem samples (~10 out of the first 100), at least by my selection.</p>
<p><a href="https://slatestarcodex.com/2019/03/14/gwerns-ai-generated-poetry/" title="Gwern's AI-Generated Poetry">Scott Alexander</a> &amp; commenters highlighted a few more samples:</p>
<pre><code>Thou know'st how Menoetiades the swift
Was dragged, of Hector and the fierce compeers
And Phrygian warriors. So, we will dispatch
Your bodies, then, yourselves to burn the ships
In sacrifice; with torches and with bells
To burn them, and with oxen to replace
Your gallant friends for ever. But I wish
That no man living has so long endured
The onset of his foes, as I have power
To burn or storm; for mighty Hector erst
Was slain, and now returns his safe return</code></pre>
<pre><code>My heart, why come you here alone?
The wild thing of my heart is grown
To be a thing,
Fairy, and wild, and fair, and whole</code></pre>
<pre class="collapse"><code>And they have seen the last light fail;
By day they kneel and pray;
But, still they turn and gaze upon
The face of God to-day.

And God is touched and weeps anew
For the lost souls around;
And sorrow turns their pale and blue,
And comfort is not found.

They have not mourned in the world of men,
But their hearts beat fast and sore,
And their eyes are filled with grief again,
And they cease to shed no tear.

And the old men stand at the bridge in tears,
And the old men stand and groan,
And the gaunt grey keepers by the cross
And the spent men hold the crown.

And their eyes are filled with tears,
And their staves are full of woe.
And no light brings them any cheer,
For the Lord of all is dead</code></pre>
<pre><code>Fair is the Lake, and bright the wood,
With many a flower-full glamour hung:
Fair are the banks; and soft the flood
With golden laughter of our tongue</code></pre>
<pre><code>How the clouds
Seem to me birds, birds in God's garden! I dare not!
The clouds are as a breath, the leaves are flakes of fire,
That clash i' the wind and lift themselves from higher!</code></pre>
<pre><code>In the dark the sun doth gleam,
And in the dark the moon doth seem
But now the evening is begun--
Gone is the sun upon the earth!
The silver moon doth like a cup
Of blood-red wine, and as that cup
Is drained of life, doth quench no drop.
What man will drink such wine?
There is no soul of earth or birth
Which man hath never known of earth.
There is no soul who doth not sit
And sing to it, and cry, "Drink!"
There is no soul whose feet are set
On youth's eternal paradise;
For all is a solemn harmony,
And all is a perpetual chant,
And all the world is a song of God.
There is no soul so wholly free</code></pre>
<pre><code>There comes a murmur low and sweet
As of far-off streams in a dream,
Or a murmur of many birds,
Or chime of little evening bells,
As of wedding-bells in the dells,
Soft, sweet and slow,
As of wedding belles that come and go.
A little green ribbon of lilies
By the door of my dear one's room,
A kiss on her cheek, and she whispers,
"I am the bride of the loveliest flower."
A moment we stand in the garden
Of dreams and things,
Dreaming of fairyland
And the fairy music there,
Sweet bells and dreams, and the fairy music,
The fairy songs of the air.</code></pre>
<p>The top percentile of poems are probably quite good, especially with some light human editing to fix up the more glaring issues. To get a decent number of top percentile poems would require a lot of reading, but on the other hand, there is no reason why selecting or ranking poem samples could not itself be treated as a supervised learning task for retraining <span class="smallcaps-auto">GPT-2-117M</span>-poetry on, by using selected/non-selected as labels and training to predict the probability of a given sample being selected, and then such a NN could be used to prioritize likely-good <span class="smallcaps-auto">GPT-2</span>-poetry poems (or any source of poetry) for human review (and, in a form of “active learning”, the results of the manual review can be fed back in as additional data to help discriminate between the best and the merely good samples).</p>
</section>
</section>
<section id="gpt-2-poetry-prefix-completions" class="level2">
<h2><a href="#gpt-2-poetry-prefix-completions" title="Link to section: §'GPT-2-poetry-prefix completions'"><code>GPT-2-poetry-prefix</code> completions</a></h2>
<p>Prompted samples can be done like this:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="ex">python</span> src/interactive_conditional_samples.py --top_k 40 --temperature 0.9  --seed 2000 <span class="kw">\</span>
    <span class="ex">--model_name</span> 2019-03-06-gwern-gpt2-poetry-prefix-projectgutenberg-network-224474</code></pre></div>
<p>The downside of using the stock OA interactive prompt is that it returns on the first newline, so one either deletes newlines or uses a single line. Neither is good: a single line is hardly any context, while smashing many lines into a single super-long-line is dangerous because neither <span class="smallcaps-auto">GPT-2</span> has <em>ever</em> seen poems formatted that way (only, perhaps, some prose that snuck in) and newlines have important semantic functions in poetry. So, to avoid either problem, I bypassed the interactive prompt entirely, and I modified the Python script to replace <code>input</code> (for taking 1 line of keyboard input) to instead read standard input (<code>import sys; sys.stdin.read()</code>) so I could simply pipe in multiple lines from files or from the copy-paste buffer using <code>xclip -o</code>.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode Diff collapse"><code class="sourceCode diff"><span class="dt">7a8</span>
<span class="va">&gt; import sys</span>
<span class="dt">69,74c70,71</span>
<span class="st">&lt;</span>
<span class="st">&lt;         while True:</span>
<span class="st">&lt;             raw_text = input("Model prompt &gt;&gt;&gt; ")</span>
<span class="st">&lt;             while not raw_text:</span>
<span class="st">&lt;                 print('Prompt should not be empty!')</span>
<span class="st">&lt;                 raw_text = input("Model prompt &gt;&gt;&gt; ")</span>
<span class="kw">---</span>
<span class="va">&gt;         for i in 1:</span>
<span class="va">&gt;             raw_text = sys.stdin.read()</span>
## Example use:
# echo "&lt;|endoftext|&gt;" | python3 src/conditional_samples.py
#                         --top_p 0.99 --model_name 117M-clean --nsamples 1000</code></pre></div>
<p>The next issue in prompts is the question of the metadata; given that all the training data was properly labeled with origin and learning the meaning/associations was much of the point, it doesn’t make sense to not exploit this control in generation. If I was using authors, as with my previous char-<span class="smallcaps-auto">RNN</span> experiments, the prefix is simply whatever author one wants completions from, but in this case, it’s not quite so simple since we only have book IDs</p>
<p>If an author is already represented in the PG corpus, hypothetically one <em>could</em> look them up in it and see what IDs their poems were included under and use that, but that is a pain and doesn’t work for ones outside the corpus like Ginsberg. So, one could instead simply <em>ask</em> the model what prefix it thinks a prompt should use by feeding in the input several times and seeing what prefix it ‘hallucinates’ in the samples, and then adding that to the input for the real samples. If <span class="smallcaps-auto">GPT-2</span>-poetry-prefix consistently returns a specific prefix, then that is what it has learned and is useful scaffolding for the inputs; if it can’t do so consistently, then the prefixes aren’t useful for this particular input and it doesn’t matter.</p>
<p>So, to generate samples conditioned on relevant metadata, I pipe in the whole input unmodified several times, look at the generated samples for an ID, and if there is a consistent ID, then prefix it to the input and sample again several times.</p>
<p>Of course, now that everything is trained &amp; I have a good input method, I want to see how <span class="smallcaps-auto">GPT-2</span>-poetry-prefix does on the same poems as <span class="smallcaps-auto">GPT-2-117M</span> before!</p>
<section id="howl" class="level3">
<h3><a href="#howl" title="Link to section: §' '">“Howl”</a></h3>
<p>First, “Howl”. Given that the Project Gutenberg corpus is entirely old poetry and wouldn’t include much in the vein of “Howl”, I didn’t expect this to be good. The finetuning would wipe out the knowledge of free verse.</p>
<p>Finding a good prefix was hard, also unsurprising—not much like it in the PG corpus! I ultimately had to settle for a “1997” prefix from a relatively free-verse sample for generating the 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
1997|Mona, gazing, voluptious! exhaling human blood.
1997|Bare, the sun, in a golden mist,
1997|Leans on the meadow; the clouds are gone;
1997|Yet the heaven of green mists around
1997|Cannot but give them a fresher light.
1997|The sun hath a juice that no time can stay--
1997|And a golden night-cap.
1997|How softly the quiet clouds go by:--
1997|One sees a star through the evening sky
1997|Shine in the valley.
1997|There is no sound but the stream that strays--
1997|The very air how soft, and oh!
1997|How the waters of the meadows flow,
1997|In the warm afternoon;
1997|And the gentle wind from the hills above
1997|Is a murmurous tune.
1997|Through the still wood the cicalas sing,
1997|And every blade grows young and wan:
1997|A man is carrying a heavy heart
1997|To save himself from the cold, and then--
1997|And the last of it comes.
1997|He makes his coat, and, as he goes,
1997|His garment of a color dies.
1997|It is the night-wind, blowing over and under,
1997|With its great peasings of thundering wonder and wonder.
1997|It is the night-wind, echoing and shaking,
1997|With its great peasings of wonder and wonder.
1997|The night-wind suddenly blows a rosy bud--
1997|It is the sound of the forest trees breaking out,
1997|And a great rumbling of branches, and hoarse, and heavy and strong,
1997|And it is a shrill voice, and clear;--and the wind that gives answer cometh out.
1997|it is the loudest the voice alone in the cry of the silence of the noises in the darkness and the sky, and the moon:--and the water in the little wind in the trees.
1997|sing of the raving, and the tree with its silence that followeth:
1997|And it too long and a quiet water and they that cry of the
1997|tree-playing sea, and the air and the silence.
1997|flashing of the sky.
## Sample 2:
1997|The thundering guns! the splintering masses! the thunderous mass! the splintering guns! A welcome!
1997|Gitcheers! Welcome! the river that from thee will pass:--the storm of flame burst!
1997|The fire that is thine will be there:
1997|Hear it, my Sun-god, breathe upon me;
1997|Be strong! We ask not one again.
1997|Thou, mother Earth, if this is done,
1997|That thou wouldst be for aye to-morrow,
1997|What is thy name? to what will follow
1997|The sun that to thee sang one breath,
1997|What is thy name? In what will follow
1997|The sun in darkness, breathless, fell?
1997|The sun must rise in a day's breaking:
1997|What is thy name? to what will follow
1997|The sun in the world's last spinning-wheel?
1997|There comes a word of a holy token
1997|From Jove, the cloudless king of rain:
1997|"What canst thou, then, that I should follow
1997|The sun in heaven as on earth again?"
1997|The Lord and the Lord are lords together:--
1997|The sun will rise in a day's faring!
1997|The cloud will darken all earthly matter;
1997|The sun will burst in a day's strong-wing.
1997|The earth will moan through the black night
1997|And utter a little scream behind:
1997|"What is thy name, then, to what wilt thou,
1997|Lulled in the silence by Jove of rain?"
1997|The Sun would sink in his blue heaven
1997|And be a torch toothed in the main;
1997|The earth would be a chamber filled once more
1997|And the voice, and the heavens rise of the sea.
1997|And break a voice of the music of the heavens and the rain.
1997|Erebell of that speaks the wind again, the thunder.
1997|And the day and the flowers of all things that sang, and ocean's breath of the sun,
1997|And thou and the morning, and all that sang thee and thunder, the rain and the rain and the rain.
1997|And thou, and all.
## Sample 3:
1997|The whole cathedral of the church; the human procession, _Lyral._ 4. It had hardly gone
1997|The temple, the Capitol, the nation, was still seated there, and the altar, the
1997|pulpit, the water-mill in which it was used by the Greeks, was a perpetual murmur and in
1997|the old church, or the great, or the small, or rather the less, would have gone on at
1997|The altar of God--the altar, which had not yet been raised to the new, or to the
1997|"Blessed are they, blessed, among those who went down! and now is my will so
1997|And they asked the priests and the virgins of God to give them a prayer, so the
1997|dwellers on the earth, in which they are thinking, "a little space is granted for
1997|ever. If he were his own son he is guilty of a wrong. But he is guilty of the loss of
1997|righteousness! If he were to die before his race, his race and parents have been
1997|And they answered with voices of joy, "Remember righteousness, and remember the
1997|Thescore, the obstinate, bad and bold, at the word of the priest, was crucified.
1997|And the pious fathers and the pious men had gone to tell their sons, and to tell
1997|Then the father replied, "I would ask not the grace of thy God to form the
1997|The son was seized with the thought of death.
1997|injunction.
1997|The priests, he received the gift with the body, and the priest at the same time threw him
1997|turban and the priest's mantle. This, however, was the ransom for the son received
1997|given, and the father increased family increased his blood. The priest departed; but his body was given, he went away with tears that a widow, he died. The sated the widow gave into the father in a
1997|And it was raised, after his body, for a sowing blood in compassion of him.
1997|inery. The venerable face was brought him into the house in all the funeral and buried.
1997|giveness. But before him. The priest, together with himself.
1997|The shepherd took the widow married is thus, the priest,</code></pre>
<p>While they may be OK on their own and plausible as unconditional samples, they are disappointing as conditional completions, largely ignoring both the vocabulary &amp; style. It would seem that the finetuning wiped out whatever it was <span class="smallcaps-auto">GPT-2-117M</span> was using to generate its amusing “Howl” completions.</p>
</section>
<section id="ozymandias" class="level3">
<h3><a href="#ozymandias" title="Link to section: §' '">“Ozymandias”</a></h3>
<p>For “Ozymandias”, I fed it in a few times, and it seemed to like numeric IDs starting with ‘88’, so I used this as a prompt:</p>
<pre><code>8820| I met a traveller from an antique land
8820| Who said: Two vast and trunkless legs of stone
8820| Stand in the desert... near them, on the sand,
8820| Half sunk, a shattered visage lies, whose frown,
8820| And wrinkled lip, and sneer of cold command,
8820| Tell that its sculptor well those passions read
8820| Which yet survive, stamped on these lifeless things,
8820| The hand that mocked them and the heart that fed;
8820|
8820| And on the pedestal these words appear:
8820| 'My name is Ozymandias, king of kings;
8820| Look on my works, ye Mighty, and despair!'
8820| Nothing beside remains. Round the decay
8820| Of that colossal wreck, boundless and bare
8820| The lone and level sands stretch far away.</code></pre>
<p>Yielding (3 samples):</p>
<pre class="collapse"><code>## Sample 1:
8820|... the last captive died of his own shall
8820|and the stateliest of towers was the king...
8820|And he entered the city.
8820|He sat ironing in his counting-house,
8820|Flinging his burdens from a hundred kings;
8820|(A queen had one, and her name was Pepban)
8820|And the king was the son of Pepban.
8820|Then he said: 'O son of Hernando!
8820|Not a crown of gold nor iron can he save,
8820|But he shall conquer by his own right hand,
8820|And the palace of the king be free and save,
8820|And the crown of the head be free and save,
8820|And the wealth of the world be free and save,
8820|And the wealth of the world be free and save,
8820|And the king be free and save.'
8820|And the seer took the gift. He received it:
8820|And he wrought a wonder in his brain, and died:
8820|But the seer, like a god, whose history
8820|Is unendowed, and expoundeth unto it,
8820|--So shall one tell the story of his glory....'
8820|And the seer surveyed his old hard case.
8820|But the king was old and rough, he died
8820|Of age and weakness; and the seer replied,
8820|'I will tell them to my tale, and they shall learn
8820|Who sing this song, and live like equal lives
8820|In the kingdom of the king.'
8820|Again the seer said, 'We will go and speak
8820|A gentler story in a palace: it shall keep
8820|These seers in silent wonder still and deep
8820|As sleep within his grave.'
8820|And the seer took the gold:
8820|'I have no doubt that I shall tell it out,
8820|And I knew that for me shall there be no fear,
8820|If I tell a lie or clear.
8820|But tell it forth,--the poet shall be clear,--
8820|For never any hero
## Sample 2:
8820| Alluding to the curious fable of the Aztec king, who, in his
8820| The palace called319iform; the people of the region of Bahr
8820| The palace called31920|--Rua, behold! a new king came to mingle these two
8820|_So that I wept beside the dead,
8820|And buried deep in thought I wept.
8820|I, like a desert in a glade,
8820|Keep wistful vigil all the night--
8820|And on the palace built a pile
8820|Of lamps that, kindled, slowly shed
8820|A ceaseless light upon the dead.
8820|On this I saw the shadows grow;
8820|And, dreaming there, I, too, lay down--
8820|The dead.  I wept.  I wept--upon
8820|A couch and saw the shadows too.
8820|And yet, indeed, I wept.  For such
8820|A cold to-morrow's light will gleam,
8820|Or such a flame toward one like this.
8820|Then shall the spirit's eyes grow dim;
8820|And in thy face she read my heart
8820|I sometimes read a thousand rhymes
8820|Of hopes, and fears, and hopes have part.
8820|A moment as I read thy thoughts,
8820|My heart was moved; I was athirst;
8820|Yet I was well content to be
8820|The slave of some great memory;
8820|Then, in this vision of the night,
8820|My heart grew very sore with grief,
8820|And I fell prone.  Oh, it was sweet,
8820|As I lay trembling near the dead.
8820|And, as the dead-man's face, that slept,
8820|Groping beneath the churchyard yestreen,
8820|I heard the murmur, soft and deep,
8820|Of a sweet spirit, wafted thin,
8820|That made the shadows seem to creep,
8820|And murmur, soft and low and mild--
8820|And watching, through the moonlight, where
## Sample 3:
8820| The ebbing tide bears only the sands
8820|And trees that wave their boughs to the western breeze--
8820|A broken landscape, where faint breezes waft
8820|The lingerers on their way to Indian seas--
8820|A land whose shores are sunset skies,
8820|Where the pale mists of evening slowly sail,
8820|And, as we steal, the rivers murmur strange
8820|To the low-murmured wind at fall of night,
8820|And to the distant rim of the far sky:--
8820|Here, silent as a solitary tomb,
8820|With all that life can purchase and wealth can lend,
8820|With youth's immortal spirit, thou, O Friend!
8820|Art not for me to give thyself to Art?
8820|To thee, who art the latest breath of lands,
8820|Whose fame has swept the world for which men sigh,
8820|I give my heart up with indignant hands,
8820|To thee, who art the only one I seek.
8820|For thee, alone with life and light impearled,
8820|I cast my eyes from out the vast of time
8820|Where nothing dwells but Destiny's brief stage,
8820|Unshrouded, till, in its frail waste of time,
8820|It shall re-risen and grow once more sublime.
8820|And then, within the bounds where all is strife,
8820|And death, and life, and love, are all my own,
8820|I take this heart up with indignant breath,
8820|That seems to harden with the weight of years,
8820|And with the bitter hardihood of tears,
8820|And with the heart that has no beat or fear,
8820|And with the hand that has no strength to bear
8820|Only the world's and with the heart that fears,
8820|And by its sorrows only known to keep
8820|The beautiful and glorious years we keep.
8820|Here are the little walls we meant to build.
8820|Here stands the doorway, opening to the East,
8820|The child-like likeness of a poet's child</code></pre>
<p>Sample #2 is over-influenced by some prose footnotes/commentary which apparently were in PG, but the analogy of Ozymandias to Aztecs is a potentially fruitful one. And sample #3 here is a particularly good extension.</p>
</section>
<section id="essay-on-criticism" class="level3">
<h3><a href="#essay-on-criticism" title="Link to section: §'Essay on Criticism'"><em>Essay on Criticism</em></a></h3>
<p>Not clear what text exactly Scott Alexander used from Alexander Pope’s <em>Essay</em>, so I quoted the famous beginning section of Part 2. 3 samples strongly indicated Pope-like writing was associated with a prefix of ‘385’ (if not necessarily a full prefix) so I used 38511 for the following 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
38511|Pride even in numbers; wit's a kind pretence
38511|To something foreign still, but ne'er to sense;
38511|A constant waste of words, the world produces,
38511|A lazy sap, that nothing that's foreign to expels,
38511; that's foreign still unknown to the soul;
38511|A young, like, but in the world in motion,
38511|Obscending not, which smells all our own the worse than our own our own the soul's, and soul;
38511.
3851166|Like sense; which is like, but in all our whole.
38511|Which thus far more like, but in all things that's an excellence; and ne'er unchanged by which is folly's the worse, they give the worse maintained by which
38511|If sick of sense;
38511|Wholubil, or snug. ills, we know our own our first in sense the worse maintained between the worse, soon expired.
38511|Is blind and first blown on both being free from sense;
38511|From sense the worse maintained
38511|Wholuteness seems at once more used to sense the worse maintained by which
38511|Wholublime or enjoying sense; and first made to the worse, will's the worse maintained to sense;
38511|For sense; by which smells now discharged, and kept unseason'd from sense;
38511|Whose.  that's soon revived.  and then past the worse maintained, by birth to sense; by sense the worse, with weightyselves;
38511|Mankind by being all else barren; the worse maintained and last by birth to sense;
38511|Would cast, since built in nature lies from sense; for which smells and last, by repugither to sense;
38511|Whole; for our present, and first in life at all else to sense so long since built o'r by life to life, is soon revived by contact with heav' we know our own th
e worse maintained the worse it burns first made equal right;
38511|Is free.
3851166|Or dead: thus far more; who survey.
38511|Or wry's profuse and then dead: but what oft the worse maintained and next to life.
38511|From all; and
## Sample 2:
38511|There lies, that write the very best of all;
38511|For the lov'd fool, for those he courts and chokes,
38511|Is but a thorn inborn martyr in grief and sin,
38511|Who would all bawls and rattlantoms.
38511|Some hazels and isle from a thorn, or a starv'd for breaking hearts abh, to hogs;
38511|Or movel sooner writ, when by the starvels, or fombe.
38511|For men of any faultless wox and bribes.
38511|For wagers who should cut apart, and wak'd to make 'gin rights, for stink; for lamb, or chase; for lamb.
38511|Pounders or cast heel or a rope. For, for lamb, for lamb, for lamb or for lamb; for lamb, for lamb or starve.
38511|For no mean; for lamb, for sheep or for lamb, for lamb, lamb, for lamb, for lamb or for lamb, for lam, for mares.
38511; for lambs, a-heats. Suchley, for mares, for mares, for themselves.
38511.
38511. (for lambkins.
38511|mells; lam, lamb, lamb; lamb, lambkins; and other's for wares, lambkins; for struts; for sheep, lamb; or pricks, lamb, lambkins; for wer clothes; for mares: for sheep for lambkins; for goats for lamb.  for goats, lamb; for lambkins; and for moo: for hethers, for wark;
38511, lamb: for babes.
38511 unman; and for lamb.
38511; for lambkins; for mares; and the wox: for wheroat.
38511; for other goods, lamb. (let, frothriars, for spoles; a drum); for goats for lamb.
38511d:) for spoons, for spoons; for goats for whelornes, lamb, for mares: for syr trespass, for goats for mares: for lamb; for mares; for goats for sycam;
38511 theirs, for dog; for mawds.
38511; for whel
## Sample 3:
38511|They talk of constancy and faithless love,
38511|A seraph trembles at the specious glove;
38511|Nor in the rich confin'd relief of state,
38511|Find proud pretence, nor in the disdiscoveries of fate.
38511|For when misfortune makes choice remains the conduct's the prize half known,
38511|Can we secret soul without due, they fear of sense of more known.
38511|Some rise where' rights, they make it pays due.
38511861.
38511861.
38511861. Sense and judgment, as equal prize seem meanly, the reward the joy, as much possess the prize paid, as well reckon the prize we do not less dare not less keenly wise.
38511861|We see;
38511861|Of happy lovers ought, as well done, like a friend.
38511861|Know they ought, 'tis ev' is the other joy, as well worth a right;
38511861|The joy, as well might, as well may, as well may all is great.
38511861|Nor need of joys not as well maysters, as well as well may they give; but as little store; but as well as well as well may shewn, as much, as well we know, as well as well can be sure might prove, as well may well as well as well as well as well may view;
38511861|The mind: as well as well as well as well as much the fair as well as well as well as well as well as well as well may,
38511861, as thou woe.
38511861, when all.
38511861:
38511861|Well done by the fair as well might rise, who merit thus prove by 'tis most of all are one;
38511861ly play the few.
38511861|There's well as well as well as well as well as well as well as well as well as well as well as well as well as not less admire;
38511861|Apostorably bright.
38511861|Well done by sight.
38511861, as well as well as by contact;
38511861|For they; the main aim</code></pre>
<p>Alexander described his <span class="smallcaps-auto">GPT-2-117M</span> sample from Pope:</p>
<blockquote>
<p>It understands there should be line breaks, it understands the approximate correct length of a line of iambic pentameter, it understands how to talk like an overeducated 18th-century dandy—but it doesn’t appreciate rhyme or meter. In retrospect this isn’t surprising; <span class="smallcaps-auto">GPT</span> has no idea words sound like anything; it would be shocked to learn anyone uses language as anything other than text strings.</p>
</blockquote>
<p><span class="smallcaps-auto">GPT-2</span>-poetry-prefix still has “overeducated 18th-century dandy” down pat, but it manages to improve on the rhyming aspect: there’s quite a few rhyming lines in samples #2 &amp; #3 (#2 seems to be screwed up by taking a digression into footnotes defining words and then bad sampling getting it trapped), like “pretence”/“sense”, “soul”/“whole”, “love”/“glove”, “state”/“Fate”, “bright”/“sight”, and a number of <em>almost</em> rhymes like “right”/“great”. One wonders if it’s learning by brute force and memorizing specific pairs of rhymes (although could there really be that many rhymes of “state”/“Fate” in even 3m lines of old poetry?), or if it’s doing something more equivalent to inferring the latent phonetics from the co-occurrence of byte-pairs? (That may sound unlikely, but word embeddings do many unlikely-sounding things with no more supervision than co-occurrence<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>.)</p>
<p>More concerningly, the samples are terrible. Pope’s poetry should be straightforward for <span class="smallcaps-auto">GPT-2</span>-poetry-prefix, as it follows standard meters and rhyme and relies on a classical vocabulary well-represented in the PG corpus. Why, then, are they so bad? I suspect this may reflect the corpus itself doing Pope a disservice. Pope’s inclusion in the <a href="https://www.gutenberg.org/ebooks/author/907">PG</a> corpus appears to consist of the following (grepping for “Alexander Pope”):</p>
<pre><code>32190|The Works of Mr. ALEXANDER POPE. London: Printed by W.
32190|The Works of Mr. ALEXANDER POPE. Volume ii. London: Printed
32190|Letters of Mr. ALEXANDER POPE, and Several of his friends.
32190|The Works of Mr. ALEXANDER POPE, in Prose. Vol. ii. London:
32190|The Works of ALEXANDER POPE, ESQ.; vol. i. with explanatory</code></pre>
<p>Checking PG entries and looking through the <code>32190</code> prefix, it starts:</p>
<pre><code>32190|INTRODUCTION                                                 xv
32190|The Works of Mr. ALEXANDER POPE. London: Printed by W.
32190|BOWYER for BERNARD LINTOT, between the Temple Gates, 1717.
32190|This volume consists of all the acknowledged poems which Pope had
32190|The Works of Mr. ALEXANDER POPE. Volume ii. London: Printed
32190|by J. WRIGHT, for LAWTON GILLIVER, at Homer's Head in Fleet
32190|Letters of Mr. ALEXANDER POPE, and Several of his friends.
32190|London: Printed by J. WRIGHT for J. KNAPTON in Ludgate
32190|Street, L. GILLIVER in Fleet Street, J. BRINDLEY in New Bond
32190|Street, and R. DODSLEY in Pall-Mall, 1737. 4to and folio.
32190|The Works of Mr. ALEXANDER POPE, in Prose. Vol. ii. London:
32190|Printed for J. and P. KNAPTON, C. BATHURST, and R. DODSLEY,
32190|The Works of ALEXANDER POPE, ESQ.; vol. i. with explanatory
32190|Notes and Additions never before printed. London: Printed
32190|commenced printing his particular section of the octavos when the
32190|Quo desiderio veteres revocamus amores
32190|Atque olim amissas flemus amicitias.
32190|Nutrix mea fidelissima M. Beech, obiit 5 Novem. 1725, aet. 77.
32190|Edwardus Blunt, vir amicissimus obit, Aug. 1726.
32190|Francisc. Atterbury, Roffens Episcopus, vir omni scientia clarus,
32190|The fourth volume contains the Satires, with their Prologue,--the
32190|alterations. --_His Last Will and Testament._--WARBURTON.</code></pre>
<p>This is perhaps not good training material for <span class="smallcaps-auto">GPT-2-117M</span>-poetry/prefix and explains the bizarre degeneration—it is ‘expecting’ sudden random irruptions of largely-irrelevant prose such as introductions or footnote-annotations (rendered inline by PG’s text formatting). Other entries in the corpus will be more free of scholarly or prose apparatus. (In retrospect, a preprocessing step like dropping lines longer than ~60 characters might’ve been a good idea.)</p>
</section>
<section id="famous-first-lines" class="level3">
<h3><a href="#famous-first-lines" title="Link to section: §'8 Famous First Lines'">8 Famous First Lines</a></h3>
<p>The prefix trick doesn’t work on the 8 famous first lines nearly as well as it does with the long excerpts from “Howl” etc; I assume they are simply too short to home in on a relevant prefix. Nevertheless, I tried.</p>
<section id="ulysses-lord-alfred-tennyson" class="level4">
<h4><a href="#ulysses-lord-alfred-tennyson" title="Link to section: §' , Lord Alfred Tennyson'">“Ulysses”, Lord Alfred Tennyson</a></h4>
<p>“It little profits that an idle king,” yielded no consistency in prefixes, so I skipped adding one. 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
2428|And he who wears a crown, may die, but little lives.
2428|It was a pleasant day, when snugly shut,
2428|For many happy moons and many a sun
2428|Had lit the laughing waves with constant scarf,
2428|And heard the pealing waters on the run.
2428|'T was noontide; and with such a sky-lark flight
2428|The fleet white pieces flew in waves of sound;
2428|The silver stars, their fitful little light,
2428|Were beating on the liquid night around.
2428|Down went the cold, and all the dreariest blasts
2428|That ever chanced to blow, came mopping near:
2428|The first and fierce, the youngest, was our statesman here,
2428|Old Man-Autumnus!--An oak-headed oak!--
2428|The oak that in our court has grown so hard!
2428|"_Here am I_," answered he, "when I am dead,
2428|And round these sacred ashes I shall cling;
2428|And all the while my heart, unsatisfied,
2428|Holds but one clue,--and how must I be king?"
2428|How could he hope that a short space of day,
2428|And the warm blush of eve, and a fierce warning felt,
2428|Could prove the son of an old hunter, killed
2428|By the wolf's hounds in the wood! He crouched, and leant
2428|Against our shadowy robes; and the boughs bent low,
2428|Like the snow-flakes that cover the world below,
2428|Bent with a conscious motion. As the breeze
2428|Fluffs away, and the suns shivered and sighed,
2428|A shadowy motion, like a sigh, went by.
2428|It seemed a shadowy grief, and a mysterious
2428|Pestilence, like the voice of one who has sinned.
2428|At length, to his own chamber silently,
2428|We came, and gathering up the golden rim
2428|Of the great golden moon, looked down and wept.
2428|Then in a voice so soft, so
## Sample 2:
36287|Weary of wandering,
36287|Weary of loving,
36287|Can love a tawny cheek,
36287|Or a eye of ruby beaming through?
36287|My heart, why come you here alone?
36287|The wild thing of my heart is grown
36287|To be a thing,
36287|Fairy, and wild, and fair, and whole;
36287|With but the lion-heart to mate,
36287|And yet the heart to feel and know
36287|But in the wildest nightingale;
36287|The tongue you have for praise or blame,
36287|The heart that is more soft and tame
36287|Than the wild flower, this summer day,
36287|Shall never know your own dear way.
36287|Though cold and bleak your cheek I feel,
36287|Though bleak your mouth,
36287|Though cold your hand I would not steal,
36287|Oh, love, a thought beyond all words,
36287|Could I but win that word of birds,
36287|Could I but win that heart of yours!
36287|O sweet and tender is the lute,
36287|And soft the song the scarlet throat,
36287|When song-birds wake its wildwood lyres,
36287|And wild-bees through the glens go by.
36287|To you the wild-flower opens wide,
36287|To you one ripple stirs the tide,
36287|One long sweet longing seems to be
36287|A look of love made yesterday.
36287|To you the wild-flower bows its head,
36287|To you the wave is musical,
36287|Each kiss a kiss--but what of that?
36287|Ah, love, beyond your reach you keep
36287|All joys for you--ah, hear you me!
36287|Above your grave the winds are laid,
36287|Love lulls asleep within your shade;
36287|There is no rest for weary feet,
36287|The years are unfulfilled, I trow,
36287|And 'neath your sad and smiling eyes
36287|The sun tracks every path that lies
36287|Across your
## Sample 3:
1020|Who had too much of common human things,
1020|Being at peace with all he had on earth,
1020|Had something of a high and godly pleasure
1020|To do without him, since he had been
1020|In all the world so utterly,
1020|And that he had not far from his ambition,
1020|Had done no act for any human duty,
1020|Could not have felt his life's inheritance,
1020|If some good friend had lived to teach him thus
1020|His father's story of himself and country,
1020|And to restore his kingly name to men.
1020|O, the days that grow in my lament,
1020|And the nights that grow more still,
1020|When the earth lies awake,
1020|And the heaven it girdles,
1020|And the woods all shake,
1020|And the bright stars glitter
1020|Through the shadowy mist;
1020|When the night comes trailing her star-tinted hair,
1020|And the stars are the gray of our being,
1020|And the night wind is like some old sorrow
1020|Whose sweet touch is as of heaven,
1020|With its shadows and with its stars,
1020|Gathered in one place, is a child's face,
1020|A child's face made of golden sea foam:
1020|And the tears and the prayers that we utter
1020|Will be as those broken tears
1020|That fall upon bosoms, at the hour of their birth,
1020|With a sad and sorrowful melody,
1020|With a last, far-off singing of earth.
1020|Yet the night is a golden sleep,
1020|The day is like gold,
1020|And the stars go out alone
1020|Through the dark leaves and the darkness, alone.
1020|Night above and stars about,
1020|All our wanderings ended,
1020|Time above and darkness about,
1020|And the sun all hidden,--
1020|We are two two; the world is two.
1020|Howsoever the hours clap and clap,
1020|All the earth is happy,</code></pre>
</section>
<section id="sailing-to-byzantium-yeats" class="level4">
<h4><a href="#sailing-to-byzantium-yeats" title="Link to section: §' , Yeats'">“Sailing to Byzantium”, Yeats</a></h4>
<p>“That is no country for old men.”, no consensus. 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
28591|I would to God that it all came to pass,
28591|That my old line of trouble and poverty
28591|Was left here desolate of my poor discontent,
28591|That the first cause of all my own heart's moan
28591|Was to be born in this sad world--a lone lone lone way?"
28591|I read it in my sonnet; his deeds were so brave,
28591|So kindly a husband, so cunning a guide;
28591|So tender a heart he went with his work,
28591|That his life never stinted its comforts at home.
28591|I read it in my sonnet--and he was so true,
28591|So loving my child, that it ever proved best,
28591|And though I was forced from his tender command,
28591|I felt I was growing up, when his voice, all tender, said,
28591|"If Love be unfaith, then he is the earth's father."
28591|I read in my sonnet--and he was so sweet,
28591|So eloquent was his look and his tones,
28591|That I smiled when I saw him, and ran forth to meet
28591|His father--and asked him in all things to meet.
28591|But some were too haughty, the most malgracious,
28591|To love their loved Father--the more to annoy;
28591|And some were too cold and abandoned, and when
28591|They saw how much reason has melted and flowed,
28591|They cried, "O Love, give us to love, give us fire,
28591|So we may die, as we will; O perfect, as dead,
28591|With the dead silence!" Then each took his own way,
28591|And, in its nature, an exquisite sorrow lay,
28591|Like love in a garden of fresh green;
28591|They were happy--too happy--by nobody knows!--
28591|The world, too, was happy by Roger and John,
28591|Though some at her heart cannot understand,
28591|Yet the whole world was happy on that first day.
28591|O thou who art so beautiful!
28591|What is the world without thee?
## Sample 2:
1365|O, there's a country, there's a town,
1365|And here's the place that I have stroll'd.
1365|See! there's a town!
1365|And here's the place, and here's the goal;
1365|And here's the flower for me and you--
1365|For you and me.
1365|Here's my little girl;
1365|And here's the spot that I call mine--
1365|"Come over here!"
1365|He's under the lilac-tree
1365|Where I loved so often to dwell;
1365|He's near his own tree-trunk,
1365|But here's his knee--
1365|And he's floating away
1365|To the roses in the churchyard,
1365|To the fields where I was born.
1365|Here's the lily-thorn, and here's
1365|"But where's the rose?"
1365|Here's a little lad;
1365|But a marigold he's brought
1365|Here's a ribbon at most:
1365|"Spring comes in the sun," says he,
1365|"And you dream of flowers gay
1365|On the hills of Arcady."
1365|That's the way he's trod;
1365|He shall be a king, and he
1365|Must have to sing--
1365|Ay, a king--a queen--a grave--
1365|Whose name was love, his heir--
1365|For the royal blood he gave
1365|At the royal marriage feast,
1365|And the crimson wine he drank
1365|At the royal marriage feast,
1365|And the crimson wine he drank
1365|At the royal marriage feast.
1365|They love to wake alive,
1365|To gaze at life and die;
1365|But they do not understand
1365|How many thousand voices call'd
1365|Out of the ages by the dead,
1365|Out of the ages by the living's voice.
1365|How many voices, and what else
1365|Did they call the living dead?
1365|All which of them was gone
## Sample 3:
1020|A time will come for tears and cries,
1020|And then I'll know her by their eyes
1020|I had her, but I cannot tell,
1020|And I am lonely in my cell
1020|Since the time the fair one came to me,
1020|I who loved her since my first year,
1020|Though in my youth she had been dead.
1020|I had grown quiet as the night,
1020|And I had won her tender sight;
1020|And the soft notes of her voice
1020|Would haunt me without joy or fear,
1020|Though the day had come.
1020|This breath in the air took the damp,
1020|And was gone to the damp,
1020|And was gone to the black,
1020|And was gone to the black,
1020|And was gone to the black,
1020|And was flown to the white,
1020|And was gone to the white,
1020|And was flown to the black,
1020|And was flown to the black,
1020|And was flown to the black,
1020|And was gone to the black,
1020|And was carried away in the mould
1020|With its all-written countersign.
1020|I thought the words would mean the fight,
1020|But the fighting face was not my sight,
1020|I thought the men would take a share
1020|Of pain and strife and weariness
1020|And leave me on the dusty street,
1020|Where I would meet that sorry man
1020|And tell him tales and talk to him;
1020|He said, "I'm sure I've done my best,
1020|But I'm not the man to meet the west."
1020|He said, "I'm going on our track,
1020|With my best friend here at the back,
1020|And my master, as I understand,
1020|And the rest of the country side,
1020|Where people go and people ride,
1020|And some end never to decide."
1020|"I can't," I ordered, "I can't!"
1020|He said again, "I say</code></pre>
</section>
<section id="sonnet-29-shakespeare" class="level4">
<h4><a href="#sonnet-29-shakespeare" title="Link to section: §'Sonnet #29, Shakespeare'">Sonnet #29, Shakespeare</a></h4>
<p><a href="https://en.wikipedia.org/wiki/Sonnet_29" class="docMetadata" data-popup-title="Sonnet 29" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Sonnet 29</b> is one of 154 sonnets written by the English playwright and poet William Shakespeare. It is part of the Fair Youth sequence. In the sonnet, the speaker bemoans his status as an outcast and failure but feels better upon thinking of his beloved. Sonnet 29 is written in the typical Shakespearean sonnet form, having 14 lines of iambic pentameter ending in a rhymed couplet.</p>" title="Wikipedia: Sonnet 29">“When, in disgrace with fortune and men’s eyes,”</a>; no consensus, 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
28287|With heart and hand he went--
28287|The boy, like any lad of five,
28287|Was never seen to pair;
28287|For he ne'er met with a taste of grog;
28287|And now, howe'er his foes may say,
28287|I wish the boy would do all that he can,
28287|And so will merely go to bed;
28287|For, ere he goes to bed,
28287|He'll do most things, not more.
28287|He's gone to bed as readily;
28287|Then, before breakfast, he politely shuts
28287|The book, unless he's said,
28287|And talks of how the breeches blows,
28287|And how the shadows fall;
28287|And how a lady steals a cheese;
28287|And what a consternation comes to dress
28287|That baby sleeping there!
28287|He knows it is a drowsy child,
28287|And therefore it is very wild:
28287|For he ne'er goes to bed--'tis true--
28287|But he is there, and he is fast asleep.
28287|My dears, he says, if you can look
28287|In your writing, he'll just show you just
28287|How very fond you were of me,--
28287|And for how long I loved you so!
28287|And 'tis a very different thing
28287|He reads me, as he so loves me,
28287|Just to think and write of me.
28287|The moon's a very different thing
28287|I should prefer to read to him;
28287|The dogs do bark, the parrake buzz,
28287|And so do all upon me.
28287|One day there was a certain man
28287|Who said, "I love you better,"
28287|Because he had four teeth, they say,
28287|And still was most inquisitive.
28287|They sent him to a stylish drive,
28287|And, though they griev'd him greatly,
28287|They gave poor vulgar names--I trow--
28287|The names--and what do you think?
## Sample 2:
1852|He sees his wretched wife's fate, and dies.
1852|"And she, perhaps, may die in future time."
1852|In Moore's "Ballad of Trees and stones"
1852|A friend to Boston, now far known,
1852|In Boston town, "Is Alexis," sung
1852|A strain of triumph, and these words she had:
1852|"The city's honour, name, and wealth have I!
1852|"For this, a man of blood, toil, battle, strife,
1852|Mechanics labor, die, like this of life;
1852|The whole world's fame, the wind of it, the wave
1852|Of some great deep, that rolls to heaven or cave;"
1852|In that same song, by all the prophets sung,
1852|It is the song to tell the mighty throng
1852|Of those who for their country's honour take
1852|Life's journey to that sacred hill of Fame,
1852|When with great souls, that have been, and are still,
1852|The first great men, to do great things for ill;
1852|When, from no other world, the world's heart breaks,
1852|That noble man who, though he singly speaks,
1852|Has less of good to do than such a song,
1852|And lives in little more than we of him.
1852|What cause has he, this loss of friends and kindred?
1852|What hope of fortune? Wherefore has he perished?
1852|Why is he living on one man's estate?
1852|The poet sings. Where goes the soldier?
1852|The soldier dies. One man lives, all is one.
1852|Let him by no means lily stand alone,
1852|For one man is not worthy of it--all
1852|Is to be tasted by a thirsty sword,
1852|Which, while he lives, will drink up all he can.
1852|There's good in war, but better does he die.
1852|No, this, no more--that man is not the man.
1852|He may be used to giving freely what
1852|He gave, that he should dare
## Sample 3:
392|So many a noble man he scorns in fight,
392|Away he wanders, and his heart declines
392|To be among the dead, and live in death;
392|And now he meets with fear and trembling fear;
392|His courage fails, his faith is hardened, dead,
392|His courage fails, he cannot hope to appear
392|Forward he flies, now threatening danger dread,
392|He calls his lords, and charges them and flies;
392|In dust and ashes, as in sands that bee,
392|Now drives his foot, or in the grass that lies.
392|The trembling sailors, in their care to keep,
392|Now of their craft, and now of theirs surprise,
392|As if they wanted courage, speed, and sleep,
392|And all their thoughts on their audacious prize.
392|The watchful Franks at last had seen this feat,
392|Their flags down to their knees out-stretched they lay,
392|Their head all bare, their helmet on their feet,
392|Their bodies all of gold, their locks of gray,
392|Their bodies all so fair that death might deal
392|The blow, and break the shield, the hauberk too,
392|They drew their swords, and on their backs struck through.
392|After the first they flew in fierce career,
392|Like devils monsters come from hell and fire;
392|They changed their hue, their tunes, their forms, their cheer,
392|And drew their weapons; what their foes desire,
392|They turn and fly, and both their bodies spare.
392|The hosts, whose courage, arms, and arms, were fled,
392|Yet no dismay it had, when, with a spell,
392|The king's enemies were forced to stay,
392|That night each knight was used to aid them well.
392|But now the day drew on with dreadful ending,
392|The Franks, in fury, spurred them on their way,
392|And, for that great king's fall, had turned their backs,
392|And fled, before that host, to God and day.
392|To God or mistress, had they made request,
392|Save of his presence to those lords of rest,
392|That they would go, ere</code></pre>
</section>
<section id="invictus-william-ernest-henley" class="level4">
<h4><a href="#invictus-william-ernest-henley" title="Link to section: §' , William Ernest Henley'">“Invictus”, William Ernest Henley</a></h4>
<p><a href="https://en.wikipedia.org/wiki/Invictus" class="docMetadata" data-popup-title="Invictus" data-popup-author="English Wikipedia" data-popup-abstract="<p>&quot;<b>Invictus</b>&quot; is a short Victorian poem by the English poet William Ernest Henley (1849–1903). It was written in 1875 and published in 1888 in his first volume of poems, <i>Book of Verses</i>, in the section <i>Life and Death (Echoes)</i>. It shows how Henley never lost hope and kept faith in himself and faced the struggles unafraid.</p>" title="Wikipedia: Invictus">“Out of the night that covers me,”</a>; no consensus, 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
27139|And the stormy winds that blow
27139|All day long across the world
27139|I shall fly from earth below.
27139|"Oh, I am so weak and bowed!"--
27139|And the sun, in dazzling pride,
27139|Shook, and loosened as he heard
27139|All the turmoil of men's rights.
27139|In the town I should not be
27139|Wasted, wounded, and tormented;
27139|Made my spirit drunk with joy
27139|In the wonder of the sun.
27139|Oh, to see the sun shine through,
27139|And to hear the voice I knew,
27139|And to kneel because of old,
27139|And to speak, though it were gold!
27139|How it works, how it glows,
27139|In the sun, in the moonlight,
27139|And on earth, in the stream, in the odorous air,
27139|For the beauty of the moon.
27139|How it shines, how it glows,
27139|In the softness and the glow,
27139|And on earth, in the heaven, in the dusky air,
27139|Is the beauty that we know.
27139|Little, little, little sun,
27139|On your breast like the dawn of day,
27139|How I know about you always,
27139|With my heart and soul astray.
27139|Oh, to shine forever in your light,
27139|To shine forever in your beams,
27139|To shine forever in your brightness,
27139|To be shining, and to be!
27139|I knew the birds were going
27139|Out in the fair,
27139|Singing and working away,
27139|Going and staying and waiting
27139|Just as it might be.
27139|But the song of the children's singing
27139|Came every day to me
27139|Sweeter, and more than the singing
27139|That I may ever see.
27139|It rang so sweet, so clearly
27139|And far away,
27139|Making the angels, after
27139|Their work, but not a
## Sample 2:
38839|Where are the hours when we shall sleep?
38839|The night grows deeper, ruddy now,
38839|And on the east the morning light;
38839|No longer are the tears that weep,
38839|The noon-tide's drowsy murmuring
38839|That rises up through crimson leaves,
38839|As though the soul had fled away,
38839|To hide forever in the day.
38839|How sweet is the valley of Noodles!
38839|How quiet the little brook-heads there!
38839|What little bells by the cottages stood
38839|Tolled 'neath the heavy hours' flare!
38839|What tinkling bells the brook-folds heard
38839|As they crotoned by the millstone there?
38839|How soft the little brook-faces were
38839|Dimpled beside the cottage-door,
38839|And from the windows and out the trees
38839|The long-lost love-notes peeped once more.
38839|He came: the little brook he loved;
38839|And the brook beside his hearth he loved
38839|Where the crook whispered low, and the grasses
38839|That fringed its little patch of moss
38839|Beneath, the mossy water flowed.
38839|He came, and in God's blessed air
38839|He felt new life in that sweet air,
38839|New life and everlasting youth,
38839|And morning freshness, spring and sun.
38839|Weep not for me; our Father sent
38839|His little ones up to be content;
38839|As He had promised our poor ones
38839|We're glad to be thank'd like them;
38839|And let our taker daily say
38839|We're spared for our old love to-day.
38839|A rosy red rose is the rose
38839|And dear to me as children use;
38839|And though we sing it day by day,
38839|We know that mother's waiting here,
38839|And mother is the waiting-maid.
38839|God's plenty here, and He has sent
38839|Two
## Sample 3:
25953|The night that gives its glory,
25953|And the blue skies of the heavens;
25953|Sidney were worthless in that sight,
25953|All in a narrow space apart,
25953|I only had my life preserved,
25953|Left to myself to serve and guard,
25953|To guard and cherish, rest and guard,
25953|Of me the noblest was the king.
25953|Thus has my soul its homeward way,
25953|Thus has my soul its strength obtained,
25953|That what is dark may there appear,
25953|But evil in the light it feared,
25953|And in the light now burns my brain.
25953|With such a life, a wondrous life,
25953|Sounding through many realms and seas,
25953|The soul to millions makes at ease,
25953|And when it leaves the pleasant place,
25953|Binding in union is of grace.
25953|He who the body's life confers,
25953|From aught that is not, never slights,
25953|And when his body's life is spent
25953|The spirit cannot be content.
25953|I asked him what way he went,
25953|How many leagues he had not seen.
25953|The foolish wish I dared to own,
25953|To give the secret now I dare;
25953|I hid my foolish thought away,
25953|And lest the truth should be revealed,
25953|I thus began to test my will,
25953|And thus the answer was expressed:
25953|"I wish, O soul, that thou wouldst keep
25953|At present, in thyself at rest:
25953|But thou from sense hast naught to fear,
25953|And I my truth would not deny.
25953|And now by strength to hold thee fast,
25953|I do thee scorn, and I will give
25953|Henceforth my soul, and seek thy love
25953|As of the race of gods above."
25953|Then quickly I the thought requir'd,
25953|And thereupon this answer grew:
25953|"O soul, what wonder hath our Lord,</code></pre>
</section>
<section id="pioneers-o-pioneers-walt-whitman" class="level4">
<h4><a href="#pioneers-o-pioneers-walt-whitman" title="Link to section: §' , Walt Whitman'">“Pioneers! O Pioneers!”, Walt Whitman</a></h4>
<p><a href="https://en.wikipedia.org/wiki/Pioneers%21_O_Pioneers%21" class="docMetadata" data-popup-title="Pioneers! O Pioneers!" data-popup-author="English Wikipedia" data-popup-abstract="<p>&quot;<b>Pioneers! O Pioneers!</b>&quot; is a poem by the American poet Walt Whitman. It was first published in <i>Leaves of Grass</i> in 1865. The poem was written as a tribute to Whitman's fervor for the great Westward expansion in the United States that led to things like the California Gold Rush and exploration of the far west.</p>" title="Wikipedia: Pioneers! O Pioneers!">“Come, my tan-faced children,”</a>; no consensus, 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
31314|Do you think this summer would be just one?
31314|Did not all the gods in their thunder-storm,
31314|Say, would make the trees, temples, rocks, and tops, . .
31314|And now would they think these trees didn't know,
31314|But they couldn't be built, I think they do,
31314|And would like to live on in a greener place,
31314|And the gods have made it to be a good history;
31314|And we haven't the time, you see, to say.
31314|The woods, the fields, are green,
31314|And the flowers everywhere.
31314|The sun and the grass,
31314|And the birds and the fishing-trees,
31314|Are quiet everywhere.
31314|And there is the sun,
31314|And the flowers everywhere.
31314|The sun and the flowers,
31314|And the flowers everywhere.
31314|The sun and the flowers,
31314|And one after one,
31314|Are quiet all around.
31314|But there is an end.
31314|There is another place.
31314|The end of the road
31314|To which to go.
31314|The end of the road
31314|To which to go.
31314|Ah, why do you go
31314|Into the garden there into the forest,
31314|And into the wood, and into the village,
31314|And into the field?
31314|Why does the rain
31314|Make me uneasy?
31314|Do you see,
31314|Do you see?
31314|Listen to me,
31314|I am in a hurry,
31314|And away.
31314|There is a strange thing:
31314|I have that alone.
31314|It is not at all like I married:
31314|There is a new man,
31314|Who has just been married.
31314|I have a new hat,
31314|That's in my hat.
31314|I wish I could find out another,
31314|But then there is one . . .
31314|Oh, why do you
## Sample 2:
19|Let us sing this song of mine:
19|Where I am, there I'm,
19|Tell the mighty, mighty sounding,
19|Ocean's awful son of old,
19|In the islands of the blessed,
19|In the groves of Arcadian
19|In his cradle, cold and cold.
19|We, the Fairies, we the children,
19|We the islanders, the bold!
19|We are all that has been fashioned
19|In the wondrous dreams of old,--
19|We, the revelers, the giants,
19|We the children, and the bold!
19|O the wondrous song of battle!
19|O the spoils of men of might!
19|O the spoils of conquest, conquest,
19|Where the many are not quite.
19|By the walls of ancient stories,
19|By the marble-mantled wall;
19|By the chains of dread OENEUT,
19|And the marvels of the fall.
19|By the ramparts of the giants,
19|By the caverns of the deep,
19|By the graves of men immortal,
19|By the caverns of the deep,--
19|By the temples of the Morning,
19|By the temples of the Night,
19|Where the warriors and the giants
19|Met in vision met in fight,
19|And the dying captive maidens,
19|Sat in still and stately light.
19|Child of Earth, too fair for sorrow,
19|Mother of the Light of Life,
19|Fairest daughter of bright radiance,
19|We have sung thee a new song!
19|By the ramparts of the giants,
19|By the temples of the night!
19|By the peaks of Alleghany,
19|Where the eagle cleaves the sky,
19|Trojan prince of fire and glory,
19|Comes in might the ivory-headed:
19|By the palaceer Hippolyta,
19|And the brilliant dame who leads him
19|Where the women weep and smite them
19|On the great steed Aristagrus,
19|Sorrowing, binds his loins of golden
## Sample 3:
1165|Bring us back our olden treasures.
1165|In the small white house with the crumbs for food and fire,
1165|How you cursed the stars and drummers!  I hear you drumming.
1165|Are you getting sleepy-hearted?
1165|Are you writing toiling while night was ebbing?
1165|Do you yet look in the face so white?
1165|How your furrows, how you pined away!
1165|Up the ladder--who would fain be strong?
1165|Who would hunger after freedom long?
1165|Are you being hungry, child, and tired?
1165|Mother, mother, come blow me a song.
1165|Out of doors a man came and sat beside me:
1165|He was black, he was white, but I could not see him;
1165|He was robed in a sackbut of very little gold,
1165|And the words that he said were, "Hotel-fever!" . . .
1165|He said:
1165|He has seen Barra's vision.  He is not old;
1165|He is not fit for the desert.  He can find
1165|Sight and freedom, voice and heart and mind,
1165|And he has not learnt to forget . . . yet he
1165|Has a vision of Paradise.
1165|But, alas! the thing is over,
1165|And there is no chance can take me.  That is why
1165|I looked, he said.
1165|The place is filled with flowers,
1165|With curiosities and secret pain;
1165|One has a face like those of flowers,
1165|One has an accent like a bell.
1165|The small sad music of my days
1165|Moves on.  The grassy fields and lawns
1165|Are not more silent than the stones;
1165|But one face moves beneath the stars. . . .
1165|All this is very beautiful, perhaps;
1165|The hills and woods, the fields and meadows,
1165|The clouds and clouds and all the sky,
1165|The sea's sky and the hills' sky.</code></pre>
</section>
<section id="the-love-song-of-j.-alfred-prufrock-t.s.-eliot" class="level4">
<h4><a href="#the-love-song-of-j.-alfred-prufrock-t.s.-eliot" title="Link to section: §' , T.S. Eliot'">“The Love Song of J. Alfred Prufrock”, T.S. Eliot</a></h4>
<p>“Let us go then, you and I,”; no consensus, 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
22142|When the summer comes again,
22142|When the birds on the sunny land
22142|Make the winter to come again,
22142|I may say that the happy hour
22142|When the harvest-time comes again,
22142|When the heart aches for the land of my love
22142|And the day is my heart's desire,
22142|When the harvest home is come
22142|And the days are my heart's desire."
22142|"Oh! what if we both would wander over the sea,
22142|Afar from the home on the lowland, and stray
22142|O'er the hillside and over the dale, as we stray
22142|O'er the hillside and over the moor, through the wood,
22142|By the light of the moon on the hillside and stray
22142|Till the golden mist o'er the landscape is gray.
22142|And often for me the olden log-house is seen,
22142|The cabin'd log and the swinging door,
22142|The house where I lay till the break of day
22142|Till the sun shone out and the shadows drew away,
22142|While the shadows still wandered o'er valley and hill,
22142|Till the heart stood still, till the hush of the hill
22142|Came o'er the meadow and wandered awhile,
22142|In the sweet early gloaming with autumn awhile,
22142|To lie in the light of the long autumn days,
22142|We two.
22142|The gray-haired woman that leaneth on my side
22142|And holdeth me fast in her arms,
22142|Hath bid me lie down 'neath the old oak tree
22142|That so thickly embosoms me.
22142|The greening spring came with its silent voice
22142|When the autumn leaves hang their green,
22142|And the winds from the woods whisper'd a strange
22142|remembrance of many a long vanished year,
22142|Till the gray shoon dropp'd 'neath the gray shoon's shade,
22142|'Mid the gray shoon's shade;
22142|But where is my home
## Sample 2:
30488|Over the plains and slopes, where the wild roses blow,
30488|And the low valleys and the brown hills meet the sea,
30488|And the winds hurry by with a cry, "We want you! we want you!"
30488|And we stand on the roadway, and walk in the rain,
30488|And weep for the dead that have gone from us--we are waiting,
30488|Waiting the word that the wind shatters over the hill.
30488|We have come back from the hills, from the plains where our
30488|blood was shed,
30488|From the fields where the grass was the ground and the
30488|grain;
30488|We have come back from the valleys, we have come back
30488|from the hills!
30488|We have come back from the plains, from the plains where
30488|the grass grew green;
30488|And now that the voice of our winds shatters the
30488|world between.
30488|We have come back from the plains, have we come back
30488|to the graves,
30488|And the wind shatters the breath from the hurrying
30488|ship o'er the waves.
30488|We have come back from the plains, have we come back
30488|to the dead years,
30488|And the winds cry aloud, "Go up, and seek
30488|the faces of the gods!"
30488|We have come back from the plains, have we come back
30488|to the years,
30488|And the winds call out, "Go up, and seek the
30488|wilder world appears!"
30488|We have come back from the plains, we have come back
30488|from the days,
30488|And the swift leaves quiver, the bright leaves dance on
30488|our happy way;
30488|We have come back from the plains, we have come back
30488|to the dead years,--
30488|For the voices of men, or the gods' high
30488|heavenly joys!--
30488|We have come back from the plains, and
30488|from the long ago,
30488|Where in the cold and the gloom, on the
## Sample 3:
27494|To meet old friend Angelico's,
27494|And kiss the little one's beard.
27494|We shall have tea in the library;
27494|We shall have tea in town;
27494|By my good friend, Michael dissentund,
27494|I wish all you gentlemen!
27494|The sun has a face as fair as ever,
27494|As fair as ever was seen;
27494|As fair as ever was seen.
27494|There's a smile on his beautiful lips, my boy,
27494|As bright as the eyes of a lark;
27494|As bright as the sun at day-break gray, my boy,
27494|As bright as the eyes of a lark.
27494|There's a smile on his lips, my boy, my boy,
27494|As soft as the kisses he yields;
27494|As bright as the sun at noon-tide's end, my boy,
27494|As bright as the smile of a lark!
27494|Oh, there's something quieter than sleep, my boy,
27494|That twinkles in every eye.
27494|There's a smile on his lips, my boy, my boy,
27494|That shines like a lamp on the earth;
27494|As soft as the sun at noonday gray, my boy,
27494|As bright as the smile of a lark.
27494|A soft, pure love, that endures but ill, my boy,
27494|A soft Persuader that calms not nor charms,
27494|A king of the world-flowers and balm of the sea, my boy,
27494|A bright-hearted lover that seeks and finds, my boy,
27494|A rainbow of Heaven without a stain, my boy,
27494|A rainbow of Heaven without a stain.
27494|A sweet, pure love, that abides in a magical land,
27494|A kingdom-king, and a world-workeen band;
27494|A queen of the wildwood, a maid that is only one name,
27494|The bright-faced one calls her by name.
27494|A sweet, pure love, that all other names name, my boy,</code></pre>
</section>
<section id="hamlet-william-shakespeare" class="level4">
<h4><a href="#hamlet-william-shakespeare" title="Link to section: §'Hamlet, William Shakespeare'"><em>Hamlet</em>, William Shakespeare</a></h4>
<p>“To be, or not to be: that is the question:”; some consistency, so prefix “1006”; 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
1006|Shall wretched mortals, for a certainty
1006|Of cowardice, from forth their secret places
1006|Take flight, and turn in view the point askance,
1006|From whence they can, on any object bent,
1006|So that the mirror may not abstain
1006|To 'GOD,' as in the sky grows colourd there
1006|The sunbeam.  If I had, I should prove
1006|The fountain's well-accustom'd form there.
1006|I should not be a word, that is not parting
1006|Betwixt the sun and moon, nor they, for sowing
1006|In opposite direction, doth import
1006|Their night and their moonshine to that shadowy land.
1006|Therefore into dark caves are we brought,
1006|Where underneath is alabaster, save
1006|Of dusky idol, in which shining moors
1006|'Cross-bows do us sometimes view.  Rumm, there,
1006|That consecrates the world, are seen below.
1006|How many stairs, as 't were, to o'erpass the threshold
1006|Of my triumph, when I did before thee,
1006|Enter'd, and were below, though light were not
1006|But in those hours to which this chant is not.
1006|For thereon is my mind alone intent,
1006|And this there be the singing souls, that wait
1006|Their Master's will.  If but e'en now thou choose
1006|To listen to this be melody sweet,
1006|Here may thy heart find rest.  If this thou hear,
1006|Thenceforth what argument there smote thy love;
1006|That, well perceiv'd of me in its ascent,
1006|Thou mayst at last the good from that have learn'd
1006|Of me made perfect, and of now prove o'ercome.
1006|That next I saw, which, hearing, thou shalt take
1006|For guide, arose from out my last embrace,
1006|As a good mother should, that her embrace
1006|Around each form Adams in her son,
1006|The same who
## Sample 2:
1006|Shall they not fear?--let one fear have his throat?
1006|No: for I see, and can perceive it not,
1006|That they, who were before me, were lamenting
1006|For the nature of the world, and in its light
1006|Have pity on their prison-gries, and therefore
1006|Spirits of low estate do not accuse.
1006|As oft along the still-perduing Nile
1006|Along the Indian isle the night-owl thronged,
1006|And with her breath had led them to the rivers,
1006|Thus each along the shore, in company
1006|With the fair consort of a lady, moved
1006|To supper with a working-place of eggs,
1006|Whereon all love was painted; and "O Lord!"
1006|The singer, as he finished singing, struck
1006|On the sea-shore at left and right below,
1006|Stay'd his steps, and thus in accents clear did:
1006|"Behold a wonder! behold a thing
1006|Which, whoso of mortal thing may tell,
1006|Hastes before him, dark and terrible,
1006|Of an eternal weal; and this accursed
1006|Foul thing, which may the doubting furnish try'd
1006|To feebor, I have set before him all,
1006|Who must to nothing turn.  But list: resound
1006|With me the patience, and the pity move."
1006|He then: "Now, sister!  think upon thyself;
1006|For how may I inform thee, if thou hearest,
1006|How soon and how slow time and means he takes?
1006|What world or language may at last he ask,
1006|Who to the world may'st give thee all things fair,
1006|Thou know'st, who hold'st it?"  Compassing that,
1006|And beck'ning the approach of his approach,
1006|I answer'd: "By new stars, and by George, spirit
1006|Of sign all cardinal virtues I'm assured,
1006|That wrong thy sight; but
## Sample 3:
1006|What became of thatch of yours
1006|That you sally forth so suddenly?'
1006|I did not know him; but he felt
1006|As freshening blood the air beneath: and tow'rds me
1006|He moved, and I towards him moved, and fond
1006|As father, and in words, and with the sweet
1006|And with the sweet comply'd; as he had will'd,
1006|I wept, and to his bosom lifting me
1006|Upward, and all the while distill'd my blood.
1006|Proceeding from the river, and I thence
1006|Shelter'd and still, I lifted up mine eyes
1006|Toward the sun upon the matin light,
1006|And looking, ever taking it in view,
1006|Again beheld it tow'rds the other bank,
1006|From those eternal Poubs where first it was.
1006|Unto the chasm precipitous, I found
1006|A narrow path and new to me expos'd,
1006|And to the left hand and the right mine eyes
1006|Directed, and between, on the sharp rocks
1006|Directed, and between, on each side me,
1006|And on the ground, were dark and cavern'd depths.
1006|Then to the right it spake, in semblance such
1006|As sometimes Indian women have beheld
1006|Saw in their dreams the latter realms of Hell:
1006|"Why partest from me with looks enchanted,
1006|"That farthest from thine eyes withdraw thee not?"
1006|I said it somewhat, and turn'd to the right,
1006|And when the chasm was gone, and I had reach'd
1006|The bottom, where the stairs were not beheld,
1006|Backward my eye alone its course directed,
1006|Directed to another care and deep,
1006|That in its bark it led me; and the sage
1006|To my recalling them returnd I not.
1006|"Of all that I have shown thee, since that gate
1006|We enter'd first, whose is the self-"</code></pre>
</section>
<section id="romeo-juliet-william-shakespeare" class="level4">
<h4><a href="#romeo-juliet-william-shakespeare" title="Link to section: §'Romeo &amp; Juliet, William Shakespeare'"><em>Romeo &amp; Juliet</em>, William Shakespeare</a></h4>
<p>“Romeo, Romeo! Wherefore art thou Romeo?”; some consistency, with 1006 popping up again as a prefix (Shakespeare perhaps is memorable enough for <span class="smallcaps-auto">GPT-2</span>-poetry-prefix); 3 samples:</p>
<pre class="collapse"><code>## Sample 1:
1006|If once thou see, the sport is vanity.
1006|But fix thine eyes beneath: the shameless'st thou.
1006|Ah! yet no better sight hath ever brought us
1006|To the destruction of Eve, from the rape
1006|Of the fair world, where wickedness lay hid.
1006|Ah! now before the good Augustus returns
1006|In daily deeds, let our pray'rs to the Furies
1006|Be changed; for that he longs counterfeited
1006|The world's adornment, which in books for us
1006|His saintly he still wears, who for us did
1006|Lays his beloved, us'd have thereby
1006|Been so enamoured of this roundelay."
1006|As it was spilt by him who severs the dry land
1006|And, each eve earlier than the setting sun,
1006|Wines underneath the rose that Tarquin leaves;
1006|So ravished I the more the fair, the more
1006|The great disdain, the pity, the long-sought,
1006|Toward the ribald nature of this law,
1006|Which the new world much sooner would be war
1006|With my sweet-will, the which the will hath oft
1006|Been usefully perverted.  I beheld
1006|Four mighty shades, and they besought for speech,
1006|But spake but slowly; whereto I re-turned,
1006|They showed me the grievous shame that clustered
1006|In their resplendence, who had all confounded
1006|With Adam, that Adam's fair abode
1006|Long time abode in peace.  "Remember," thus
1006|The teacher cried, "Adam, I the three."
1006|"And is this the true mettled nymph, that shine
1006|Thy twins now in th' ascent, as thou dost now
1006|Below?"  I said: and he, "My son, who know'st
1006|The art of song, and who but hear, may be
1006|Lest thou divide the loaves from five to six."
1006|Thereat the
## Sample 2:
1006|If truly the mad people of Raymond dread thee,
1006|Well hast thou 'scaped yourselves, they are already
1006|Each other's torment; and the wretches, mortals!
1006|Are to the shades of thy polluted ways."
1006|Whence she again replying, after us?"
1006|Purgator to the Red relates this; and Dante,
1006|"That spirit of the just hath spoken and said,
1006|'Purge now the mountain of the unquench'd, and make
1006|The sea of fire purge otherwhere he sees,
1006|Where Hood and other vain adventures 've led;
1006|Let the o'er-shadowing bridge ill-beseem'd
1006|Yoke up the mighty demons while they walk
1006|In different paths, and divers guides disentangle
1006|The tangled ways, so that no stork may turn them back;"
1006|And the good Master to me: "See thou findest
1006|E'en thus, how in the little one the race
1006|Is to be cumber'd with the broken blossom.
1006|That from this blow across theims of brightness
1006|Has not so virtue worried thee, it seems,
1006|That one with good intent in passing 'bove it
1006|Thou seest the harmonies of the mind,
1006|And therefore dost not comprehend them.
1006|That the disturbance which the sound invades
1006|Interpreteth not, maketh thy fancy
1006|Take in good hieroglyphs of everichoon;
1006|Thou seest that not only he who bids it
1006|Be, but that other who behind the hill
1006|Comes from step's shadow, comes in solitude;
1006|So that the evil one, who, in disdain
1006|Of creature freedom, 'bove the other waits,
1006|The same, who hoping in complacency
1006|To aid thee in thy loftier thoughts, like them
1006|I see now aids, which thee, thus overcome,
1006|With light returneth to thy mind from thee."
1006|Ill shapes that course which
## Sample 3:
1006|If thou no longer liv'st in the old age,
1006|To stop the running of the vein thou well;
1006|Thou by thy deeds hast purged away my youth,
1006|Strengthen'd by me in good deeds and old,
1006|And banish'd with my beauty from my song.
1006|O how much better were it I had Ruthven,
1006|With Beatrice and high excellence
1006|Of soul! to have companion'd such, who yields
1006|To her for intermingling of the world!
1006|But I see thee already clasp'd in thine,
1006|Not in thy only Heaven, but in the life
1006|To which thou crav'st it, and dost here renew it.
1006|This good companion, speak from whence it is
1006|That I should fear not, when the hour arrive
1006|That to your sighs a second time shall bring,
1006|Then with her grace, which that affection gave
1006|And your fair looks, which with her will I scan,
1006|Till my changed nature prove itself as yours."
1006|Whence her reply the thoughtful Sage imposed;
1006|"And I am He, whom thou didst long for, dead
1006|Cord de'il me not," he answer'd, "yet for thee
1006| hamm'st me.  Heart of Potentate was I,
1006|And life was in the willow.  That the words
1006|Which now I hear, shall marvel thee, wert thou
1006|A dearth in love.  Ah, how I long, long to
1006|For it am pain and grief!  that knowest well
1006|My soul's fair metes on earth!"  His sovran speech
1006|Bespake the meek thanks of the high Lord.
1006|I do not over 8 oceans turn," he then
1006|Began, "if new device can be of late
1006|Received in thee, tell us, who thou wast, and by
1006|The heels of th' mountain, times such words to me."
1006|"That I frizzano elsewhere is</code></pre>
</section>
</section>
<section id="jabberwocky-lewis-carroll" class="level3">
<h3><a href="#jabberwocky-lewis-carroll" title="Link to section: §' , Lewis Carroll'">“Jabberwocky”, Lewis Carroll</a></h3>
<p>Upon request, I <a href="/docs/ai/poetry/2019-03-16-gpt2-poetry-prefix-jabberwocky-100samples.txt">generated 100 samples</a> of Lewis Carroll’s “<a href="https://en.wikipedia.org/wiki/Jabberwocky" class="docMetadata" data-popup-title="Jabberwocky" data-popup-author="English Wikipedia" data-popup-abstract="<p>&quot;<b>Jabberwocky</b>&quot; is a nonsense poem written by Lewis Carroll about the killing of a creature named &quot;the Jabberwock&quot;. It was included in his 1871 novel <i>Through the Looking-Glass, and What Alice Found There</i>, the sequel to <i>Alice's Adventures in Wonderland</i>. The book tells of Alice's adventures within the back-to-front world of Looking-Glass Land.</p>" title="Wikipedia: Jabberwocky">Jabberwocky</a>”. Examining preliminary samples, the closest prefix was #24650, corresponding to <a href="https://www.gutenberg.org/ebooks/24560"><em>The Jingle Book</em></a>, Wells 1899, an anthology of humorous children’s verse (which makes sense). “Jabberwocky” itself does not appear in the PG corpus but the “Jabberwock” is mentioned in one of the poems in Wells 1899, the acrostic poem “An Alphabet Zoo”, so, close enough.</p>
<p>Some samples:</p>
<pre><code>24560|He found a foxy in the brake,
24560|A cunning fox of scarlet dye,
24560|And from that foxy followed make
24560|The scrawny fox in glee.
24560|He followed with his dam and horn
24560|To where the river-water runs,
24560|And as his living current on
24560|The river-water likes him up
24560|A mighty rocky heifer heaves,
24560|And in a single field, or twain,
24560|Shows like the yellow corn;
24560|And when the wind doth blow, so too
24560|Low in his bottom lies his head,
24560|And in the grass leaps up again,
24560|In fearful freedom unbetrayed.</code></pre>
<pre class="collapse"><code>24560|And while the sword-strokes gushed and ran
24560|A-gathering like a hurricane,
24560|The wildest war-cries, loud and shrill,
24560|Came surging through the hills.
24560|At times a wild yell rose in the air,
24560|From steep to steep, from moor to moor,
24560|A scream of horror and despair,
24560|Of death approaching neathernock,
24560|Of dying embers in the pore.
24560|"Thy shield, my son! thy sabre bright!
24560|My hands! thy lips! thy lips of flame!
24560|O hasten! I have done the same."
24560|It made the blood tingle and squall
24560|To hear that wounded warrior's cry;
24560|It made the life blood quickly fall
24560|On the proud swine that fought and died---
24560|"Say on! say on! thou wounded swine,
24560|Say on! thou hewer of the sanguine!
24560|Say on! thou hewer of the sanguine!
24560|Say on! thou hewer of the sanguine!
24560|Say on! say on! that pierced youth
24560|Who crossed the heath in days of yore,
24560|And when he crossed it, "Say on!"
24560|It made the blood tingle and squall
24560|To hear that wounded warrior's cry;
24560|It made the life blood quickly quail,
24560|To hear that death did soon draw nigh.
24560|It made the blood tingle and squall,
24560|To hear that death did soon draw nigh.
24560|"Say on! say on! thou wounded swine,
24560|Say on! thou hewer of the sanguine!
24560|Thou art not doomed to fetch a pail,
24560|'Tis not the ebbing blood so drain."
24560|And then it made the blood tingle,
24560|To hear that agony within:
24560|"Thou liest"</code></pre>
<pre class="collapse"><code>24560|"It is morn!" said the clover-bush,
24560|As the sun came up the sky;
24560|"It is morn!" said the cattle-cock,
24560|As it drifted by.
24560|Then loud cried the blackbird, and forth
24560|On the moor the clamorer fell;
24560|"The rusty ruts!" he cried, "is morn!"
24560|And he opened his little cage,
24560|And went to his morning cell.
24560|Two ravening birds sat at his feet,
24560|One flew away and one fell,
24560|The blackbird with a shining face,
24560|Sang these grand welcomes to his race.
24560|"Who is the Blackbird?" they said,
24560|"Singing high, sing low, sing low,
24560|Sings the song for the giver and light?"
24560|"I am the Blackbird---a joy befall
24560|To the young in the woods so fair.
24560|I am the wild-duck, which roams at will
24560|Up and down in the gulphyry rill.
24560|I, the giver of rivers fair.
24560|I, the joy-devoted mole,
24560|I, the joy-devoted mole.
24560|I, the humming-bird, which sings so sweet
24560|I, the giver of rivers fair.
24560|I, the bird-cail'd eggs, with a silver sound,
24560|I, the joy-devoted mole.
24560|I, the giver of goodly things,
24560|I, the swift-wing'd eagle, I,
24560|I, the joy-devoted mole.
24560|From the sunny, sunny south,
24560|From the sunny south,
24560|The swarm departed,
24560|But woe to every wicked wight
24560|That ever them befel!
24560|"O shame to every wicked wight
24560|That ever them beguile!"</code></pre>
</section>
</section>
<section id="gpt-2-345m" class="level2">
<h2><a href="#gpt-2-345m" title="Link to section: §'GPT-2-345M'"><span class="smallcaps-auto">GPT-2-345M</span></a></h2>
<p>In May 2019, <a href="https://openai.com/blog/better-language-models/#update">OpenAI released the next-largest model</a>, which increases the parameter count from 117 million to 335 million, an increase of almost 3x. The 345M model has increased layer depth &amp; more attention heads but apparently similar window size; as such, while it may not be much more able to maintain coherency across long samples, its coherency &amp; quality should be superior to 117M within each window, as it can absorb more knowledge into its parameters &amp; the increased depth may allow for more ‘thinking’ at each step.</p>
<p>The regular text samples from the 345M model struck me as somewhat subtly but noticeably higher-quality than 117M, so while I was hoping someone would supersede <span class="smallcaps-auto">GPT-2</span> entirely by releasing a more advanced model (like a large-scale Transformer XL or Universal Transformer, or even newer models like the <a href="https://arxiv.org/abs/1905.03197" class="docMetadata" data-popup-title="Unified Language Model Pre-training for Natural Language Understanding and Generation" data-popup-author="Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon" data-popup-date="2019-08-26" data-popup-abstract="This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling objectives: unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. We can fine-tune UniLM as a unidirectional decoder, a bidirectional encoder, or a sequence-to-sequence model to support various downstream natural language understanding and generation tasks.   UniLM compares favorably with <span class=&quot;smallcaps-auto&quot;>BERT</span> on the <span class=&quot;smallcaps-auto&quot;>GLUE</span> benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, our model achieves new state-of-the-art results on three natural language generation tasks, including improving the <span class=&quot;smallcaps-auto&quot;>CNN</span>/DailyMail abstractive summarization <span class=&quot;smallcaps-auto&quot;>ROUGE</span>-L to 40.63 (2.16 absolute improvement), pushing the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), and the SQuAD question generation <span class=&quot;smallcaps-auto&quot;>BLEU</span>-4 to 22.88 (6.50 absolute improvement)." title="'Unified Language Model Pre-training for Natural Language Understanding and Generation', Dong et al 2019">UniLM</a> which marries bidirectional &amp; unidirectional Transformers), I decided to train <span class="smallcaps-auto">GPT-2-345M</span> on the PG corpus to compare it with 117M.</p>
<section id="training" class="level3">
<h3><a href="#training" title="Link to section: §'Training'">Training</a></h3>
<p>This proved more difficult than 117M. The 117M model was already large, at 480MB for the whole, so making it 3x larger bloats it to 1.4GB on disk; and the <span class="smallcaps-auto">VRAM</span> use on a <span class="smallcaps-auto">GPU</span> is even worse: with 117M, a training minibatch of <em>n</em>=2 could barely fit on a 1080ti’s 11GB, but at 345M, <em>n</em>&lt;1! The main culprit seems to be the self-attention layers, as regular self-attention scales more than linearly, so <span class="smallcaps-auto">GPU</span> <span class="smallcaps-auto">VRAM</span> gets eaten up fast, and apparently 16GB might not have been enough for 345M either. While I have enough system <span class="smallcaps-auto">RAM</span> to train 345M without any tricks, my Threadripper <span class="smallcaps-auto">CPU</span> is still ~14x slower than a 1080ti, and if one guesses that 345M takes 3x longer to train than 117M, and 117M takes 1–2 days, and <span class="smallcaps-auto">CPU</span> is 14x slower, then that’s &lt;84 days for the poetry finetuning, which would not be fun.</p>
<p>To solve this, nshepperd extended his <span class="smallcaps-auto">GPT-2</span> training codebase to employ a technique OpenAI helped introduce (and presumably used in training <span class="smallcaps-auto">GPT-2</span>, although the <span class="smallcaps-auto">GPT-2</span> paper is silent on the details): <a href="https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9" title="Fitting larger networks into memory: TLDR; we release the Python/Tensorflow package openai/gradient-checkpointing, that lets you fit 10x larger neural nets into memory at the cost of an additional 20% computation time">“gradient checkpointing”</a>. Gradient checkpointing is a space-time tradeoff which throws away some of the intermediate states of a NN, potentially greatly reducing total <span class="smallcaps-auto">VRAM</span> use, but at the cost of some slowdown when those intermediate states need to be recomputed for doing the backpropagation; the slowdown, fortunately, turns out to be fairly modest.</p>
<p>The downside of gradient checkpointing is that for 345M, it is <em>still</em> not memory-efficient enough to train it just like 117M—the self-attention layers checkpoint nicely (as the <a href="https://arxiv.org/abs/1904.10509#openai" class="docMetadata" data-popup-title="Generating Long Sequences with Sparse Transformers" data-popup-author="Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever" data-popup-date="2019-12-23" data-popup-abstract="Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more." title="'Generating Long Sequences with Sparse Transformers', Child et al 2019">Sparse Transformers paper</a> remarks<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> apropos of needing <em>extremely</em> wide Transformer windows to accomplish <a href="https://openai.com/blog/musenet/" class="docMetadata" data-popup-title="MuseNet: a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles" data-popup-author="Christine Payne (OpenAI)" data-popup-date="2019-04-25" data-popup-abstract="<p>We’ve created MuseNet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as GPT-2, a large-scale transformer model trained to predict the next token in a sequence, whether audio or text.</p><p>[See also: <a href=&quot;https://arxiv.org/abs/1904.10509&quot;>“Generating Long Sequences with Sparse Transformers”</a>, Child et al 2019</p><blockquote><p>Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n ⋅ √n). We also introduce (a) a variation on architecture and initialization to train deeper networks, (b) the recomputation of attention matrices to save memory, and (c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. ]</p></blockquote>">MuseNet</a>), but it’s not enough, due to the giant word/<span class="smallcaps-auto">BPE</span>-embedding, which blows out <span class="smallcaps-auto">RAM</span> usage. (Although it’s possible nshepperd didn’t implement gradient checkpointing quite right for <span class="smallcaps-auto">GPT-2</span>, as the OpenAI papers don’t mention any difficulties related to the embedding or using gradient checkpointing.) His initial solution was to simply disable training of the embedding and train only the Transformer layers, reasoning that the generic English embedding probably wouldn’t need to be trained <em>that</em> much as the Transformer layers are where the real work is done; much later, it occurred to us that the Adam <span class="smallcaps-auto">SGD</span> optimizer was part of the memory problem, as, being an adaptive <a href="https://distill.pub/2017/momentum/" title="Why Momentum Really Works">momentum</a>-based <span class="smallcaps-auto">SGD</span> optimizer, it must store a mean/variance for every parameter to adjust its updates per-parameter, which greatly increases memory use (and which gradient checkpointing does nothing about); when we switched to simple <span class="smallcaps-auto">SGD</span>, that freed up enough <span class="smallcaps-auto">RAM</span> to re-enable the embedding. This is important in part because the learning rate for Adam &amp; <span class="smallcaps-auto">SGD</span> differs by orders of magnitude: a LR &lt;0.01 seemed good for me for <span class="smallcaps-auto">SGD</span>, but Adam wanted a LR more like 0.00001. With <em>n</em>=1 minibatches, the training loss is extremely noisy and it is difficult to see the impact of any hyperparameter changes for the usual hand-tuning, so nshepperd also implemented a simple ‘validation loss’ function, which was helpful toward the end.</p>
<p>So, the upshot seems to be that 117M can be trained end-to-end with Adam on a commodity <span class="smallcaps-auto">GPU</span> in 11GB <span class="smallcaps-auto">VRAM</span>; and 345M must be trained with gradient checkpointing, and one must choose between either fancy <span class="smallcaps-auto">SGD</span> optimizers or full end-to-end training including the embedding; and 744M (released 20 August 2019) can’t be trained at all. Toward the end, I switched from Adam+Transformer-only to <span class="smallcaps-auto">SGD</span>+all, and this seemed to drop my 345M-poetry validation loss by ~0.01 to a final 1.915 (which is not nothing, so perhaps the embedding did need some adjusting for a more poetic vocabulary).</p>
<p>In total, I trained 345M-poetry for 815326 steps (minibatch <em>n</em>=1), with an Adam LR ~ 0.00001 and <span class="smallcaps-auto">SGD</span> LR ~ 0.001, over ~7 days (4–13 May 2019) on 1 Nvidia 1080ti; the necessary training time, with the benefit of hindsight, was probably closer to 3 wallclock days. 345M-poetry converged to a final loss of 1.915, an improvement of ~0.1 over 117M’s ~2 loss (so, in some objective sense, which is indirectly related to generated poetry quality, one could say that 345M is 5% better than 117M). I had expected somewhat more quantitatively, so I wonder if more aggressive training methods like <a href="https://arxiv.org/abs/1506.01186" class="docMetadata" data-popup-title="Cyclical Learning Rates for Training Neural Networks" data-popup-author="Leslie N. Smith" data-popup-date="2020-01-15" data-popup-abstract="It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate &quot;reasonable bounds&quot; -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-10 and <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks." title="'Cyclical Learning Rates for Training Neural Networks', Smith 2015">cyclic</a> <a href="https://arxiv.org/abs/1608.03983" class="docMetadata" data-popup-title="SGDR: Stochastic Gradient Descent with Warm Restarts" data-popup-author="Ilya Loshchilov, Frank Hutter" data-popup-date="2020-01-15" data-popup-abstract="Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-10 and <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of <span class=&quot;smallcaps-auto&quot;>EEG</span> recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/<span class=&quot;smallcaps-auto&quot;>SGDR</span>" title="'SGDR: Stochastic Gradient Descent with Warm Restarts', Loshchilov &amp; Hutter 2016">learning rates</a>+<a href="https://arxiv.org/abs/1803.05407" class="docMetadata" data-popup-title="Averaging Weights Leads to Wider Optima and Better Generalization" data-popup-author="Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson" data-popup-date="2020-01-15" data-popup-abstract="Deep neural networks are typically trained by optimizing a loss function with an <span class=&quot;smallcaps-auto&quot;>SGD</span> variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of <span class=&quot;smallcaps-auto&quot;>SGD</span>, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (<span class=&quot;smallcaps-auto&quot;>SWA</span>) procedure finds much flatter solutions than <span class=&quot;smallcaps-auto&quot;>SGD</span>, and approximates the recent Fast Geometric Ensembling (<span class=&quot;smallcaps-auto&quot;>FGE</span>) approach with a single model. Using <span class=&quot;smallcaps-auto&quot;>SWA</span> we achieve notable improvement in test accuracy over conventional <span class=&quot;smallcaps-auto&quot;>SGD</span> training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-10, <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-100, and ImageNet. In short, <span class=&quot;smallcaps-auto&quot;>SWA</span> is extremely easy to implement, improves generalization, and has almost no computational overhead." title="'Averaging Weights Leads to Wider Optima and Better Generalization', Izmailov et al 2018"><span class="smallcaps-auto">SWA</span></a> would have worked if they were implemented in this codebase &amp; I had the patience to wait a week or two for multiple cycles? In any case:</p>
<ul>
<li><a href="https://mega.nz/#!XMl3UI7b!KNLvp5wuxe_WAgJwkMVDiyyNmDl9XDXuipl-dQ6Phow"><span class="smallcaps-auto">GPT-2-345M</span>-poetry model download</a> (1.2GB)<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></li>
<li><a href="/docs/ai/poetry/2019-05-13-gpt2-poetry-345m-5000samples.txt">5000 random <span class="smallcaps-auto">GPT-2-345M</span>-poetry samples (0.95 temperature)</a></li>
</ul>
</section>
<section id="samples" class="level3">
<h3><a href="#samples" title="Link to section: §'Samples'">Samples</a></h3>
<figure>
<img alt="" height="499" loading="lazy" sizes="(max-width: 30em) 80vw, 1400px" src="/images/2013-09-11-xkcd-1263-reassuring.png" srcset="/images/2013-09-11-xkcd-1263-reassuring.png-768px.png 768w, /images/2013-09-11-xkcd-1263-reassuring.png 1400w" title="Transcript: [Megan is sitting at a computer, and Cueball is standing behind her.] / Megan: 'Looks like computers will beat humans at Go pretty soon.' / Cueball: 'Wow.' / Cueball: 'That's the last of the big ones.' / Megan: 'Yeah.' / [Megan looks back over her shoulder at him.] / Cueball: 'Well, at least humans are still better at, uh...' / Cueball: 'coming up with reassuring parables about things humans are better at?' / Megan: 'Hmm.' / [Megan types on her computer.] / *type type* / [She leans back over her chair again and addresses Cueball.] / Megan: 'I made a Python script that generates thousands of reassuring parables per second.' / Cueball: 'Dammit.' / Computer: 'Computers will never understand a sonnet computers will never enjoy a salad comp---'" width="1400"><figcaption><a href="https://xkcd.com/1263/"><em><span class="smallcaps-auto">XKCD</span></em> #1263</a>, “Reassuring” (cf <a href="https://github.com/JackToaster/Reassuring-Parable-Generator" title="This is a Python script as described in XKCD #1263: 'Reassuring'. It generates thousands of reassuring parables about things humans are better than computers at every second.">“Reassuring-Parable-Generator”</a>)</figcaption>
</figure>
<section id="training-samples-1" class="level4">
<h4><a href="#training-samples-1" title="Link to section: §'Training samples'">Training samples</a></h4>
<pre class="collapse"><code>[285999 | 5982.87] loss=2.07 avg=2.29
Saving checkpoint/run1/model-286000
Generating samples...
======== SAMPLE 1 ========
1170|Thy soul, thy very soul is burning!
1170|I look, the old-time woman I love
1170|Comes in for a kiss.  The room returns
1170|A ghost of its faded glow the same,
1170|But the old woman is gone as before;
1170|The moon has come down with a veil,
1170|And the old woman sits by my side
1170|Like a new moon rising from my grave!
1170|One day, I saw a bird full ripe:
1170|And a little flower in my garden
1170|Was the signboard's sign of peace to be.
1170|Like unto one, it told of the spirit
1170|Who dwells with all things and can see them not.
1170|The birds of all colors sang freely,
1170|And the flowers of every hue and scent
1170|Ran on like a dance up heaven's stair,
1170|With songs and flutterings of the blue and red,
1170|And sunbeams, glittering the branches overhead,
1170|And the wild sunbeam streaming upon them.
1170|And my soul was filled with a sweet desire,
1170|A need that was strange and divinely dear;
1170|And the birds sang so that the day was gone;
1170|And the flowers came full when night was near,
1170|With full hearts of light and fragrance blown,
1170|And the flowers and the birds would sing again
1170|Toward the evening that would bring them near.
1170|And the sunbeam was white as the sea,
1170|The flower was white as the evening skies,
1170|The star on the rainbow was gay and bright,
1170|And the bird of fire was a flutter of wings,
1170|And the flower and the bird would sing once again--
1170|Till I knew my soul was full of its sweetest dreams.
1171|I see how the leaves of the trees fall
1171|In the sunlight of morning;
1171|And in many a wood, with a thousand sounds
1171|And a thousand eyes, a thousand suns to behold,--
1171|I see how the leaves of the trees fall.
1171|How they fall in summer on the slopes
1171|And the meadows and the streams and streams!
1171|How they fall in the winter, soft and slow,
1171|How they fall in spring!
1171|Yet this fall must ever be beautiful;
1171|I think of the leaves, I hear them speak,
1171|The stars in heaven see them pass and pass,
1171|The rains are silent with them in their fall.
1171|The leaves of the trees are lightening and falling;
1171|The leaves of the trees are falling;
1171|And the leaves of the trees are all of their leaves
1171|Fallen on the fields,
1171|And they fall, and they fall on the fields,
1171|And I see how the leaves of the trees fall.
1171|The stars of the heavens look above,
1171|And the leaves of the trees are falling and falling,
1171|And bright winds sing,
1171|As in gladness, softly, soft and low,
1171|Loud and low;
1171|And in every star--what a wonder is it!
1171|The leaves of the trees are falling on the plain,
1171|And light-begotten clouds are lifting up the leaves,
1171|That fall like drops of rain.
1171|They fall on the flowers; and a thousand roses,
1171|And a hundred white lilies,
1171|And a thousand roses, and a hundred blossoms,
1171|And a thousand white lilies.
1171|They fall on the flowers, and droop and decline,
1171|And sleep in the sun.
1171|The leaves fall on the grass with an eddy they weave,
1171|And a thousand golden lilies
1171|Dream of the night of the leaves that was swept away.
1171|Their dreams are done.
1171|The leaves fall on the fields. They sleep in the sun,
1171|And the flowers shall be green,
1171|For the leaves of the trees are falling in the sun,
1171|With their rich lights of blue.
1171|I go to the door to-night,
1171|And look in the

[286000 | 6003.19] loss=1.03 avg=2.28</code></pre>
<pre class="collapse"><code>[347999 | 52462.43] loss=1.98 avg=2.06
Saving checkpoint/run1/model-348000
Generating samples...
======== SAMPLE 1 ========
 closed, or wearily
24869|Lest some foul fiend should be o'erthrown,
24869|Our hosts of fiends in wild rage we
24869|Began with furious rage to fight.
24869|We met them at the ramparts high,
24869|In numbers few and in strength not few:
24869|Whence we, our life like water spent,
24869|Abandoned the walls and basely fled,
24869|And the great flood swept like a raging sea,
24869|That o'er the deep ocean's bound
24869|Comes rushing on a torrent sea.
24869|And now the fiend's fury we met,
24869|And, like an eaglet chased by an eagle,
24869|We met, O King, with our lives in stake,
24869|As if our lives were forfeit by the foe:
24869|For in the raging tide that swept
24869|Our spirits from the wall and low,
24869|We lost and were scatheless once more.
24869|No foe may stand the fury of the king
24869|Who comes so fierce and bold to fight.
24869|And ere night's dark clouds have passed away,
24869|Our fleet will fly, and, safe return,
24869|We shall be in our country won.
24869|So will he take his own, O King,
24869|As some strong bull that fights in sport.
24869|The fiercest foe we ever saw
24869|Has strength to fight, has courage long,
24869|Is he whose flesh is fiercest, bold,
24869|Or who has lost his princely sway."
24869|They heard with trembling hearts and fear
24869|The angry speech of Rávaṇ sad:
24869|In trembling hearts and fearful fear
24869|The brother of the King of Thieves.
24869|His anger at her words they felt,
24869|And many tears they hurriedly shed:
24869|The king who ne'er to others shed
24869|His rancour of unwholesome truth.
24869|Like some vast crocodile or snake,
24869|Like the vast crocodile or snakes,
24869|He who his pride of power would show,
24869|His brother whom he scorned and belied,(856)
24869|Saw the sad news with many a sigh
24869|To Ráma by that voice revealed,
24869|And thus he spoke, most pitiful:
24869|"If thou wilt fight with Ráma, then
24869|The giant King, thy vassal, may
24869|This city all take securely in:
24869|But thou with faithful Ráma fight.
24869|'Twere well that you the city bring,
24869|Our lives in danger should ye die.
24869|To-day and to the morrow, he,
24869|Great king of giants, may he meet,
24869|The giant king I know who stole
24869|From thee thy darling Lakshmaṇ, when
24869|Our fathers on the day of vengeance
24869|Saw thee in this city slain.
24869|To-day my son is born to thee,
24869|To-morrow to the King may he
24869|Thy Ráma and thy Vritra slay,
24869|And thy own heart-sick brother, too.
24869|To slay my brother thou hast vowed
24869|This day and day in turn to dwell.
24869|Why, Ráma, is the world thus full
24869|Of fierce wrath, full of deafening noise?
24869|Thy brother's life and thee's, I ween,
24869|Lie evermore in my eyes.
24869|For all a life like this can yield
24869|Is precious little: thou must play
24869|With thy own brother, and a prey
24869|To thy most furious foe consent,
24869|If Lakshmaṇ and thy brother die.
24869|Now I would fain the King of Thieves
24869|A thousand worlds in strife destroy,
24869|And all the race of fiends who dwell
24869|Deep in the wood, a million moons.
24869|I would not seek retribution here
24869|For thine own hand upon the dead:
248

[348000 | 52483.09] loss=2.75 avg=2.06</code></pre>
<pre class="collapse"><code>[531999 | 9443.46] loss=2.09 avg=1.97
Generating samples...
======== SAMPLE 1 ========
 better his life and wealth to him
2136|I will set the gold in store."
2136|He spake no word, but bowed his gray head low
2136|And left him swiftly, and he came
2136|Through the great doors his sons to see.
2136|And there within the court his sons brought
2136|The golden treasure, all bound in gold,
2136|The precious gems and the wise gems round
2136|They held, and bore them back and brought
2136|To King Olaf, and from them cast
2136|As gifts before the king his gold.
2136|And as he heard the sons bring in the gifts
2136|He spake unto King Olaf,
2136|"My mother, look ye here and see
2136|What hap lies layed before me here."
2136|On the ground the King his mother spake
2136|From the dark wall, where her breast showed white
2136|With misgiving and dread:
2136|"Nay, but this man's heart that did breed
2136|Fell full of bitter grief as mine shall fall,
2136|And grief is for the day that comes,
2136|And sorrow is for heart to know."
2136|"Nay, O King, to-day is good and fine
2136|Withal as glad and long to be;
2136|But to-morrow may bring mickle pain
2136|Withal for grief," the king replied.
2136|"And shall be."  "That will I say."
2136|Then rose and said, "My friend, I stand
2136|Before the king in glory now:
2136|"The gold now lies at my royal hand,
2136|The gems of my folk's great king there be:
2136|And from my mother, the dark woman, bear,
2136|The dark woman, the great lord's daughter fair
2136|And mother of our good king and lord,
2136|This golden thing:
2136|This ruby and this chrysolite,
2136|Gleaming bright with golden light."
2136|And Olaf spake, for quickly he spake
2136|Sooth and plain, in full or hidden wise,
2136|And as the king in council him eyed
2136|Stood to speak, saying, "Be it so, now,
2136|Here is for mourning and for mourning sake
2136|A costly gift for our father's sake."
2136|"Goodly gift as I ween," the king rejoin'd,
2136|"And I will take it all, I ween,
2136|Than for grief or for sorrow some wise need."
2136|And so spake the good knight Grettir, and said,
2136|"My son, let the gift-bag now bring too,
2136|With silver wrought in gold and satin wrought
2136|And gold, and then our lord and master true
2136|May go in as wont:
2136|"And we will bring it," the king replied, "with speed."
2136|And so he took the golden gift and laid
2136|The ruby and the chrysolite by him,
2136|Which the cold hand that laid it must give
2136|And let him lay it next before him
2136|Behind the arm to bring it to the king.
2136|And straight the two princes went to the king,
2136|But went in silence, gazing at the golden ring
2136|Unto the lord of gems, whose shining face
2136|With a low voice them said, "Why dost thou stand
2136|And look all gleaming in the kingly door
2136|Where kings and heroes pass?
2136|"I knew not that thy heart so glowed with hate
2136|As to look on me, though I stood there high:
2136|Now know I nought," the warriors said, "dear king,
2136|Thy great heart's wrath shall not thee displease.
2136|"Not thou thine anger or thy great displeasure:
2136|For now at last with death thy father dies,
2136|And in thee as in his sons doth the will

[532000 | 9459.46] loss=1.31 avg=1.97</code></pre>
<pre class="collapse"><code>[534999 | 1390.41] loss=2.18 avg=1.89
Saving checkpoint/run1/model-535000
Generating samples...
======== SAMPLE 1 ========
1728|the ground: the other two with their hands each on his
1728|sleeve laid flat against his face, and strove to cover
1728|himself with the cloak. Then each was moved by pity, and
1728|clothed him in the cloak. And the hero Alcinoüs rose
1728|straightway at his father, and gave him his spear, and spake
1728|in his turn to Alcinoüs, saying:
1728|'Alcinoüs, no longer endure to sit, as though a
1728|mere man, and let the great Menelaus come in; but come in
1728|and see the son of Nestor, the Achaean hero; and ask him
1728|all about thy deeds and thy father's house, whether to have
1728|stayed, or to have returned to thine home. Say, is he still
1728|living, or hath he now gone down to the house of Hades
1728|gods to abide by them?'
1728|Then the noble son of Nestor answered him, saying: 'Father,
1728|I was not yet the son of Menelaus, when a god set me
1728|here of mortal man; yet he will not lay an hand on me,
1728|unless I too go down to the house of Hades, through fear
1728|of Zeus, lest I repay the wrong that He hath done. My noble
1728|son has gone with thee to the halls of godlike
1728|Eurymachus, and hath vowed thee unto death with a spear,
1728|on account of thy evil counsels; and in a fit of anger
1728|he promised to slay thee by the hands, on pain of death.'
1728|Then Antinous, the son of Nestor, answered him and said:
1728|'Yea now, my son, I will go thy house and seek thy
1728|father, that he may tell thee all aright.'
1728|Thus he spake, and with him went forth the noble
1728|son of Nestor. Then Alcinoüs and the other Achaeans
1728|greeted the king, and each one offered gifts. But he did
1728|not speak to mortal man about his ransoming of the
1728|gods: as for the sons of Nestor, he made them wait a long
1728|time at the ships, for he was fain for gifts in form of
1728|merry or gold, and a goodly store of raiment, and himself
1728|a well-beloved son. In like manner he minded that the
1728|children of men should offer gifts for his sake, and
1728|ponder his counsel; and the other captains and counsellors
1728|would be listening in their halls.
1728|Now the noble son of Nestor had gone down to be dead,
1728|for his house was near, and the people thronged that way about
1728|him, and at the first dawn they found him, sitting
1728|at the entrance of his hall, where he had left his golden
1728|branch,
1728|and he looked great tears upon their faces. Then his spirit
1728|came not again to his own body with him.
1728|So long as he dwelt beneath the waves of the dark sea,
1728|some god had nursed him in the dark, but as soon as the
1728|winds swept him from the sea to the land, he fled to the
1728|tomb of Zeus. There a great swan lay in wait for him, the
1728|lordly Erystus, grandson of old Oceanus, and was first
1728|to woo him, being the first who came to woo a maiden,
1728|for surely there was none else in the wide deep to
1728|prevent him. So the lordliest son of Nestor was minded to
1728|offer him a goodly fair gift of goodly gold and bronze,
1728|and he did so, and the gift was the best, the fairest of
1728|all. And the king Alcinous and the other chieftains and
1728|favored of goddesses then brought him to the house of
1728|Zeus, who is godlike and lord of generous souls. And
1728|the blessed gods showed great joy of the deeds they
1728|had done in the sight of all. As for him he lay lying</code></pre>
<pre class="collapse"><code>[659999 | 5522.96] loss=2.36 avg=1.90
Saving checkpoint/run1/model-660000
Generating samples...
======== SAMPLE 1 ========
In vain: there is one true knight that we
1728|shall meet, if we seek it so, and there will be
1728|no need for anything."
1728|So he spake, and stirred the blood of Thrace, where
1728|the thick-sown reek of the battle had made a
1728|great pond, and the blood-drinkers were all weary
1728|of battle for their wine-youth, which they had got in
1728|their hands. And now they set fire to their braziers
1728|and went to bed, and their eyes still looked toward
1728|Ithaca, for they thought to see the haven called
1728|Lacedaemon, and the good news of Agamemnon.
1728|Now when they were come to their place, they feasted
1728|on plenty, and by great signs the Trojans led
1728|themselves about the gates, and when they had put
1728|everything of weight in, they took their stand
1728|straight on them, smote on their helmets with their
1728|hoofs, and the loud clamour of the fight was made
1728|heard and known; so there were great confusion and
1728|revenge in their breast, and the stout men
1728|thrust down their armour from them and dashed
1728|them upon the ground. Thus they fought in the
1728|thick of the battle. But the valiant Dardanian
1728|strong in fight found his strong men among the horses
1728|of the foremost, and caught them up in one. And he
1728|sitting by those heroes, put to their mouth a
1728|salt, and he sprinkled them with the drops of
1728|grapes, and spake and hailed them:
1728|'Friends, it is meet, O friends, that we should show ourselves
1728|good men, who know not warlike service by a name
1728|otherwise than this, a mind of mortal men: for war
1728|shows weakness in the hearts of men. Let us be men,
1728|and fight with heart and hand. As for this word of
1728|sorrow that ye bear after so long sorrow, know
1728|none the less that it is the will of the gods:
1728|we have seen great evils in our father's house, and
1728|this very day the people have perished for a
1728|little, for there was no man in them save but three
1728|old brothers, men of renown in the halls, yet they
1728|wrenched apart themselves and went their ways as wolves and
1728|foxes. And the daughter of Poppaea bare them sons,
1728|And the lord of the Credans, a man of evil mind,
1728|came at the fourth year to Mantinea: one came in the
1728|twentieth year, and begat seven sons, and he was
1728|the lord of the city of Mantinea, where he made an
1728|immediately for himself an offering from his own substance.
1728|Therefore we will not fight with these people, but let
1728|them take their way across the sea from the land, and
1728|the old man's tale he answered with bitter mouth:
1728|'"If this will not win ye to the passage of the
1728|sons, then bid your men stand guard at once upon the
1728|shield-wall, lest that too I take the sons of my
1728|father and sons-in-law in the house and see the
1728|evil deed done in all my flesh. As for me, I do not
1728|believe that I am of any worth whatsoever, for ever as now
1728|I am lying under the earth with bare shoulders,
1728|having lost even my manhood and the strength of my
1728|hands, and ever through the days of my life I am without a
1728|strength, and ever in the counsels of my heart I was
1728|strong.'
1728|Then wise Telemachus answered him, saying: 'Stranger,
1728|not till all these things have come to a pass and
1728|the people of the Achaeans make great feast and
1728|have a banquet, then wilt thou boast to thee of thy own
1728|courage and thine own strength in going and bringing
1728|them by strong armament over the sea,</code></pre>
<pre class="collapse"><code>[656999 | 2795.58] loss=2.49 avg=1.85
Generating samples...
======== SAMPLE 1 ========
 hope the night shall cease before the day.
27441|As the last year lies at last,
27441|And the last year's sun
27441|Ascends the hill-tops once seen in;
27441|So th' last year dies, and shall not die;
27441|For the day is spring's and spring's heir.
27441|When the last year sleeps,
27441|When the last year's day
27441|Darkens the hills,
27441|When thy light is no more, but only thy shade,
27441|Then will love make the hills and valleys glad.
27441|When the last year lies at peace
27441|And the last year's sun,
27441|Shines, as it before, upon thine altars alt,
27441|Then to thee, O God, th' offering will be offered.
27441|To the music of the spring-time we come in,
27441|We love to trace old Ocean's flow
27441|'Gainst the banks of every shallow lea,
27441|Or watch the rolling green of every hillock green
27441|Like gold in the dawning;
27441|We love to hear the sea run screaming and laughing--
27441|We love to hear the deep grow wilder and wilder--
27441|We love to hear the rolling of the sea-waves
27441|Crashing through all the coasts of the world.
27441|We love to watch the birds and the rising sun,
27441|Like a great stream through an isle of gold;
27441|We love to see the birds in their lovely green,
27441|And a sunbeam in their shining;
27441|We love to hear the sun beating, beating--
27441|It's a joy to be young when you're old!
27441|The spring-time's in its rosy prime.
27441|All the birds are flying;
27441|We're happy, and we love to be young, in summer too!
27441|'Twill come to us still, as old as we,
27441|To sing our story ere we sleep,
27441|And wake, and find the old world bright in the morning!
27441|_The_ WORLD, we know, is passing away,
27441|With all its pleasures and its woes:
27441|But one thing, sure, must remain:
27441|Our story, on the morrow, we'll tell.
27441|On the morning of the morrow I, the sea-king, lay in a lonely deep,
27441|And heard a strange old song;
27441|A sound like thunder came from the heavens, and it seemed the old sea
27441|The night has long been cold and dark and dead,
27441|Yet a ghost-like light gleams from the eyes of the stars,
27441|And a strange old song I heard.
27441|The starry heavens grow dim;
27441|The waves are foaming white and red;
27441|But I, in a lonely, weary way,
27441|Hear the great waves roar and roar,
27441|And the old sea-song still sounds on
27441|In the lonely hours of the night.
27441|Sweet-voiced as the misty-colored birds,
27441|The wind on my beach has been singing;
27441|Ah! who is this that sings of the sea?
27441|What are the songs of the sea
27441|That I have heard afar on the shore?
27441|'Tis a ghostly thing the winds are singing:
27441|'Tis a strange old song; O who is this that sings?
27441|It is many a summer o'er the waters
27441|After the sun has sunk,
27441|The sea-bird, high in heaven,
27441|Has never been seen by mortal eye;
27441|But he has been, and it has been well
27441|The waters passed, so he would stay.
27441|With the leaves of the forest all glisten,
27441|And the star of the sky above
27441|Looks lustily with his golden bars,
27441|And the trees are bending low;
27441|He brings the night a blessing, and he makes it clear,
27441|He lulls the weary stars to slumber,
27441|And he lulls the waters under till dawn may keep
27441|The dawning of the day.
27441|</code></pre>
<pre class="collapse"><code>Generating samples...
======== SAMPLE 1 ========
 let you know
9578|What God will never do to you in trouble.
9578|If it's through some brave, stalwart soldier-man
9578|You strive to follow, out-fighting him will spring
9578|Like the break in a wall the water makes
9578|Just when it's nearly over!
9578|A soldier's a soldier for life, and must
9578|Rise up on either hand, firm and growing,
9578|To the work he's called to do, unshaken!
9578|Your country calls you to it; rise up and do
9578|Whatever it asks of you.
9578|It may not be great fighting; may be home drubbing;
9578|May be a life of disgrace and shame;
9578|But never, never, last of any man who strives
9578|To do God's will for country or God's will for home.
9578|Your heart is in it; up and do your work!
9578|God bless you, soldier!
9578|And when the shadows lengthen
9578|And it's very dark inside,
9578|And you hear the droning of traffic in the street,
9578|And the wet fog of the night is about you,
9578|Remember that you are heroes and that your work
9578|Is never done well enough;
9578|That your battles of yore are not lost though you grope
9578|Wildly at your work, nor yet finished nor done;
9578|God bless you, soldier!
9578|Never let your fame be counted
9578|A victory or a defeat!
9578|God loves all great and mighty men,
9578|Not great or mighty in the least,
9578|And He knows that among the sons of men
9578|There are some who cannot reach Him!
9578|"He hath called his seer."
9578|Now that I've tried, God rest my soul!
9578|My feeble prayer failed of a dose.
9578|You'll find a lot of soldier-people, I'm told,
9578|Who'd like to be really great, and they'll try.
9578|I had a vision of a vision once
9578|Of two white boys, one who wore a hood,
9578|And one who, more like a black-hooded ghost,
9578|Held up in his hand the stick of wood!
9578|A soldier boy from the house before me
9578|Stepped forward in his work-shirt white,
9578|And, holding up my old rifle and my old butt-stock,
9578|He turned the wrench of the safety down!
9578|"You'll never," said the seer, as I stood at attention,
9578|"See such a rifle anyhow!"
9578|And that was a vision false and dim,
9578|An apparition of the past,
9578|Of a gray boy from the village of my childhood,
9578|And of my boyhood at home and here,
9578|Who had turned the wrench of the safety to decide
9578|Whether from that dark future or no,
9578|The rifle I would choose at the other's risk.
9578|Not much longer I had written to Colonel Taylor
9578|In the hope of getting prompt reply;
9578|But I found in Colonel Taylor's file a page or two
9578|Where I could search; and there myself said he,
9578|"If a black boy from the village of my childhood
9578|Were to go down into the world to-day,
9578|Would you send up your heart to him? And then
9578|Imagine how the earth would hear and bow,
9578|And smile on you and help you up, and set you free,
9578|And then send up its pitying bosom to you!"
9578|The sun sinks mournful down on the dying of day,
9578|And the leaves are turning into night;
9578|But let not your heart fail
9578|To mourn forever the boy,
9578|Who died before his birthday,
9578|For he dreamed of a great future, too,
9578|And dreamed that the world was his to share.
9578|He dreamed that his step was on the way
9578|To that bright and glorious thing,
9578|Where all who are gifted, gifted, shall go,
9578|To the far, far shore,
9578

Calculating validation loss...
100%|███████████████| 40/40 [00:17&lt;00:00,  2.34it/s]
[658000 | 3737.42] validation loss = 1.93</code></pre>
</section>
<section id="random-samples" class="level4">
<h4><a href="#random-samples" title="Link to section: §'Random Samples'">Random Samples</a></h4>
<p>Testing 345M-poetry, a slightly higher temperature felt warranted, so to generate <a href="/docs/ai/poetry/2019-05-13-gpt2-poetry-345m-5000samples.txt">5000 random poetry samples</a>:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode Python"><code class="sourceCode python">python src<span class="op">/</span>generate_unconditional_samples.py <span class="op">--</span>top_k <span class="dv">40</span> <span class="op">--</span>temperature <span class="fl">0.95</span> <span class="op">--</span>nsamples <span class="dv">50000</span> <span class="op">\</span>
    <span class="op">--</span>batch_size <span class="dv">10</span> <span class="op">--</span>model_name 345M<span class="op">-</span>poetry</code></pre></div>
<p>I also generated <a href="/docs/ai/poetry/2019-05-24-gpt2-poetry-yeatssecondcoming-500completions.txt">500 conditional samples</a> for Yeats’s “The Second Coming”.</p>
<p>Reading through training &amp; random samples, they <em>feel</em> noticeably more coherent; it feels easier to extract meaningful subsections which form reasonable poems. (In particular, the pastiches of classical epics or Dante have gotten remarkably good.)</p>
<p>Some further samples:</p>
<pre class="collapse"><code>======================================== SAMPLE 530 ========================================
16265|In their little room with the door ajar
16265|And the candle hanging on the wall ajar,
16265|I have come across the word "Rise"
16265|With a face as grave and flat as you please.
16265|The one thing I remember of "Rise"
16265|Is the way it makes you feel--so bad, so bad.
16265|And I've come across many words to-night
16265|That are so like "Rise"--so like--so vague, so vague.
16265|"Elegance," and "Artistic Vigour,"
16265|But "Rise" is far above the rest,
16265|And I cannot hear--or see--the word,
16265|I will just stop here (I'll stop if I can).
16265|If you don't know what "Rise" means, try.
16265|"Rise" says--but don't ask.
16265|Well, I believe that I do--
16265|And if you don't know what "Rise" means
16265|You won't either!
16265|It's when the sun makes his round
16265|On clouds a-shine;
16265|And the little stars come out
16265|From under the silver dawn
16265|To shine about me so.
16265|When the sun puts out his light,
16265|And closes his earth-door,
16265|And with shadows blackens the blue
16265|Of heaven's blue;
16265|When all the stars I own
16265|Have put their clubs in bed,
16265|And I lie just like a log
16265|In the deep, still earth;
16265|When I don't hear my mother
16265|Any more in the hall;
16265|When the cat is out of the kitchen,
16265|And my brothers and I
16265|Are up to something unsaid
16265|That we've long been keeping--
16265|I have been like to catch a cold
16265|For weeks beyond reach.
16265|And I know that this is so--
16265|And the fever that wrings
16265|My bones is a cunning one
16265|That will not go well
16265|With the morning.
16265|And when I feel all warm and well
16265|And young again,
16265|I have a thought--and I say it--
16265|Of the night that's come.
16265|'Tisn't much: a summer's night--
16265|To-morrow!</code></pre>
<pre class="collapse"><code>======================================== SAMPLE 570 ========================================
18007|With a face of a smile, and a mouth that was redder than wine,
18007|And a hand, and a mouth that was whiter than snow,
18007|And a voice that was softer than dew of the morn!
18007|And I leaned my face between his, and I held him--I
18007|Holded him, and I kissed him, and--ah, there's a stain!
18007|I held him, to prove him faithful, while I dreamed
18007|Of a face of a smile, of a mouth that was redder than wine,
18007|Of a hand, of a mouth that was whiter than snow,
18007|Of a face, of a mouth that was blacker than black.
18007|But all in vain; and the moon, the moon, the moon
18007|Came a-drifting, like a careless sister, and stole
18007|Away from the village, and left them aghast,
18007|When they found him at last in the house of his wife--
18007|Sick with love, with a heart that was blacker than black.
18007|One moment as he wept, and then, like a priest,
18007|He stood weeping, with a pallor on his face,
18007|He stood with his head bowed, and prayed, and was silent;
18007|Then he turned to his wife and said, "Wife," and I
18007|Shuddered--and kissed her, and said a poor thing, and bad;
18007|And--there's a stain!
18007|And then came the moon that came so seldom this year,
18007|And vanished so swiftly that I forgot to say "Good-night."
18007|And she answered, "Kiss me, my sweet; it is time."
18007|And she rose and went to the door; and then--I cried,
18007|And--there's a stain!
18007|Then I thought that I had forgotten all,
18007|All that I had felt and done and said
18007|For I had little to lose, and I had not much to say;
18007|But I knew what did come after I turned in the door,
18007|And that is, that the stains will never go away.
18007|What matters it whether the moon come again,
18007|Or the sun rise, or the little birds sing;
18007|That the little things that I have known and done
18007|Will hurt me still, and stain my lips with black.
18007|The moon is low, and from the windows, low,
18007|The little stars wheel in their canteens,
18007|Ripening for a night when the world is gray,
18007|In the west, far, long ago;
18007|Like flowers, they come, the stars of midnight, bright,
18007|Like flowers, in a cloud of gold and snow.
18007|The stars of midnight hang like ripples of wine;
18007|The flowers have bloomed and vanished away,
18007|Too early to be precious to us all.
18007|But the things I love the best, they have not died,
18007|And shine through the twilight, gold and red,
18007|Too late for our love, too soon for our tears;
18007|So dark and drear for their tender glow,
18007|But yet they are bright, for they will not die.
18007|In the land of shadows I will seek
18007|The star of night that shone above her urn;
18007|And she will laugh in the darkness again,
18007|When I have been gone with a heavy heart,
18007|And she will smile in her lonely home on the shore,
18007|When I shall come no more.
18007|Out of the West, out of the night,
18007|A star has come.
18007|He has rolled in glory before
18007|My soul and I.
18007|I think I have felt him before,
18007|But I never was human.
18007|Now the stars are turning into night,
18007|And the clouds are brightening.
18007|For I never was half so fair
18007|As he now is;
18007|And I never shall see him, only,
18007|Till our bodies meet, heart to heart,
18007|In the bosks of love.
18007|When all is</code></pre>
<pre class="collapse"><code>======================================== SAMPLE 610 ========================================
29700|Hark! from those shadowy depths thy voice
29700|Mournfully echoes, "AUTH".
29700|That was the night when from her fane
29700|Mangu Damoneo, her priest,
29700|Sang by moonlight in the glade
29700|Of her garden; but, alas,
29700|Her lovely form was gone.
29700|The night of sudden twilight,
29700|Of sudden funeral, laid
29700|The flowers of her beauty by,
29700|And left her lying dead.
29700|The mourners, with no pity
29700|For that fair maid, who had died
29700|So young and sweetly, they
29700|Drew from the crowded bier.
29700|Now the long twilight mourns
29700|In dark convulsions slow,
29700|The sun, whose brightness, when it climbs
29700|To meditate its last,
29700|Turns the great globe of heaven
29700|To a vast blackness; and the moon,
29700|In the blue distance lost,
29700|Waves her orb all palely pale
29700|O'er earth and ocean's bed,
29700|While, at her last sad funeral,
29700|The waters are still.
29700|"Where, O, where," the poet cries,--
29700|"Where is she, whom so fair,
29700|So pure a form has laid,
29700|Whose heart was true, and tender,
29700|And soft as summer air!"
29700|Yet there is sorrow in the words;
29700|For in the middle space,
29700|The grave is silent, but the sea
29700|Is louder still than I.
29700|The poets write how fair
29700|She lay in moulded tomb,
29700|A shape of beauty, soft of feature,--
29700|Yet still she's dead!
29700|I look to distant lands,
29700|Yet still I see her face,--
29700|How fair she lies in tomb!
29700|And all that Nature left,
29700|Which never came to pass,
29700|Is on her cold dead breast
29700|That cold dead face seems to look,
29700|As if his own it were.
29700|From sea to sea he sailed,
29700|And then upon the shore,
29700|The man-child found his mother's breast,
29700|The mother's bosom bare.
29700|On many a foreign shore,
29700|When sorrowed he passed along,
29700|The deep-embattled ocean cried,
29700|"A bitter heart, I've had!"
29700|The storms and the tempests of his youth
29700|He turned upon himself alone;
29700|And though the storm and the tempest had
29700|Their rage, he had passed with the wind,
29700|With God,--his only sign;
29700|And, ere the cloud of the tempest blew,
29700|His soul was with the world at play.
29700|He looked to the stars, and the stars smiled,
29700|And the moon in the heaven looked;
29700|And, as he looked, he beheld her light,
29700|And all the heaven smiled with him.
29700|When winds and tempests fly,
29700|When floods and fires fail,
29700|As their wake doth meadow and fen,
29700|'Tis the man-child's heart that craves.
29700|And I--I shall be bound,
29700|With the hoary-headed, strong, old,
29700|To earth, and the graves of the dead,
29700|Whose feet are mowed down, as they lie;
29700|And I shall rest my weary head,
29700|In the silence of Eternity,
29700|In the peaceful arms of God.</code></pre>
<p>Here is a ‘failed’ example, where <span class="smallcaps-auto">GPT-2-345M</span>-poetry imitates the scholarly apparatus that unfortunately contaminates the PG poetry corpus; it is quite plausible-sounding, even including plausible-looking Latin:</p>
<pre class="collapse"><code>======================================== SAMPLE 620 ========================================
38566|and of the two main branches of the lyric poets, he is
38566|of the first. This latter is more like a novel, the former
38566|a mere exercise in poetic technique. It is true that some of the
38566|longer poems are only in two or three of the seven acts,
38566|(as 'Cytherea and Amphithoüs', 'Odysseus and the Daughters of Pose)
38566|The metre of the whole of the tragedies is at most
38566|two or three distinct feet, two or three acts, or perhaps
38566|no more than one line of one act. But there is always no
38566|clear, distinct point of passage, and the metre may change
38566|after the action has begun, and in doing this the danger of
38566|distinction. As regards Lucan, we might as well look to one of
38566|the last days of the Republic, the 'Ante-ratu' of The Deed
38566|(cl. 27)--'Ad locum triplice trieter hoc sibi sibi dicas, etc.
38566|Præestus huic erat precatus in sæpe Lucania per Bettini
38566|Ergo sæpe diadema'--quoting an asp of the same name as the
38566|antagonist,--'Ad mihi quærent horribilem fratres' being a better
38566|means than the one named 'Hæcam.' And thence we can infer from this
38566|that the 'Ante-ratu' of the other tragic poems was a regular and
38566|accidental epigram of Lucan himself,--the kind of thing which
38566|Sextus in his Annales, 65 says of the poets of A.D.
38566|Mihi sæcula, quærent Hæc morbis,
38566|Nec coelo mutum modo vel fratris
38566|Rara ferus.
38566|If it is right to treat of the tragic poets of a later period, it
38566|should not surprise us that Lucan is so honoured in our day. Already
38566|GREEK, and still more after that, in the days of A.D. 60 and A.D.</code></pre>
<pre class="collapse"><code>======================================== SAMPLE 670 ========================================
36773|But not with the first men of Greece;
36773|Not so much the Greek itself
36773|As what this Greek gave and took from him
36773|His power to change what the past had been.
36773|The past he made more perfect than it was;
36773|Not for the beauty of the things,
36773|Not for the new things it is, and strange;
36773|But for an inner sense and power,
36773|The force of a spirit to penetrate
36773|Truly into Nature; the power of Nature's mind.
36773|But these are, in spite of words, a vain,
36773|A futile spectacle!
36773|For what were all
36773|But scenes of dream
36773|And scenes of glass,
36773|Till that which best is is at last all?
36773|The image of the true?
36773|Not this, as of old; not this, as of old
36773|When, leaning on his pike the Spartan king
36773|Spoke to his multitude like one man alone.
36773|Nor is it so with me; since naught can prove
36773|My nature's image; all I feel is known
36773|And knows what it is: the image of man;
36773|The body; the sense; the mind; the will; the blood;
36773|And so goes ever on to the dark heart of things.
36773|But this image, as the great natural bulk
36773|Of all the world, and the universal frame
36773|And home of all things, is a wordless sound,
36773|A little, but a strong one, heard and seen
36773|In the unspeakable calm of the supreme;
36773|That, like one man's word of it, "A little more."
36773|The soul shall know
36773|How, like the world of things,
36773|And like the thoughts,
36773|Like these our thoughts, our thoughts are many,
36773|Like the world of things,
36773|And like the sounds,
36773|Sounds of life heard and unseen,
36773|Like the world of sounds,
36773|Sounds of thought and thought, they are the same!
36773|Though what we think is like all things, like we,
36773|Though we be like one thing, not like all.
36773|This is the meaning of life.
36773|For each is like to him, like all;
36773|Though all like him are like to him, not all.
36773|Not like, but like how, and with these two
36773|The earth is like most in the sense of being strong,
36773|The rocks and the sky, sea and shore,
36773|And air and water, and all things like to blood.
36773|_With many a rose for the dead_
36773|_Is laid the flower of mine._
36773|_The flowers, the rose, the sea, the air, the water, the land._
36773|'For we are the pictures of our lives, and our ways
36773|Make up a universe.
36773|'We live life out in the open,
36773|And leave the rest
36773|To fancy._
36773|_The mind-for-life of the sea._
36773|'The mind-for-life of the sea
36773|Takes place for us, for us
36773|The little seas, the little woods,
36773|The little leaves, and the little birds,
36773|The little fishes, the little things that live
36773|The round world over,
36773|And love us with its love of the things we see;
36773|And the mind-for-life of the sea
36773|Is our outward life,
36773|And we would leave it undone, undone,
36773|Right, by ourselves,
36773|For any one to do it for us,
36773|Without any help,
36773|For any one to bear such pain,
36773|That we could never be content,
36773|For any one to stay,
36773|As we have been,
36773|And go on being, and on being.'</code></pre>
<pre class="collapse"><code>======================================== SAMPLE 740 ========================================
19385|"While his auld foot beats thae ances,
19385|Ah! daur ye bee at the auld bar?
19385|Ah! daur ye bee at the auld bar?,
19385|An' if ye have ten min'us ances,
19385|Daur ye be at the auld bar?
19385|There's a cauld, dingy bar,
19385|An' ae blythe young lady
19385|Was a' in his auld house,
19385|He had a fondness for her,
19385|She was his auld ha'."
19385|"I am a king o' the Irish clover,
19385|An' nae sic clogs o' gowlds I ken;
19385|But, whisht! I'm wae to hear men bleedin'
19385|At my reign o' the Irish clover,
19385|An' to see women's looves like wee diz,
19385|When the land that I rule is the land o' clover!
19385|I hae a wife an' a daughter fu' o' age,
19385|She lives down in the clover ha'--
19385|The land that I rule is the land o' clover."
19385|Oh, I was sae blest to leave the play,
19385|That, like a ghaist, rusts awa!
19385|On a windy hill, I heard the snowdrift sweep,
19385|But, oh! how gleyerfu' it was to die!
19385|I set me down upon the heather, where
19385|I heard the wild bee as it swarm;
19385|While the linties all were on me blithe and bonny,
19385|In a bonnie ring I lay.
19385|I slept the sleep that fyfu' the night end,
19385|I thought that I was left alone,
19385|But in a swoon I see the light is glint,
19385|I saw the light was west.
19385|My gowld was yellow, my heart was saft,
19385|My hopes o' long ago were fled--
19385|But lo! ilk bird's sweet singing on,
19385|I heard it far and near.
19385|I think upon my past, the while I'm casting
19385|Ae thought o' Heaven above me, yet
19385|I'm ever glad to be gane!--
19385|The land that I rule is the land o' clover!
19385|'Tis a saft breeze that gies me a kiss--
19385|'Tis a saft, fresh caftinge on my love,
19385|That waves upon my face,
19385|As gladness I do kiss, and he kiss me,
19385|Though he be sae slow.
19385|I canna get used to a kiss from your lips,
19385|But I'm wae to think that I'm blest,
19385|Whar the winds are blowing, and the rain's in bar;
19385|That they may be a-kissin' and a-kissin' at the same.
19385|The gowan-stalks grew high in the heather,
19385|The kye lay snug in their beds;
19385|And ne'er in the days o' our childhood
19385|Were we wint o' such joy;
19385|For, as life and its pleasures might come annee,
19385|So we fain was we to lie where the heather lay.
19385|Then he leaned o'er the heugh, and he laughed a hearty laugh,
19385|And the kye looked aghast, and the deil look'd asker anither,
19385|For their heads were sune to change.
19385|But the gowan-stalks they turn'd in the air,
19385|And the kye wad sieve awa',
19385|For, like a saut bird, O, saut were the tears
19385|That we shed as we fain had we seen, a wild bee on,
19385|But what could be a bee on!
19385|And sair, sae we sieve awa!
19385|And sair, sae we sieve awa!
19385|And we fain would lie where the heather lies, close</code></pre>
<p><em>Ganbare</em>, <span class="smallcaps-auto">GPT-2</span>-chan:</p>
<pre class="collapse"><code>======================================== SAMPLE 800 ========================================
27669|We never say "Thank you"
27669|(Except when it's very late!)
27669|But he's the kind that would go
27669|And do his very best,
27669|If you'd give him a hand
27669|If he only were mine.
27669|_And sometimes when we sit around the fire
27669|We talk of a thing or two:
27669|Of the boy that was mine when I was young--
27669|(No matter!)--we talk of the boy we used to know,
27669|When he played in the wood,
27669|When he went a bowing round,
27669|In the golden olden time;
27669|He was just another boy._
27669|It's a fine old wood and I've tried to walk
27669|Through the boughs above a few leaves,
27669|And I've looked where there's little green places,
27669|To the shade of a great blue sky.
27669|There's the sea, there's the meadows, the white road and yellow sheep,
27669|And there's the old, old, old road where I met him last._
27669|'Twas a wood that he liked; it seemed just green and fair;
27669|And he liked the boughs above the great blue sky.
27669|'Twas a field we used to pass, now he is far away;
27669|And he loves the meadows, the white road and yellow sheep.
27669|And he loves the field when he is dead, but he'd rather be
27669|With his little boy there in the woodland far away.
27669|When he's gone
27669|He's a rich young boy sitting in his mother's lap
27669|And that is why he'll not forget
27669|To kiss and to hug me when he's gone._
27669|_Oh! would you'd come back, this would-be friend,
27669|And we'd talk of a time when it hadn't been too hot
27669|Since we all set out; and how we'd walk
27669|Along the road again, where he'd left us.
27669|And when we'd reached him in the valley he'd say,
27669|With a smile, "Good-by, my dear, good-by!"
27669|And look at me with his dimpled smile;
27669|And I think we'd talk and joke about such things.
27669|And maybe we'd play at our old home game,
27669|If you'd come back
27669|And help to share in our old home joy.
27669|And when you are back
27669|It's just that we two can walk,
27669|Along the road again:
27669|We can watch the sheep and the grasses grow,
27669|And tell each other stories and listen to the wind
27669|Crying to the woods and calling in the trees
27669|Through the blue day, "Old dear, old dear!"_
27669|_As we were riding down that lane of blue,
27669|O'er the brown hill's crest;
27669|And there in the valley between our feet
27669|There stood a house._
27669|'Twas a tiny, simple-minded place,
27669|With a fence, and railings, and railings more,
27669|That made us feel so small.
27669|It could only be called 'Half Price' Town.
27669|There were houses up and down,
27669|In the distance we could scarcely see,
27669|To the horizon's end.
27669|_We sat, in the shade of a timber tree;
27669|Our eyes half closed, and our faces bent;
27669|And with a heavy heart we thought of home,
27669|And of the woman who'd died asleep,
27669|In the house we had grown to cherish.
27669|And the voice was quiet again;
27669|In the silence of the night,
27669|In the light of the moon and stars,
27669|And the night-wind we could feel
27669|The pulses of the sleeping and dying,
27669|In the quiet, moonless house.</code></pre>
<p>This is a peculiar one; it starts as a satirical poem but I can’t make out what it is trying to switch to partway:</p>
<pre class="collapse"><code>======================================== SAMPLE 950 ========================================
1279|The King of Wales is a very clever fellow;
1279|He's sly as a cat, and a thief as well,
1279|He's like to set round a lamb a score,
1279|To take and turn round, and a-mimicking her tone.
1279|But to crown my Lord of Leven's min' fun,
1279|He's just like a dunce in w'at he can do;
1279|He's all in a twirl if he's going to be tail,
1279|For the man that heaps things at Lord Brougham's feet.
1279|Lord of the Whinnybone and the Hooplebooroo!
1279|You're a wit all as fine as Sir Thomas Suckly,
1279|And I'll venture to say, in my noddle,
1279|That when all the rest are out to piss,
1279|Then your Lord of Leven is just the man to go.
1279|Now, the best and the brightest of our ancestors,
1279|Who bore the race formerly known as Bow-wow-wow,
1279|Were all of one gender, and of one sex,
1279|And yet they were men of many other breeds,
1279|And more and more there was variation in their breed.
1279|For example, of the male, who was black and of white,
1279|There was also a fair breed of females all over the country;
1279|And yet 'tis a fact that the best men of the day
1279|Were all of one sex and of one species,
1279|And yet they were men of many hundreds thro many centuries.
1279|So there never was anything like it--there never was,
1279|In any age or clime, a breed like to the first:
1279|All were of one sex and of one sex combination--
1279|So, you see, they were men of many hundreds,
1279|That were all of one sex, and of many thousands,
1279|Whose species combination was myriad times more numerous than yours.
1279|In every locality from Cornwall to the Cape,
1279|There were divers sorts of males, and divers sorts of females,
1279|In every locality and every rank and class;
1279|For example, of the first five generations,
1279|There were divers sorts of weds, and divers sorts of brides;
1279|In every locality, and every rank and class,
1279|There were divers sorts of housings, and divers sorts of shoes.
1279|And therefore in every locality and rank and class,
1279|There was variation, combination, and change of occupation:
1279|There were divers sorts of women, and divers sorts of men,
1279|In every county and every town and town-house too.
1279|The first five generations were all of one sex;
1279|And this was probably why before the time of the fifth,
1279|There were divers sorts of brides, and divers sorts of men;
1279|For example, in the first five generations,
1279|The wife was the opposite of the husband--most probably;
1279|And the husband was usually a bachelor, or a man,
1279|With another wife, and the same sex as his wife,
1279|In every county and every town and town-house too.
1279|In every county and every town and town-house too,
1279|There were divers variations of dress and apparel;
1279|Some were dressed in crape, and crape-cloths and bonnets,
1279|Some in kirtles, and some in hose and hose-at-heel;
1279|Some in high-heels, and some low-heels, and some hose-nigh-loose,
1279|Some in high-heels, and some in hose-nigh-loose;
1279|And some wore noddles trimmed with blue or white,
1279|And some with knobs, and some with rings, and some with rings;
1279|And therefore there was plenty of noddles for all ranks and classes.
1279|The last five generations (which I shall say was one
1279|generation before the last), were all somewhat further off
1279|than the last, and thus left me to interpret
1279|The names of the divers divers divers generations.
1279|A woman in red and white was as happy as a girl;
1279|</code></pre>
<p>This one I think must be a mix of <a href="https://en.wikipedia.org/wiki/The_Song_of_Hiawatha" class="docMetadata" data-popup-title="The Song of Hiawatha" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>The Song of Hiawatha</b></i> is an 1855 epic poem in trochaic tetrameter by Henry Wadsworth Longfellow which features Native American characters. The epic relates the fictional adventures of an Ojibwe warrior named Hiawatha and the tragedy of his love for Minnehaha, a Dakota woman. Events in the story are set in the Pictured Rocks area on the south shore of Lake Superior. Longfellow's poem is based on oral traditions surrounding the figure of Manabozho, but it also contains his own innovations.</p>" title="Wikipedia: The Song of Hiawatha"><em>The Song of Hiawatha</em></a> and the <em><a href="https://en.wikipedia.org/wiki/Kalevala" class="docMetadata" data-popup-title="Kalevala" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>The Kalevala</b></i> is a 19th-century work of epic poetry compiled by Elias Lönnrot from Karelian and Finnish oral folklore and mythology.</p>" title="Wikipedia: Kalevala">Kalevala</a></em> (but if a wizard offers you rainbow-colorful draughts of rum strained through his magic red beard, I suggest declining in the interests of hygiene):</p>
<pre class="collapse"><code>======================================== SAMPLE 1060 ========================================
5185|"To the fish-lake of Pohyola,
5185|To the lake of honeyed Hiisi;
5185|Weep Iwi, weep upon Lempo,
5185|On the blue rock of Hiisi;
5185|Weep the god whose tears enrich;
5185|Weep the hero who subdued
5185|By his iron-handed arms
5185|This enfolding water-brook,
5185|By his magic hurl the rock
5185|To the upper deeps of Hiisi!"
5185|On the floor of clay they throw it,
5185|On the stone on which they write it,
5185|Spake these words in magic measures:
5185|"Lo-ye-Yo-Ru-Raka, son of Hiawatha,
5185|Take this magic measure,
5185|Words of ancient prowess!
5185|Put your senses into it,
5185|Take these mental forms into it,
5185|Turn to toe the enchanted drink
5185|Of the ancient, good, and wise Wabun!"
5185|As they twirl the mental measure,
5185|As they pour the mental formulae,
5185|Words of magic potency
5185|From the wizard's magic red beard
5185|Stream into the magic waters,
5185|Rainbow-colorful draughts of rum
5185|From the six-pending arts of brewing.
5185|Thus at midnight, hour of darkness,
5185|From the six-pending arts of brewing,
5185|Munchausen' of the spirit SAMURAI,
5185|Bitter his bitter sorrowful experiences,
5185|Tears flow to see the brewing measures,
5185|Casting him dark as night to HIAWASH,
5185|To the dancing-bower of Winansi,
5185|To the palace of the Song-element,
5185|To the home of ancient Wabun.
5185|There he sees the wizard fire-breather,
5185|There he eats the fruit of fire-fruit,
5185|Warm the water in the blue-smoke,
5185|Honey of the fire-flies, Sahwa;
5185|Nevermore to rise from Sahri
5185|To the glittering skies of Hiawatha,
5185|To the shining islands in the ocean,
5185|To his home in Winansi's honey-lands.
5185|True he sings no more of former loves,
5185|Sings no more of former partnerships;
5185|Beauty and truth the singer forsakes,
5185|Joys and dreams of CHEWBEL tree-top hollow,
5185|Thus again he sings to little honey-paws,
5185|Sings but one sweet thing, one only,
5185|Sings of the honey-pastoral flowers,
5185|Sings not of the meadows golden-rod,
5185|Nor the soft, corn-fields resting in slumbers.
5185|Young again the wizard sings of combing
5185|Sunny downs and forests of awaking,
5185|Of the golden balls in linden-groves,
5185|In the nooks of meadow-lands adorned
5185|With the purple blossoms of the ash,
5185|And the golden globes of chrysanthemum.
5185|As he sings, he flays and bleaches piarmate
5185|Peppers, onions, garlic, and ears of corn,
5185|For a meal to make his night's banquet.
5185|Straightway Hiawatha asks the artist,
5185|Places in his basket heaps of grain,
5185|Hangs himself to earth in alley lowly,
5185|Lays his hands and knees upon the rafters,
5185|Drops his baskets of grain into annihilation,
5185|Sings in low, third parts, low alto-dingule.
5185|Finally he hastens to the stable,
5185|Finds the black-frost of the winter gathering,
5185|In the center of high rafters framing;
5185|In the center a famine-gathered mass,
5185|In the center a man and his banditti,
5185|In the rafters great store of grain reaped from snow-sledge.
5185|Homeward goes the artist, homeward.</code></pre>
<pre class="collapse"><code>======================================== SAMPLE 1420 ========================================
1381|The loon of the moon!
1381|In the night, to me,
1381|The air was full of stars;
1381|The wild bird piped upon the branch,
1381|And the blackbird, tuned his song
1381|To the sweetest note
1381|That ever a lark knew!
1381|And I heard it ring, above the sea,
1381|In the land of the dreaming sea,
1381|In the land of the dreaming stars!
1381|A star is born when the summer moon
1381|Is full in the sky:  she wets her tears
1381|To kiss the star and sigh to hear
1381|One sigh from a thousand stars.
1381|A star is born when a nightingale
1381|Sings in a blue-bell tree; and she
1381|With such a song and such a tune
1381|Fills all the fragrant heart of the wood
1381|With such a fragrance as is there,
1381|And she sings in the heart of it.
1381|A star is born when on the wildest seas
1381|An Indian maiden dances alone,
1381|Pale as a ghost in the moon's eclipse,
1381|And the stars and the wind sing from the foam;
1381|And as the moon is dimmed in the heaving deeps,
1381|The heart of the maiden lies high and free,
1381|Shaking her curls in the beating breeze,
1381|And her eyes are bright with ecstasy.
1381|A star is born when the wind-flower sings,
1381|Or the hawthorn bathes in dew;
1381|And the nightingale sings like a bird in the sky,
1381|And all the stars and the nightingale ring,
1381|And the heart of the wind-flower sings.
1381|A star is born when the snow-white swan
1381|Drops her white soul to the river:
1381|And the winds and the water sing with glee,
1381|And the white swan lifts her face to the sun,
1381|And the stars sing out in the glory of day:
1381|And the heart of the swan is happy in heaven,
1381|And the heart of the river sings in the sun!</code></pre>
<p>An alt-history where Germany won <span class="smallcaps-auto">WWI</span>:</p>
<pre class="collapse"><code>======================================== SAMPLE 1550 ========================================
937|And, as you will, I'll be bound to you,
937|And always be and never be forgot.
937|We were a little band of heroes then,
937|With our flags all waving by our side,
937|And our hearts all set on some deeds of fame,
937|And our eyes all full of good-will light,
937|When the Kaiser came along to help them.
937|He was a mighty thing of might,
937|And when with us our flag all flowed,
937|When all men called each other comrade,
937|When the earth was made for happiness,
937|When freedom's dawn was dawning on all lands,
937|Our lives were all to some great cause done
937|We might not be wholly silent there,
937|For our hearts all yearned to join with 'em,
937|And we could not all go quietly by
937|But stood to fight the battle hard to the end;
937|And so, long as we had hearts to move,
937|The Kaiser, to keep back our strong men,
937|Was always there to tell us that we were called
937|To fight for liberty---and all were we:
937|And every man must be a Kaiser
937|When our glorious flag we salute.
937|And now that there's a Kaiser in town,
937|The hearts we had at first to thank
937|For the end of the old war we went on --
937|To keep back the strong men to stand by us;
937|We are so proud to hold in scorn
937|The Kaiser's warning words back.
937|So when you hear our cheering chorus,
937|Look down in the faces of your foes.
937|And when you're called to your father's home,
937|Oh, just smile---and let him go by:
937|It was the Kaiser's own call
937|That your country was called to give.</code></pre>
</section>
</section>
<section id="tao-te-ching" class="level3">
<h3><a href="#tao-te-ching" title="Link to section: §'Tao Te Ching'"><em>Tao Te Ching</em></a></h3>
<p>The <em><a href="https://en.wikipedia.org/wiki/Tao_Te_Ching" class="docMetadata" data-popup-title="Tao Te Ching" data-popup-author="English Wikipedia" data-popup-abstract="<p>The <i><b>Tao Te Ching</b></i>, Chinese: 道德经; pinyin: <i><span><b>Dao De Jing</b></span></i>), also known as <i><b>Lao Tzu</b></i> or <i><b>Laozi</b></i>, is a Chinese classic text traditionally credited to the 6th-century BC sage Laozi. The text's authorship, date of composition and date of compilation are debated. The oldest excavated portion dates back to the late 4th century BC, but modern scholarship dates other parts of the text as having been written—or at least compiled—later than the earliest portions of the <i>Zhuangzi</i>.</p>" title="Wikipedia: Tao Te Ching">Tao Te Ching</a></em> (<span class="smallcaps-auto">TTC</span>) is a famously enigmatic text, written in a difficult style in an more difficult language, and because of the challenge, has attracted many highly-varied translations.</p>
<p><a href="https://twitter.com/HW">Hiræth Weltschmerz</a> compiled <a href="/docs/www/decaut.org/e54057a5a624dcea3604a899990389811c7d0814.html" rel="archived alternate nofollow" data-url-original="https://decaut.org/situ/index.php/ttc-compilation/" title="(Original URL: https://decaut.org/situ/index.php/ttc-compilation/ )">~270 translations of the first verse of the <span class="smallcaps-auto">TTC</span></a> in one text (108Kb/19k words), and compiled per-chapter translations of the rest of the <span class="smallcaps-auto">TTC</span> as well (4.8M/870k words). Using <span class="smallcaps-auto">TTC</span> snippets to prompt <span class="smallcaps-auto">GPT-2-345M</span> didn’t produce good results, so he asked me to train a <span class="smallcaps-auto">GPT-2-345M</span> on the <span class="smallcaps-auto">TTC</span> to see what it did.</p>
<p>I converted the first corpus to Unix text format, replaced various escaped character entities with their Unicode equivalents, and replaced double newlines with single lines, and trained the original <span class="smallcaps-auto">GPT-2-345M</span> (I was unsure if using one of the poetry models would help) with the usual settings for ~6 <span class="smallcaps-auto">GPU</span>-hours, at which point it had reached a loss of ~1.8 and I began to worry about overfitting &amp; stopped.</p>
<p>I generated the usual ~1k unconditional samples:</p>
<ul>
<li><a href="/docs/ai/poetry/2019-07-19-taotehching-ch1-1ksamples.txt"><code>2019-07-19-taotehching-ch1-1ksamples.txt</code></a></li>
</ul>
<p>Some training samples:</p>
<pre class="collapse"><code>...
[12999 | 10468.36] loss=2.03 avg=1.96
Saving checkpoint/taotehching/model-13000
Generating samples...
======== SAMPLE 1 ========
 can find a way to rid yourself of these issues.
Forced to see them (the spiritual essence), there we receive freedom.
Free from spirituality, we receive nothing but desire.
These are not separate things. They are one in common. Together they are the Mystery.
The Mystery of the World is all illusion.
Everything we see is but a small part of the Mystery.
The true mystery is beyond understanding.
The mere awareness of the world is a mere shell.
By being aware we become a turd.
Those who try to hide the true inner essence will become trapped in the shell.
Those who hide their inner essences are free to become free.
These two pairs of differentiations arise from the same source: nothingness and everythingness.
Their differences are mysterious, but still they are infinitely subtle.
Beyond spirituality, they are the portal to all wonder.
The words we use to express the nature of these differences are not the same as the words we use to express the nature of other differences.
The same name may be applied to different things.
As a mere passing fable, it is the boundary of mysteries.
The real is infinite, but it has limits. The empty of the empty is the gate to all spirituality.
The name that we use to explain it, is not the name of the real.
It is the Mystery of all things; as "the ground of the profundities of the heavens and the abode of the ten thousand names.
The profound subtleties are discovered from the deepest minutiae.
The Uncomprehending is the gate to all mystery.
The unsearchable is the door of all knowledge.

The Tao that can be spoken is not the eternal Tao. The name that can be named is not the eternal name.
The nameless is the beginning of heaven and earth; it is the mother of all non-being.
The named is the mother of all things. From the named it is known what is above and what is under.
Words are ways of naming; words are names for things.
There is nothing beyond the limits of the mind and thoughts; there is something beyond the limits of the mind.
This vision that can be voiced in words is not eternal vision; this that can be voiced in words is not eternal words.
The indescribable is the essence of heaven and earth; it is the Mother of all things.
Exclamatory to the absolute is the threshold of all comprehension.
Descriptive to the absolute is the gateway to all differences.
Non-being is the limit of what is known. Exclamatory to the absolute is the threshold of all transcendence.
Non-being is the origin of heaven and earth. Exclamatory to the absolute is the gate to all diversity.
The name of the universe is not the true name of the nameless.
The Nameless is the gate to all mysteries.
Having a body we see, The body is the door of all wonder.
Hence, The unmanifest is the gateway to every wonder.
Names are ways of naming; Names are limits of names.
Unmanifest is the gate to all things.
The unmanifest is the mother of all things.
Nameless it is the mystery of the universe.
Existence is the domain of all that is.
Obscure is the domain of all that is known.
"It" is the beginning of heaven and earth.
The manifest is the Mother of all things.
This unity, Is the beginning of the universe.
Non-being is the Mother of all things. Non-being is the Mother of all things.
It is the mystery of the universe.
All things are (A Real) if they are perceived as a number, or unity, The mystery is the key to all things.
Names that can be names are not real. Names that cannot be names are real. Nothingness and all its aspects can be perceived as the subtlety of the mind.
The Namelessness or totality is the essence of all things.
Names that can be names are not real. Names that can not be names are real.
Nothingness is the Mother of all Things.
Nothingness is the mystery of all things.
Non-being is the Mother of all things. Non-being is the secret of the universe.
The Teaching which we can name is not the Mystery we can observe. The name that we can give is not the name we can give; but the Name that we can call is the Name that we can see.
The way that we can see is not the Way of the Past; it is the Way of the Past that is manifest, But without lasting, though enduring, we shall see the Unseen.
This we call the Mind, The Mother of the Past and of all Past: It is the gate

Calculating validation loss...
100%|███████████████████████| 40/40 [00:16&lt;00:00,  2.45it/s]
[13000 | 10503.37] validation loss = 1.81
[13000 | 10504.22] loss=2.17 avg=1.96
...</code></pre>
<pre class="collapse"><code>...
[15099 | 12666.34] loss=1.95 avg=1.78
Generating samples...
======== SAMPLE 1 ========


The maiden-maiden is the Mother of the Serpent,
The mother of all things, is called Mother,
Who has been named and is called mother, has no share.
He who has no name conceives
He who has an idea conceives
These two, existing without separation,
Have their root in the same source; In the same sort of source,
The One and the Being of the same and both differing in name,
Reaching out from the same source, Dissolves both.
Through these two there arises within all things a measure.
From these two measure and measure, They are called the World-Dao.
So, just as Nameless, let us look upon the mysterious depths of existence;
As Manifest, the sight of its manifestations.
To rid ourselves of desire, let us view its nameless outer form;
As we perceive its unmanifest core, we see its pure Inner Light.
This is the mystery hidden in the entire mystery.
As for the world, Nameless we see its beginning.
As Manifest we see its progression.
That which we are yet not filled with, Is filled with fire.
The fire of all-pervading Truth, Wherece from without can spring, From which flow all those things that are known.
Truly alone, viewing the Unconstructed, I see a mystery deep within the Unconstructed.
That which is Unconstructed is but the beginning of great things;
That which is Named is but the gateway to great things.
It is by working that the Heavenly Way may become manifest,
That through an effort that We may see its full extent; That is, We seek to see the hidden parts, We may see its mysteries,
that is, We must see the Unconstructed.
In that which is unobstructed, the dawn of all things is brought.
In that which is filled with things, the darkness of all things.
These two emerge and separate, Yet together, All things are called darkness and light.
They are both of One Origin, Which is the Gate to all Profoundness.

The Nameless is the door to Mystery,
The Manifest is to a Reasoned Way the gate to all hidden Mystery.

It is from mysteries which we shall not know, That we may approach to the Eternal, In order that we may obtain the Path of Life.

It is not always the case that, While nothing is experienced, The Essential Principle itself is hidden.
This state of Nothingness is the gateway to the Manifestation of all that is Eternal.
It is the origin of that which is lasting, Therefore it is the Emptiness which is the Gateway to the Finality.
That which is without beginning, It is the gate to all the subtleties of the Eternal;
That which has an origin, Being a process, Is the source of all the subtleties of the Eternal.
If you would know the Divine mystery, Become a Deserter. Life is a spring from a Deeper Mystery.


The unvarying Manifest is the Source of the Unvarying;
That which has a name is the mode of manifestation;
That which is unnamed is the mode of existence.

Unity is source of Manifest, Non-manifest the mother of the manifold.
Thus to watch, to wonder; to have desires and desires; All things, Manifest and non-manifest, To return whence to seek its way.

Tribulation is its mother.
The mystery of the Tao is the Body of the Tao.
The Essence of Tao is Tao, Tao the Cosmic Universe;
All non-being Is its mother.
Non-being is its mother Because it has the Secret Principle hidden,
Though without Secret Principle It is the Mother of all.
That Which Is Deeper Goes Before Deeper,
And That Which Is Secret Goes Before Secret;
They Are Called the One and the Mother of All Things.
The Name Is from Mystery,The Creature from Form,The Origins of all things
To perceive the Secret, Become a deserter
Desire Returns from the Secret: It is its Mother The Secret of all existence.
These words:
"Empty Words"
To see them Is to see the Secret,
To understand The Secret Is to understand The Origin
Its Dynam is Manifestness (Of all)
That is Its Mother (Of all)
That which is empty
Remain a Mystery
Beyond Mystery
Beyond Proserely
With all mystery in Its Depth
Has the Serpent walked
To understand Its Secret
Has become a Serpent
As for that which has a name It is the beginning of All Things
The One without a name has originated All things.

What arose is called the Essence of All Things.
These two that were separate
With opposite names But Ever Dissolving Is the

[15100 | 12682.90] loss=1.69 avg=1.78
...</code></pre>
<p>I then began training it on the full <span class="smallcaps-auto">TTC</span> corpus, which was split into per-chapter files. Remembering the problems with run-on poetry, I added the <code>&lt;|endoftext|&gt;</code> markers to the end of each file. (It would be better to add that to the end of each translation, but HW didn’t include any delimiters for each translation, and doing so manually would be too much work.)</p>
<p>This required substantially more training time.</p>
<p>Some training samples:</p>
<pre class="collapse"><code>...
[88999 | 6150.00] loss=2.33 avg=2.44
Saving checkpoint/taotehching/model-89000
Generating samples...
======== SAMPLE 1 ========
 Tao Tao's Way is universal. It covers all lands. Therefore those who realize it can accomplish all under Heaven. Therefore he who maintains the Tao, can be believed to know the Universe.

There is an ocean and a river, and all beneath the heavens aspire to be carried by them. The people of the Tao country follow these rivers and streams, as they may be carried. The people of the man of the Tao country follow the rivers and streams, as they may be guided; and the people of the man of the Tao country follow the rivers and streams, as they may be measured. All under heaven strive to be carried by the Tao Way, as they might be carried by Heaven. The great man, when he knows the Tao Way, can achieve all under Heaven. When a man who has completed the Tao is believed to know the Tao, this is shown by his ability to govern.

All Tao is infinite; to people like it, it is the very mother of all things. That which flows with the Tao is universal; that which opposes it is not. The Great King possesses the Tao Way, so that he can hold everything under heaven. To know Tao, one must be a man of Tao. To know the Great King is the same as to know Heaven. The Great King who understands Tao can hold everything under heaven. A man of Tao must know everything under Heaven, just like he knows Heaven.

True and Eternal Tao Tao can take care of all things that exist, without needing a reason. The person who is able to hear Tao is capable of capturing everything under heaven. Having enough listening ability, the one who can take care of Tao Is able to know all things in the universe. Having enough being able to listen to Tao, The one able to know everything under heaven is able to govern all under heaven. Thus he who is able to control Tao is the one who will be able to control the universe. Whoever knows Tao can grasp everything under heaven. The person of Tao has no limitations, therefore he is able to grasp everything under heaven.

By Tao I can all things be fulfilled, Without needing to get into details. All things call to the Tao Way Without needing to tell the other Tao people to be fit. I know the Tao Way And all things flow with me. If you're also able to think, You can capture the whole Tao Way. Knowing all things That come to you Can help you to know all things that come to others. When one can know Tao You can also master Tao. When the Master knows Tao He can master the Tao Way. And if you are able to master the Tao You can also take all things into your own hands. You don't have to control Tao or all things.

Under Tao, Everything is fulfilled. Tao never requires a reason for doing something. All things--- even beasts and plants--- seek out Tao. But because of Tao's inner being, All things have this one thing to do with it. Knowing the Tao gives you the inner clarity of Tao. But to have the inner clarity you also have to know that you are a human being. So one who is a Taoist learns that he is a human being. The Master of Tao learned that he was a human being. The Taoist who understands the Tao is not burdened by it; He has the inner clarity he needs.

The Way of Tao: All things call to it, Through but one source. Everything that desires the Tao Must either reject or destroy itself. The Great Thinker has Tao, And can guide all under Heaven. To have the Tao Knowledge is to know the mysterious Source. Knowing the mystery reveals the subtle power of Tao. The Supreme Man can conquer all under Heaven. If he knows Tao, he can be truly the Master of All. To know Tao is to understand the mysterious nature of Heaven. If you know Tao, you also can serve Heaven. The Great Thinker has Tao to serve Heaven, And the Taoist who has Tao to serve Heaven is equal to all things.

All things around a "Way of Tao" "says to its people, "everything"s the natural way,"' and everything that should reject it's it alone. Knowing everything about Tao, people accept Tao, and everything under heaven has Tao in it. To know Tao is to know all things. To know Tao is to know that Tao itself is all things, and the Master of Tao can know all things under heaven. Therefore, he who understands Tao can serve Tao.

Tao was born of the Tao Way It's there for you But not through words, and it's everywhere All things call to it, But not through its Tao Words will not accomplish the feat. You cannot hold on to Tao's place without grasping the primeval essence of this force of nature. Know Tao, or your Tao will know the universe It's how you grasp the primeval essence that matters Don't hold Tao hostage and force it to shape

Calculating validation loss...
100%|██████████████████████| 40/40 [00:16&lt;00:00,  2.36it/s]
[89000 | 6186.11] validation loss = 2.44
[89000 | 6186.98] loss=2.34 avg=2.44
...</code></pre>
<pre class="collapse"><code>...
[91999 | 8892.62] loss=2.22 avg=2.42
Saving checkpoint/taotehching/model-92000
Generating samples...
======== SAMPLE 1 ========
 eyes, nothingness. Therefore, he who observes the Dao conceives at his door a room, the ground a mat of grasses, the great thing a hole. He finds no traces of the dwelling of the Sage. He finds in the dwelling of the Sage the Mother, the mother of all. The Sage Mother produces the mother of all. Thus, the Sage governs all things without using arms. He governs them without using the tools of the world. It is he who is the mother of all, and therefore of all things.

To know the Tao is the most important thing in the world. The minute, mysterious details of Tao are what people value. To know the Tao and have a good memory is more important than learning the world. For when the world was simple, The small details were as important as the great details. Nowadays, we are taught to study all things well, but then forget to know what the Tao is. We learn the Tao and love to be mysterious, but when we don't know what the Tao is, we can't be specific. For when we know what the Tao is, it's as if we have a place where all things are and born from. It is the way of Tao.

"The Tao" (truth) is the greatest attribute on the whole world; The minute details of the Tao are important. One who knows Tao enjoys quietude, while one who does not know Tao is restrained. One who is called "taoist" desires the dark side and the subtle, while one who is not called "taoist" wishes to be subtle. They are not the same. Therefore, those who call themselves "Taoists" are not the same as those who are not "Taoists." They who live the way of the Tao do not have weapons, they do not wage war and use the tools of men. In the matter of life, they are different. Thus they are different. That is why they live in the world.

The greater part of humanity cherishes and follows the Tao in common with Tao, even the great and glorious antiquity. This is the reason why people respect the Tao and live in the Tao. Tao is the essence of mankind, or rather the essence of all. It is not the essence of the universe. Only the Son (the Existence) knows the nature of the universe, and the former also what the latter exists. As the Son knows the nature of the universe, the latter also is known. Since that is the reason why the Son has only one nature, and that the latter the other, therefore the Son is distinct from the former in both respect. Therefore, the Son rules the universe and the universe rules the Son. The universe is understood of, by, and for the Son, and he serves the universe with his own hand.

The Tao (the Eternal) is the mother of all Things. Only those who grasp and truly live the Tao can be truly happy. Those who ignore the Tao (the Eternal) turn into chaos. Those who hold fast to belief in false beliefs deprive all beings of happiness. Those who glorify existing things end up in misery. The Tao's path is tranquil; Its ways are pure. It can act with the Tao without deceit. It can behave with the Tao without weapons. It can act with the Tao without instruments. This is what it is like.

To know the Tao is to be the mother of all things, As minute as the minute details, True as being whole. Those who know Tao love not to be ignorant; Those who do not love Tao do not know it; Hence, they cannot be good. Those who keep faith in the Tao get the mystical, Those who no longer believe in the Tao Are called demons. Those who trust the Tao Are bound to bless them, Those who love the Tao are good, Because the Tao is good. The Tao is the mother of all, Which is in all but name.

To know the Tao is to have the mother of all things. Nowadays, the tiny details are more important than the great details. Nowadays, we are surrounded by thoughts and desires. Those who do not live on the Tao have no desires. Those who are good with the Tao see everything in a whole. This is why the Tao is the mother of all things. Because it lives for all things, it does not become a weapon. Because it makes people good with the Tao, they are not evil. Because the Tao is good, it always acts as though it were able to act on its own. Thus it is all-pervading. Everyone lives like this because it always acts according to its own nature.

"To know the Dao is to be the mother of all things. Simple and dark, The depths of Tao resemble life. Who can have true purity without knowing Tao? Those who live in the Dao gain the child, Those who do not live in the Dao suffer in the nurture.

Calculating validation loss...
100%|████████████████████████| 40/40 [00:16&lt;00:00,  2.35it/s]
[92000 | 8928.83] validation loss = 2.44
[92000 | 8929.71] loss=3.07 avg=2.43
...</code></pre>
<p>Hiræth Weltschmerz was able to improve the <span class="smallcaps-auto">TTC</span> training dataset by providing the first 41 chapters with the original newlines/linebreaks, and separate translations separated by a blank line, so I replaced blank lines with <code>&lt;|endoftext|&gt;</code> and trained that for ~24 <span class="smallcaps-auto">GPU</span>-hours to a final loss of ~2.10. The results read much more poetically, I felt.</p>
<p>For this final <span class="smallcaps-auto">TTC</span>-with-linebreaks, I uploaded the model &amp; generated 1000 random samples as usual, but I also generated per-chapter samples. For per-chapter samples, I used <code>csplit</code> to split each file/chapter into separate verses/translations, selected a random 10 of them, and then generated 10 random completions of each one (100 total per chapter):</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="va">i=</span>1
<span class="kw">for</span> <span class="ex">CHAPTER</span> in <span class="kw">`</span><span class="fu">ls</span> taotetotalityrevisedpw/*revised.txt<span class="kw">`;</span> <span class="kw">do</span>
    <span class="ex">csplit</span> --prefix xx --suppress-matched <span class="va">$CHAPTER</span> <span class="st">'/^$/'</span> <span class="st">'{*}'</span>
    <span class="kw">for</span> <span class="ex">X</span> in <span class="kw">`</span><span class="fu">ls</span> xx* <span class="kw">|</span> <span class="ex">shuf</span> <span class="kw">|</span> <span class="fu">head</span> -10<span class="kw">`;</span> <span class="kw">do</span>
        <span class="bu">echo</span> <span class="va">$i</span> <span class="va">$X</span>
         <span class="fu">cat</span> <span class="va">$X</span> <span class="kw">|</span> <span class="fu">tee</span> /dev/stderr <span class="kw">|</span> <span class="fu">nice</span> python src/conditional_samples.py <span class="kw">\</span>
           <span class="ex">--top_p</span> 0.9 --model_name taotehching --nsamples 10 --batch_size 10 <span class="kw">\</span>
           <span class="kw">|</span> <span class="fu">tee</span> /dev/stderr <span class="op">&gt;&gt;</span> ttc-chapter-<span class="va">$i</span>.txt
    <span class="kw">done</span>
    <span class="fu">rm</span> xx*
    <span class="va">i=$(($i</span>+1<span class="va">))</span>
<span class="kw">done</span></code></pre></div>
<p>The idea there is that one can write one’s own <em>Tao of <span class="smallcaps-auto">GPT-2</span></em>, going chapter by chapter: select some of the chapter 1 prompted conditional sample completions to create a new chapter 1, and so on, in a way which would be difficult to do with just random unconditional samples.</p>
<p>Downloads:</p>
<ul>
<li><a href="https://mega.nz/#!m5FWGCgZ!cjvMgViPbBqep_6HqYDb2D3Kl8Tt-RsUnwg7457IfDk">model</a></li>
<li><a href="/docs/ai/poetry/2019-07-21-taotehching-all-1ksamples.txt">1k random samples</a> (4MB text)</li>
<li><a href="/docs/ai/poetry/2019-07-22-gpt2-345m-taotehching-all-ch181.tar.xz">100x81 chapter samples</a> (tarball, 17MB text)</li>
</ul>
</section>
</section>
</section>
<section id="gpt-2-15b" class="level1">
<h1><a href="#gpt-2-15b" title="Link to section: §'GPT-2-1.5b'"><span class="smallcaps-auto">GPT-2-1</span>.5b</a></h1>
<section id="b-training" class="level2 collapse">
<h2 class="collapse"><a href="#b-training" title="Link to section: §'1.5b Training'">1.5b Training</a></h2>
<div class="collapseSummary">
<p>In keeping with its gradual rollout plan, observing no particular misuse in the wild (aside from a few anecdotes about content mills), OpenAI released the final and largest model, <a href="https://openai.com/blog/gpt-2-1-5b-release/" class="docMetadata" data-popup-title="GPT-2: 1.5B Release" data-popup-author="Irene Solaiman, Jack Clark, Miles Brundage" data-popup-date="2019-11-05" data-popup-abstract="<p>As the final model release of GPT-2’s staged release, we’re releasing the largest version (1.5B parameters) of GPT-2 along with code and model weights to facilitate detection of outputs of GPT-2 models. While there have been larger language models released since August, we’ve continued with our original staged release plan in order to provide the community with a test case of a full staged release process. We hope that this test case will be useful to developers of future powerful models, and we’re actively continuing the conversation with the AI community on responsible publication.</p><p><em>Our findings</em>:</p><ol type=&quot;1&quot;><li>Humans find GPT-2 outputs convincing.</li><li>GPT-2 can be fine-tuned for misuse.</li><li>Detection is challenging.</li><li>We’ve seen no strong evidence of misuse so far.</li><li>We need standards for studying bias.</li></ol><p>…<em>Next steps</em>: Our experience with GPT-2 over the past 9 months has given us valuable insight into the challenges and opportunities for creating responsible publication norms in AI. We’re continuing our work on this issue via participation in the Partnership on AI’s “Responsible Publication Norms for Machine Learning” project and discussions with our colleagues in the research community.</p>"><span class="smallcaps-auto">GPT-2-1</span>.5b, in November 2019 along with detection tools</a> (<a href="https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf#openai" class="docMetadata" data-popup-title="Release Strategies and the Social Impacts of Language Models" data-popup-author="Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, Jasmine Wang" data-popup-date="2019-11-05" data-popup-abstract="<p>GPT-2 is a large-scale unsupervised language model that generates coherent paragraphs of text, first announced by OpenAI in February 2019<sup>[63]</sup>. We developed four variants of the model, ranging in size from small (124 million parameters) to large (~1.5 billion parameters). We chose a staged release process, releasing the smallest model in February, but withholding larger models due to concerns about the potential for misuse, such as generating fake news content, impersonating others in email, or automating abusive social media content production<sup>[54]</sup>. We released the 355 million parameter model in May as part of a staged release process. We released our 774 million parameter model in August with a six-month follow up announcement, and we are now releasing our 1.5 billion parameter model.</p><p>While large language models’ flexibility and generative capabilities raise misuse concerns, they also have a range of beneficial uses—they can assist in prose, poetry, and programming; analyze dataset biases; and more. We want to release systems that will have a widely-distributed positive impact on society and have low misuse potential, and have striven to make release decisions informed by analysis,engagement, and empirical evidence.</p><p>Instead of releasing the full 1.5 billion model in February, we adopted a ‘staged release’ process. This delay of nine months allowed time between model releases to conduct risk and benefit analyses as model sizes increased. We also hope our staged release process was helpful in allowing others time to adapt and react: giving researchers a chance to mitigate risk of potential misuse, and giving the general public time to adapt to a world in which it is prudent to mistrust everything they read a little more. In addition to finding minimal evidence of misuse so far, several other factors contributed to our confidence in publishing our 774 million and 1.5 billion parameter models. These include what we learned about the positive social impact of beneficial uses, and what we learned through our partnerships among the AI community and through discussions across fields about establishing norms for responsible publication. This report discusses OpenAI’s work related to staged release of large models, partnership-based research, and broader issues in responsible publication that the AI community will need to address.</p><ul><li><p>Overview</p></li><li><p>Staged Release</p></li><li><p>Partnerships</p></li><li><p>Engagement</p></li><li><p>Social Impacts of Large Language Models</p><ul><li>Beneficial Use Potential</li><li>Misuse: Actor Assessment</li><li>Detecting Synthetic Text</li><li>Bias: Exploratory Research</li></ul></li><li><p>Future Trends in Language Models</p></li><li><p>Recommendations for Publication Norms in AI</p></li><li><p>Conclusion</p></li><li><p>Acknowledgments</p></li><li><p>References</p></li><li><p>Appendices</p><ul><li>Appendix A: Summary of Model Sharing Agreement</li><li>Appendix B: Release Timeline</li><li>Appendix C: Examples of Biases in GPT-2</li><li>Appendix D: Partner Research, Middlebury Institute of International Studies’ Center on Terrorism, Extremism, and Counterterrorism</li><li>Appendix E: Partner Research, Cornell University</li></ul></li></ul>">paper</a>) The model was an easy upgrade for services like Talk To Transformer which simply sample from the original model, since it still fits easily onto commodity <span class="smallcaps-auto">GPU</span>s.</p>
<p>From November–December 2019, <a href="https://twitter.com/theshawwn">Shawn Presser</a> &amp; I worked on finetuning training <span class="smallcaps-auto">GPT-2-1</span>.5b on the combined PG+PF poetry dataset from above. The 1080ti <span class="smallcaps-auto">GPU</span> approach failed, so we switched to Google Colab to use the free <span class="smallcaps-auto">TPU</span>s. Colab worked, but constant failures made it painful to contemplate multi-week training runs, and so we switched to <span class="smallcaps-auto">GCP</span> to use <span class="smallcaps-auto">TPU</span>s directly. Direct <span class="smallcaps-auto">TPU</span> use is much faster, but the errors remained, so we began working on a distributed <span class="smallcaps-auto">TPU</span> approach, to work around individual <span class="smallcaps-auto">TPU</span> errors. Eventually, using Google <span class="smallcaps-auto">TFRC</span> research credits to pay for <span class="smallcaps-auto">TPUS</span>, we began running <a href="https://www.shawwn.com/swarm" title="Swarm Training: We demonstrate a new technique to train ML models using dozens of independent TPUs."><span class="smallcaps-auto">TPU</span> ‘swarms’ of &lt;60 <span class="smallcaps-auto">TPU</span>s</a> (since scaled to &lt;200). These produced meaningful training progress and we reached a loss of ~2 by 12 Dec 2019.</p>
<p>We were able to train it to ~1 loss, but it appeared to have overfit in some fashion as sampled qualities became increasingly worse by the time we halted ~20 Dec 2019, so we settled for iteration #500,522. Download:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">rsync</span> --verbose rsync://78.46.86.149:873/biggan/2019-12-13-gwern-gpt-2-1.5b-poetry-model-500522.tar.xz ./</code></pre></div>
</div>

<section id="gpu-failures" class="level3">
<h3><a href="#gpu-failures" title="Link to section: §'GPU Failures'"><span class="smallcaps-auto">GPU</span> Failures</a></h3>
<p><em>Training</em> is a different story. 345M took ~7 days to train, and 1.5b is 4.4x larger, so that alone implies a training time of a month. Worse, where 345M fits in reasonably in a 1080ti’s 11GB <span class="smallcaps-auto">VRAM</span> &amp; 745M just barely fits, 1.5B does not fit at all.</p>
<p>We began trying to train on the combined PG+PF corpus, like the 117M model I trained for <a href="/GPT-2-preference-learning" class="docMetadata" data-popup-title="GPT-2 Preference Learning for Music and Poetry Generation" data-popup-author="Gwern Branwen" data-popup-date="16 Dec 2019" data-popup-abstract="<p>Standard language generation neural network models, like <span class=&quot;smallcaps-auto&quot;>GPT-2</span>, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.</p><p>A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. <em>Preference learning</em> trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox <span class=&quot;smallcaps-auto&quot;>DRL</span> algorithm like <span class=&quot;smallcaps-auto&quot;>PPO</span>.</p><p>OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on <span class=&quot;smallcaps-auto&quot;>GPT-2</span>. Having previously used <a href=&quot;/GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my &amp;lt;a href=&amp;quot;/RNN-metadata&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;RNN metadata for mimicking individual author style&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;12 Sep 2015&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s are unsupervised generative models which learn to mimic text sequences. I suggest extending char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp;amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. A 2015 experiment using &amp;amp;lt;code&amp;amp;gt;torch-rnn&amp;amp;lt;/code&amp;amp;gt; on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; shows that a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I further try &amp;amp;amp;amp; fail to train &amp;amp;lt;a href=&amp;amp;quot;#geocities-char-rnn&amp;amp;quot;&amp;amp;gt;a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; on Geocities &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;HTML&amp;amp;lt;/span&amp;amp;gt;&amp;amp;lt;/a&amp;amp;gt; for unclear reasons.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;More successfully, &amp;amp;lt;a href=&amp;amp;quot;/GPT-2&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;3 March 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “GPT-2-poetry”, trained on the poems as a continuous stream of text, and “GPT-2-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;English Wikipedia&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;b&amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;lt;i&amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;quot;&amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;lt;/a&amp;amp;amp;gt;’s &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-image-height=&amp;amp;amp;quot;768&amp;amp;amp;quot; data-popup-image-width=&amp;amp;amp;quot;768&amp;amp;amp;quot;&amp;amp;amp;gt;website&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;For generating ABC-formatted folk music, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Music&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;quot;&amp;amp;amp;gt;“GPT-2 Folk Music”&amp;amp;amp;lt;/a&amp;amp;amp;gt;&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;I experiment in 2019 with a recently-developed alternative to char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt;s&amp;amp;lt;/a&amp;amp;gt;, the Transformer NN architecture, by finetuning training OpenAI’s &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-117M&amp;amp;lt;/span&amp;amp;gt; Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp;amp;amp; lines with inline metadata (the source book). The generated poetry is much better.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;2015–2016 text-generation experiments with char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt;s&amp;lt;/a&amp;gt;, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAI’s &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;-poetry”, trained on the poems as a continuous stream of text, and “&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Poetry Foundation&amp;quot; data-popup-author=&amp;quot;English Wikipedia&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;&amp;amp;lt;b&amp;amp;gt;The Poetry Foundation&amp;amp;lt;/b&amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;lt;i&amp;amp;gt;Poetry&amp;amp;lt;/i&amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Wikipedia: Poetry Foundation&amp;quot;&amp;gt;Poetry Foundation&amp;lt;/a&amp;gt;’s &amp;lt;a href=&amp;quot;https://www.poetryfoundation.org/poems&amp;quot;&amp;gt;website&amp;lt;/a&amp;gt;. .&amp;amp;gt; With just a few &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPU&amp;lt;/span&amp;gt;-days on 1080ti &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPU&amp;lt;/span&amp;gt;s, &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; finetuning can produce high-quality poetry which is more thematically consistent than my char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems. For the highest-quality AI poetry to date, see my followup page, &amp;lt;a href=&amp;quot;https://www.gwern.net/GPT-3&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-3 Creative Fiction&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;19 June 2020&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I continue my AI poetry generation experiments with OpenAI’s &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; successor, &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;, which is 116✕ larger, and much more powerful. &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;, however, is not merely a quantitative tweak yielding “&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; but better”—it is qualitatively different, exhibiting eerie runtime learning capabilities allowing even the raw model, with zero finetuning, to tackle almost any imaginable textual task purely by example or instruction. One does not train or program &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; in a normal way, but one engages in dialogue and writes prompts to teach &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; what one wants.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Experimenting through the OpenAI Beta &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;API&amp;amp;lt;/span&amp;amp;gt; in June 2020, I find that &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; does not just match my &amp;amp;lt;a href=&amp;amp;quot;/GPT-2#gpt-2-1.5b&amp;amp;quot;&amp;amp;gt;finetuned &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-1&amp;amp;lt;/span&amp;amp;gt;.5b-poetry&amp;amp;lt;/a&amp;amp;gt; for poem-writing quality, but exceeds it, while being versatile in handling &amp;amp;lt;a href=&amp;amp;quot;#poetry&amp;amp;quot;&amp;amp;gt;poetry&amp;amp;lt;/a&amp;amp;gt;, &amp;amp;lt;a href=&amp;amp;quot;#tom-swifties&amp;amp;quot;&amp;amp;gt;Tom Swifty puns&amp;amp;lt;/a&amp;amp;gt;, science fiction, dialogue like Turing’s &amp;amp;lt;a href=&amp;amp;quot;#turing-dialogue&amp;amp;quot;&amp;amp;gt;Turing-test dialogue&amp;amp;lt;/a&amp;amp;gt;, &amp;amp;lt;a href=&amp;amp;quot;#literary-parodies&amp;amp;quot;&amp;amp;gt;literary style parodies&amp;amp;lt;/a&amp;amp;gt;… As the &amp;amp;lt;em&amp;amp;gt;pièce de résistance&amp;amp;lt;/em&amp;amp;gt;, I recreate Stanislaw Lem’s &amp;amp;lt;em&amp;amp;gt;Cyberiad&amp;amp;lt;/em&amp;amp;gt;’s &amp;amp;lt;a href=&amp;amp;quot;#stanislaw-lems-cyberiad&amp;amp;quot;&amp;amp;gt;“Trurl’s Electronic Bard”&amp;amp;lt;/a&amp;amp;gt; poetry using &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;. (Along the way, I document instances of how the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;BPE&amp;amp;lt;/span&amp;amp;gt; text encoding &amp;amp;lt;a href=&amp;amp;quot;#bpes&amp;amp;quot;&amp;amp;gt;unnecessarily damages&amp;amp;lt;/a&amp;amp;gt; &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;’s performance on a variety of tasks.)&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;’s samples are not just close to human level: they are creative, witty, deep, meta, and often beautiful. They demonstrate an ability to handle abstractions, like style parodies, I have not seen in &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; at all. Chatting with &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; feels uncannily like chatting with a human. I was impressed by the results reported in the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; paper, and after spending a week trying it out, I remain impressed.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;This page records &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; samples I generated in my explorations, and thoughts on &amp;amp;lt;a href=&amp;amp;quot;#prompts-as-programming&amp;amp;quot;&amp;amp;gt;how to use &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt;&amp;amp;lt;/a&amp;amp;gt; and its remaining &amp;amp;lt;a href=&amp;amp;quot;#weaknesses&amp;amp;quot;&amp;amp;gt;weaknesses&amp;amp;lt;/a&amp;amp;gt;. I hope you enjoy them even a tenth as much as I enjoyed testing &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-3&amp;amp;lt;/span&amp;amp;gt; and watching the completions scroll across my screen.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Creative writing by OpenAI&amp;amp;#39;s GPT-3 model, demonstrating poetry, dialogue, puns, literary parodies, and storytelling.&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; Creative Writing”&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;For anime plot summaries, see &amp;lt;a href=&amp;quot;/TWDE#text&amp;quot;&amp;gt;&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TWDNE&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;; for generating &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;-formatted folk music, see &amp;lt;a href=&amp;quot;/GPT-2-music&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Folk Music&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;1 Nov 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;In November 2019, I experimented with training a &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; hypothetically can improve on an &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;RNN&amp;amp;lt;/span&amp;amp;gt; by better global coherence &amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I encountered problems with the standard &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;lt;a href=&amp;amp;quot;#spaceless-model&amp;amp;quot;&amp;amp;gt;fixing that&amp;amp;lt;/a&amp;amp;gt;, I successfully trained it on &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;=205,304 &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp; &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;The &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; folk model &amp;amp;amp;amp; dataset are &amp;amp;lt;a href=&amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;quot;&amp;amp;gt;available for download&amp;amp;lt;/a&amp;amp;gt;, and I provide for listening selected &amp;amp;lt;a href=&amp;amp;quot;#samples&amp;amp;quot;&amp;amp;gt;music samples&amp;amp;lt;/a&amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;We followed the &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; folk model with &amp;amp;lt;a href=&amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;quot;&amp;amp;gt;an &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt;-&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; model&amp;amp;lt;/a&amp;amp;gt;: a &amp;amp;lt;a href=&amp;amp;quot;#midi-dataset&amp;amp;quot;&amp;amp;gt;dataset of 453k &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;ABC&amp;amp;lt;/span&amp;amp;gt; pieces&amp;amp;lt;/a&amp;amp;gt; decompiled from &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; pieces, which fit into &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2-117M&amp;amp;lt;/span&amp;amp;gt; with an expanded context window when trained on &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;TPU&amp;amp;lt;/span&amp;amp;gt;s. The &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;MIDI&amp;amp;lt;/span&amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;lt;a href=&amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;quot;&amp;amp;gt;even better musical samples&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; Folk Music”&amp;lt;/a&amp;gt; &amp;amp;amp; &amp;lt;a href=&amp;quot;/GPT-2-preference-learning&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Preference Learning for Music and Poetry Generation&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;16 Dec 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Standard language generation neural network models, like &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt;, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp;amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. &amp;amp;lt;em&amp;amp;gt;Preference learning&amp;amp;lt;/em&amp;amp;gt; trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;DRL&amp;amp;lt;/span&amp;amp;gt; algorithm like &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;PPO&amp;amp;lt;/span&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt;. Having previously used &amp;amp;lt;a href=&amp;amp;quot;/GPT-2&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;3 March 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In February 2019, following up on my &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/RNN-metadata&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;RNN metadata for mimicking individual author style&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;12 Sep 2015&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Char-RNNs are unsupervised generative models which learn to mimic text sequences. I suggest extending char-RNNs with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp;amp;amp;amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. A 2015 experiment using &amp;amp;amp;amp;lt;code&amp;amp;amp;amp;gt;torch-rnn&amp;amp;amp;amp;lt;/code&amp;amp;amp;amp;gt; on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-RNN shows that a char-RNN can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I further try &amp;amp;amp;amp;amp;amp; fail to train &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#geocities-char-rnn&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;a char-RNN on Geocities HTML&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; for unclear reasons.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;More successfully, &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;gwern&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;3 March 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: &amp;amp;amp;amp;amp;lt;q&amp;amp;amp;amp;amp;gt;“GPT-2-poetry”&amp;amp;amp;amp;amp;lt;/q&amp;amp;amp;amp;amp;gt;, trained on the poems as a continuous stream of text, and &amp;amp;amp;amp;amp;lt;q&amp;amp;amp;amp;amp;gt;“GPT-2-poetry-prefix”&amp;amp;amp;amp;amp;lt;/q&amp;amp;amp;amp;amp;gt;, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;I experiment in 2019 with a recently-developed alternative to char-RNNs&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, the Transformer NN architecture, by finetuning training OpenAI’s GPT-2-117M Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp;amp;amp;amp;amp; lines with inline metadata (the source book). The generated poetry is much better.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot;&amp;amp;amp;gt;2015–2016 text-generation experiments with char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt;s&amp;amp;amp;lt;/a&amp;amp;amp;gt;, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp; text generation. Using OpenAI’s &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;-poetry”, trained on the poems as a continuous stream of text, and “&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;English Wikipedia&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;b&amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;lt;i&amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;quot;&amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;lt;/a&amp;amp;amp;gt;’s &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;quot;&amp;amp;amp;gt;website&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;With just a few &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;-days on 1080ti &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;s, &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; finetuning can produce high-quality poetry which is more thematically consistent than my char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;For anime plot summaries, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/TWDE#text&amp;amp;amp;quot;&amp;amp;amp;gt;&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;TWDNE&amp;amp;amp;lt;/span&amp;amp;amp;gt;&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for generating &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;-formatted folk music, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Folk Music&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;In November 2019, I experimented with training a &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;RNN&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; hypothetically can improve on an &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;RNN&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; by better global coherence &amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I encountered problems with the standard &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#spaceless-model&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;fixing that&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;=205,304 &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp;amp;amp; &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; folk model &amp;amp;amp;amp;amp;amp; dataset are &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;available for download&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, and I provide for listening selected &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#samples&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;music samples&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;We followed the &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; folk model with &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;an &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC-MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; model&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;: a &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#midi-dataset&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;dataset of 453k &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;ABC&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; decompiled from &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces, which fit into &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; with an expanded context window when trained on &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;TPU&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;s. The &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;MIDI&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;even better musical samples&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; Folk Music”&amp;amp;amp;lt;/a&amp;amp;amp;gt; &amp;amp;amp;amp;amp; &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-preference-learning&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;GPT-2 Preference Learning for Music and Poetry Generation&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;16 Dec 2019&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Standard language generation neural network models, like &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp;amp;amp;amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;Preference learning&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;DRL&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; algorithm like &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;PPO&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt;. Having previously used &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Neural Network Poetry&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;3 March 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “GPT-2-poetry”, trained on the poems as a continuous stream of text, and “GPT-2-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;amp;quot;Poetry Foundation&amp;amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;amp;quot;English Wikipedia&amp;amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;b&amp;amp;amp;amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;amp;amp;amp;lt;/b&amp;amp;amp;amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;amp;amp;amp;lt;i&amp;amp;amp;amp;amp;amp;gt;Poetry&amp;amp;amp;amp;amp;amp;lt;/i&amp;amp;amp;amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;quot; title=&amp;amp;amp;amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;Poetry Foundation&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;’s &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-image-height=&amp;amp;amp;amp;amp;quot;768&amp;amp;amp;amp;amp;quot; data-popup-image-width=&amp;amp;amp;amp;amp;quot;768&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;website&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;For generating ABC-formatted folk music, see &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;amp;quot;GPT-2 Music&amp;amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;quot; title=&amp;amp;amp;amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;“GPT-2 Folk Music”&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; for poetry&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp; &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;/GPT-2-music&amp;amp;amp;amp;quot; class=&amp;amp;amp;amp;quot;docMetadata&amp;amp;amp;amp;quot; data-popup-title=&amp;amp;amp;amp;quot;GPT-2 Folk Music&amp;amp;amp;amp;quot; data-popup-author=&amp;amp;amp;amp;quot;Gwern Branwen&amp;amp;amp;amp;quot; data-popup-date=&amp;amp;amp;amp;quot;1 Nov 2019&amp;amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;#spaceless-model&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;fixing that&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;amp;amp;lt;em&amp;amp;amp;amp;amp;gt;n&amp;amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;amp;gt;=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;lt;p&amp;amp;amp;amp;amp;gt;The model &amp;amp;amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected &amp;amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;amp;quot;#samples&amp;amp;amp;amp;amp;quot;&amp;amp;amp;amp;amp;gt;music samples&amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;music generation&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, I experimented with &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;GPT-2&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; preference learning for unconditional music and poetry generation.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;n&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt;≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some &amp;amp;amp;amp;lt;span class=&amp;amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;NLP&amp;amp;amp;amp;lt;/span&amp;amp;amp;amp;gt; tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp;amp;amp;amp;amp; succumb to adversarial examples easily.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp;amp;amp;amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#bradley-terry-preference-learning&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;directly model the implied ranking&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;#optimization-by-backprop-not-blackbox&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;optimizes sequences to maximize the reward model’s output&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; Preference Learning for Music and Poetry Generation”&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for playing chess, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;A Very Unlikely Chess Game&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Scott Alexander&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-01-06&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;within&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://slatestarcodex.com/2019/02/28/meaningful/&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;meaningful&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; title=&amp;amp;amp;quot;Scott Alexander&amp;amp;amp;quot;&amp;amp;amp;gt;“A Very Unlikely Chess Game”&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for the Reddit comment generator, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;disumbrationist&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-01-12&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://openai.com/blog/gpt-2-1-5b-release/&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;finally released the full 1.5 billion parameter model&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://old.reddit.com/u/gwern&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;/u/gwern&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt; (&amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://www.patreon.com/gwern&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;here’s his Patreon&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;) and Shawn Presser (&amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://old.reddit.com/u/shawwwn&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;/u/shawwwn&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required &amp;amp;amp;amp;lt;a href=&amp;amp;amp;amp;quot;https://twitter.com/gwern/status/1215005375407112193&amp;amp;amp;amp;quot;&amp;amp;amp;amp;gt;around $70K worth of TPU credits&amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;gt;, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from &amp;amp;amp;amp;lt;em&amp;amp;amp;amp;gt;all&amp;amp;amp;amp;lt;/em&amp;amp;amp;amp;gt; the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;&amp;amp;amp;quot; rel=&amp;amp;amp;quot;archived alternate nofollow&amp;amp;amp;quot; data-url-original=&amp;amp;amp;quot;https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/&amp;amp;amp;quot; title=&amp;amp;amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )&amp;amp;amp;quot;&amp;amp;amp;gt;SubSimulator&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt;&amp;amp;amp;lt;/a&amp;amp;amp;gt;; for fanfiction, the &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#archive-of-our-own-ao3-gpt-2-1.5b&amp;amp;amp;quot;&amp;amp;amp;gt;Ao3&amp;amp;amp;lt;/a&amp;amp;amp;gt;; and for video games, &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#video-game-walkthrough-gpt-2-1.5b&amp;amp;amp;quot;&amp;amp;amp;gt;the walkthrough model&amp;amp;amp;lt;/a&amp;amp;amp;gt;. For OpenAI’s &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-3&amp;amp;amp;lt;/span&amp;amp;amp;gt; followup, see &amp;amp;amp;lt;a href=&amp;amp;amp;quot;https://arxiv.org/abs/2005.14165#openai&amp;amp;amp;quot; class=&amp;amp;amp;quot;docMetadata&amp;amp;amp;quot; data-popup-title=&amp;amp;amp;quot;Language Models are Few-Shot Learners&amp;amp;amp;quot; data-popup-author=&amp;amp;amp;quot;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al [...]&amp;amp;amp;quot; data-popup-date=&amp;amp;amp;quot;2020-05-28&amp;amp;amp;quot; data-popup-abstract=&amp;amp;amp;quot;Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&amp;amp;amp;amp;#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.&amp;amp;amp;quot; title=&amp;amp;amp;quot;Brown et al 2020&amp;amp;amp;quot;&amp;amp;amp;gt;“&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-3&amp;amp;amp;lt;/span&amp;amp;amp;gt;: Language Models are Few-Shot Learners”&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;&amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; for poetry&amp;amp;lt;/a&amp;amp;gt; &amp;amp;amp;amp; &amp;amp;lt;a href=&amp;amp;quot;/GPT-2-music&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Folk Music&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;1 Nov 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In November 2019, I experimented with training a &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; neural net model to generate folk music in the high-level &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; music text format, following previous work in 2016 which used a char-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; trained on a ‘The Session’ dataset. A &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; hypothetically can improve on an &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;RNN&amp;amp;amp;lt;/span&amp;amp;amp;gt; by better global coherence &amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;I encountered problems with the standard &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; model’s encoding of text which damaged results, but after &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#spaceless-model&amp;amp;amp;quot;&amp;amp;amp;gt;fixing that&amp;amp;amp;lt;/a&amp;amp;amp;gt;, I successfully trained it on &amp;amp;amp;lt;em&amp;amp;amp;gt;n&amp;amp;amp;lt;/em&amp;amp;amp;gt;=205,304 &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; music pieces taken from The Session &amp;amp;amp;amp;amp; &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;The &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; folk model &amp;amp;amp;amp;amp; dataset are &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#combined-model-the-session-abcnotation.com&amp;amp;amp;quot;&amp;amp;amp;gt;available for download&amp;amp;amp;lt;/a&amp;amp;amp;gt;, and I provide for listening selected &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#samples&amp;amp;amp;quot;&amp;amp;amp;gt;music samples&amp;amp;amp;lt;/a&amp;amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;We followed the &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; folk model with &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#generating-midi-with-10k30k-context-windows&amp;amp;amp;quot;&amp;amp;amp;gt;an &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt;-&amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; model&amp;amp;amp;lt;/a&amp;amp;amp;gt;: a &amp;amp;amp;lt;a href=&amp;amp;amp;quot;#midi-dataset&amp;amp;amp;quot;&amp;amp;amp;gt;dataset of 453k &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;ABC&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces&amp;amp;amp;lt;/a&amp;amp;amp;gt; decompiled from &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces, which fit into &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2-117M&amp;amp;amp;lt;/span&amp;amp;amp;gt; with an expanded context window when trained on &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;TPU&amp;amp;amp;lt;/span&amp;amp;amp;gt;s. The &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;MIDI&amp;amp;amp;lt;/span&amp;amp;amp;gt; pieces are far more diverse and challenging, and &amp;amp;amp;lt;span class=&amp;amp;amp;quot;smallcaps-auto&amp;amp;amp;quot;&amp;amp;amp;gt;GPT-2&amp;amp;amp;lt;/span&amp;amp;amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;amp;amp;lt;a href=&amp;amp;amp;quot;/GPT-2-music#midi-samples&amp;amp;amp;quot;&amp;amp;amp;gt;even better musical samples&amp;amp;amp;lt;/a&amp;amp;amp;gt;.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;music generation&amp;amp;lt;/a&amp;amp;gt;, I experimented with &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/span&amp;amp;gt; preference learning for unconditional music and poetry generation.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp;amp;amp; succumb to adversarial examples easily.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp;amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model &amp;amp;lt;a href=&amp;amp;quot;#bradley-terry-preference-learning&amp;amp;quot;&amp;amp;gt;directly model the implied ranking&amp;amp;lt;/a&amp;amp;gt; of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which &amp;amp;lt;a href=&amp;amp;quot;#optimization-by-backprop-not-blackbox&amp;amp;quot;&amp;amp;gt;optimizes sequences to maximize the reward model’s output&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; Preference Learning for Music and Poetry Generation”&amp;lt;/a&amp;gt;; for playing chess, see &amp;lt;a href=&amp;quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;A Very Unlikely Chess Game&amp;quot; data-popup-author=&amp;quot;Scott Alexander&amp;quot; data-popup-date=&amp;quot;2020-01-06&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running &amp;amp;lt;em&amp;amp;gt;within&amp;amp;lt;/em&amp;amp;gt; a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this &amp;amp;lt;a href=&amp;amp;quot;https://slatestarcodex.com/2019/02/28/meaningful/&amp;amp;quot;&amp;amp;gt;meaningful&amp;amp;lt;/a&amp;amp;gt;? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Scott Alexander&amp;quot;&amp;gt;“A Very Unlikely Chess Game”&amp;lt;/a&amp;gt;; for the Reddit comment generator, see &amp;lt;a href=&amp;quot;/docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots&amp;quot; data-popup-author=&amp;quot;disumbrationist&amp;quot; data-popup-date=&amp;quot;2020-01-12&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI &amp;amp;lt;a href=&amp;amp;quot;https://openai.com/blog/gpt-2-1-5b-release/&amp;amp;quot;&amp;amp;gt;finally released the full 1.5 billion parameter model&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by &amp;amp;lt;a href=&amp;amp;quot;https://old.reddit.com/u/gwern&amp;amp;quot;&amp;amp;gt;/u/gwern&amp;amp;lt;/a&amp;amp;gt; (&amp;amp;lt;a href=&amp;amp;quot;https://www.patreon.com/gwern&amp;amp;quot;&amp;amp;gt;here’s his Patreon&amp;amp;lt;/a&amp;amp;gt;) and Shawn Presser (&amp;amp;lt;a href=&amp;amp;quot;https://old.reddit.com/u/shawwwn&amp;amp;quot;&amp;amp;gt;/u/shawwwn&amp;amp;lt;/a&amp;amp;gt;), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required &amp;amp;lt;a href=&amp;amp;quot;https://twitter.com/gwern/status/1215005375407112193&amp;amp;quot;&amp;amp;gt;around $70K worth of TPU credits&amp;amp;lt;/a&amp;amp;gt;, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from &amp;amp;lt;em&amp;amp;gt;all&amp;amp;lt;/em&amp;amp;gt; the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen&amp;amp;lt;/p&amp;amp;gt;&amp;quot; rel=&amp;quot;archived alternate nofollow&amp;quot; data-url-original=&amp;quot;https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/&amp;quot; title=&amp;quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )&amp;quot;&amp;gt;SubSimulator&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;; for fanfiction, the &amp;lt;a href=&amp;quot;#archive-of-our-own-ao3-gpt-2-1.5b&amp;quot;&amp;gt;Ao3&amp;lt;/a&amp;gt;; and for video games, &amp;lt;a href=&amp;quot;#video-game-walkthrough-gpt-2-1.5b&amp;quot;&amp;gt;the walkthrough model&amp;lt;/a&amp;gt;. For OpenAI’s &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt; followup, see &amp;lt;a href=&amp;quot;https://arxiv.org/abs/2005.14165#openai&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Language Models are Few-Shot Learners&amp;quot; data-popup-author=&amp;quot;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al [...]&amp;quot; data-popup-date=&amp;quot;2020-05-28&amp;quot; data-popup-abstract=&amp;quot;Recent work has demonstrated substantial gains on many &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 achieves strong performance on many &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;NLP&amp;amp;lt;/span&amp;amp;gt; datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3&amp;amp;#39;s few-shot learning still struggles, as well as some datasets where &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 faces methodological issues related to training on large web corpora. Finally, we find that &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of &amp;amp;lt;span class=&amp;amp;quot;smallcaps-auto&amp;amp;quot;&amp;amp;gt;GPT&amp;amp;lt;/span&amp;amp;gt;-3 in general.&amp;quot; title=&amp;quot;Brown et al 2020&amp;quot;&amp;gt;“&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-3&amp;lt;/span&amp;gt;: Language Models are Few-Shot Learners”&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&quot;><span class=&quot;smallcaps-auto&quot;>GPT-2</span> for poetry</a> &amp;amp; <a href=&quot;/GPT-2-music&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Folk Music&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;1 Nov 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In November 2019, I experimented with training a &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; neural net model to generate folk music in the high-level &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; music text format, following previous work in 2016 which used a char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; trained on a ‘The Session’ dataset. A &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; hypothetically can improve on an &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; by better global coherence &amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I encountered problems with the standard &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; model’s encoding of text which damaged results, but after &amp;lt;a href=&amp;quot;#spaceless-model&amp;quot;&amp;gt;fixing that&amp;lt;/a&amp;gt;, I successfully trained it on &amp;lt;em&amp;gt;n&amp;lt;/em&amp;gt;=205,304 &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; music pieces taken from The Session &amp;amp;amp; &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; folk model &amp;amp;amp; dataset are &amp;lt;a href=&amp;quot;#combined-model-the-session-abcnotation.com&amp;quot;&amp;gt;available for download&amp;lt;/a&amp;gt;, and I provide for listening selected &amp;lt;a href=&amp;quot;#samples&amp;quot;&amp;gt;music samples&amp;lt;/a&amp;gt; as well as medleys of random samples from throughout training.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;We followed the &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; folk model with &amp;lt;a href=&amp;quot;#generating-midi-with-10k30k-context-windows&amp;quot;&amp;gt;an &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; model&amp;lt;/a&amp;gt;: a &amp;lt;a href=&amp;quot;#midi-dataset&amp;quot;&amp;gt;dataset of 453k &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; pieces&amp;lt;/a&amp;gt; decompiled from &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; pieces, which fit into &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; with an expanded context window when trained on &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TPU&amp;lt;/span&amp;gt;s. The &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; pieces are far more diverse and challenging, and &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;lt;a href=&amp;quot;/GPT-2-music#midi-samples&amp;quot;&amp;gt;even better musical samples&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&quot;>music generation</a>, I experimented with <span class=&quot;smallcaps-auto&quot;>GPT-2</span> preference learning for unconditional music and poetry generation.</p><p>I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at <em>n</em>≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using <em>n</em>≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).</p><p>This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp; succumb to adversarial examples easily.</p><p>Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model <a href=&quot;/GPT-2-preference-learning#bradley-terry-preference-learning&quot;>directly model the implied ranking</a> of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which <a href=&quot;/GPT-2-preference-learning#optimization-by-backprop-not-blackbox&quot;>optimizes sequences to maximize the reward model’s output</a>.</p>">RL preference learning</a>, but turning on all the options in the nshepperd repo doesn’t fix the memory problems. (FeepingCreature was able to train on his new <span class="smallcaps-auto">AMD</span> <span class="smallcaps-auto">GPU</span>, which has 16GB <span class="smallcaps-auto">RAM</span>, so a few more gigabytes would’ve done the trick..)</p>
<p>Using <a href="https://github.com/shawwn/gpt-2">Shawn Presser’s fork of nshepperd’s fork</a>, we experimented with alternatives like using reduced-precision, truncating parameters to FP16. This caused serious errors.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> After fixing those errors, and reducing the context window by half (potentially hamstringing it), we <em>could</em> train <span class="smallcaps-auto">GPT-2-1</span>.5b on a 1080ti, but our naive conversion to FP16 appears to have seriously damaged the model and it emitted only garbage. We then tried using a different floating-point format, <a href="https://en.wikipedia.org/wiki/bfloat16_floating-point_format" class="docMetadata" data-popup-title="Bfloat16 floating-point format" data-popup-author="English Wikipedia" data-popup-abstract="<p>The <b>bfloat16 floating-point format</b> is a computer number format occupying 16 bits in computer memory; it represents a wide dynamic range of numeric values by using a floating radix point. This format is a truncated (16-bit) version of the 32-bit <span class=&quot;smallcaps-auto&quot;>IEEE</span> 754 single-precision floating-point format (binary32) with the intent of accelerating machine learning and near-sensor computing. It preserves the approximate dynamic range of 32-bit floating-point numbers by retaining 8 exponent bits, but supports only an 8-bit precision rather than the 24-bit significand of the binary32 format. More so than single-precision 32-bit floating-point numbers, bfloat16 numbers are unsuitable for integer calculations, but this is not their intended use.</p>" title="Wikipedia: bfloat16 floating-point format">bfloat16</a>, which in theory is much better suited to NN models than FP16 &amp; natively supported on <span class="smallcaps-auto">TPU</span>s, but it trained extremely slowly on my Nvidia 1080ti <span class="smallcaps-auto">GPU</span>. Given the daunting expected training time, bfloat16 was not a solution.</p>
<p>The only solution here seemed to be to abandon my 1080tis and upgrade to <span class="smallcaps-auto">TPU</span>s. <span class="smallcaps-auto">TPU</span>s may not be any faster, but they have far more <span class="smallcaps-auto">RAM</span> and can train a <span class="smallcaps-auto">GPT-2-1</span>.5b with no problem.</p>
</section>
<section id="google-colab" class="level3">
<h3><a href="#google-colab" title="Link to section: §'Google Colab'">Google Colab</a></h3>
<p>How to get a <span class="smallcaps-auto">TPU</span>? Fortunately, Google Colab <em>did</em> just enable free <span class="smallcaps-auto">TPU</span>s by default… So Presser enhanced his fork to support <span class="smallcaps-auto">TPU</span>s, and we started training.</p>
<p>Unfortunately, Colab notebooks are still limited in <em>system</em> <span class="smallcaps-auto">RAM</span> and disk space, so training 1.5B then encountered the surprising problem of running out of <span class="smallcaps-auto">RAM</span> &amp; crashing, running out of disk space, and saving to disk being extremely slow due to slow TensorFlow serialization of the model checkpoint. (The <span class="smallcaps-auto">TPU</span>-based serialization code would have been far faster using the standard TF way, but it would also required the user to create &amp; manage a Google Cloud bucket; we were still hoping to create an easy works-out-of-the-box Colab notebook to let anyone do 1.5B-finetuning. If there was a faster way to do it, Presser didn’t know about it.) This was partially solved by saving few checkpoints, figuring out how to attach a Google Drive folder<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> (after paying $2/month for an upgrade to ~100GB of additional space, since the default 15GB Google Drive is perilously small), and further work on optimizing the serializing. Training was slow—1 minute per minibatch, initially—but did work. An example:</p>
<pre><code>...2019-11-12T18:18:02.411370-08:00 [601 | 13372.9633 | 67.76 | 0.059033/s] loss=3.5624 avg=3.5139 rate=0.000020 step=185

Thee, thou fount of every flower,
And, in thy fair, golden-red ring,
A golden-colored rose,
That was once a bloom I knew.
Now, if the sun ever wets the tree,
The leaves shall turn once more to gold,
Thine, as the rose will be to you,
Like a sunbeam in a rose-color'd sky.
Thus we praise thee as a sweet flower,
Not only as a sunbeam,
But, in thine earthly bloom,
As the sunbeam when brightest.
Thus we praise thee for ever.

From thy golden, heavenly rose,
Which is the sun,
From thine ever-changing gold,
Wherever it shines,
As a heavenly beam
That never fades.
Thee, oh,
Thee, golden sun,
As ever-flowing,</code></pre>
<p>Presser tried out curriculum learning/progressive growing by setting the context window to a small window like <em>k</em>=50 <span class="smallcaps-auto">BPE</span> tokens, with the idea that it could be gradually annealed to the original <em>k</em>=1024 over the course of training. (Because of how Transformers scale, <em>k</em>=50 uses far less memory &amp; compute than <em>k</em>=1024, so it fits much larger faster minibatches.) This seemed to be working to some degree, but it was no silver bullet.</p>
<p>Exacerbating the problem, <span class="smallcaps-auto">TPU</span>s on Colab appear to randomly ‘freeze’, an issue unrelated to Colab notebooks timing out after a day or so; manually interrupting the training process and restarting fixes it, but at the cost of any progress made since the last (slow) checkpoint &amp; required constant babysitting; I calculated that one would have to checkpoint every hour to optimize the tradeoff between freezes &amp; checkpoints! At one point I was dealing with <span class="smallcaps-auto">TPU</span> freezes every half hour. It was already giving decent poetry samples despite a loss &gt;3, but we wanted to train to convergence, which ought to be &lt;1.6 (the final combined-117M loss).</p>
<p>This was not going to work for weeks of training. Presser again modified the codebase &amp; notebook to add <a href="https://en.wikipedia.org/wiki/Watchdog_timer" class="docMetadata" data-popup-title="Watchdog timer" data-popup-author="English Wikipedia" data-popup-abstract="<p>A <b>watchdog timer</b> is an electronic timer that is used to detect and recover from computer malfunctions. During normal operation, the computer regularly resets the watchdog timer to prevent it from elapsing, or &quot;timing out&quot;. If, due to a hardware fault or program error, the computer fails to reset the watchdog, the timer will elapse and generate a timeout signal. The timeout signal is used to initiate corrective action or actions. The corrective actions typically include placing the computer system in a safe state and restoring normal system operation.</p>" title="Wikipedia: Watchdog timer">‘watchdog’</a> processes which would watch for an apparently hung TensorFlow process due to a <span class="smallcaps-auto">TPU</span> freeze, and kill it and restart. But the lost time was a serious issue: we couldn’t checkpoint too often because then we’d waste all our time checkpointing, but <em>not</em> checkpointing meant we’d lose minutes or hours of training. We couldn’t find any information about why <span class="smallcaps-auto">TPU</span>s would freeze and figured it was some sort of Colab issue, so I decided to bite the bullet and pay for a <span class="smallcaps-auto">GCP</span> VM &amp; <span class="smallcaps-auto">TPU</span>.</p>
<p><a href="/docs/www/cloud.google.com/a3600850bf5175e458c9523bfb312b4aad2f28ec.html" rel="archived alternate nofollow" data-url-original="https://cloud.google.com/tpu/pricing" title="(Original URL: https://cloud.google.com/tpu/pricing )">A preemptible <span class="smallcaps-auto">TPU</span>v2</a> costs $1.35/hour, which is not <em>too</em> bad… A week of training would cost &gt;$226 &amp; I’d never used <span class="smallcaps-auto">GCP</span> before, but I was too curious what a fully-trained 1.5B would generate. The net cost for November 2019, due to all the experiments and costs not covered by the <span class="smallcaps-auto">TFRC</span> research credits, was $321.47, primarily for high-<span class="smallcaps-auto">RAM</span> instances, and then network egress bandwidth fees—for cross-zone traffic with the <span class="smallcaps-auto">TPU</span>s, apparently. Optimizing for <span class="smallcaps-auto">GPT-2-1</span>.5b-poetry, we got December 2019’s cost down to $199.78, and trained <span class="smallcaps-auto">GPT-2-1</span>.5b-poetry and an <span class="smallcaps-auto">IRC</span> logs model. We spent in January 2020 an additional $408.22 on a number of projects: the <a href="https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/" class="docMetadata" data-popup-title="A Very Unlikely Chess Game" data-popup-author="Scott Alexander" data-popup-date="2020-01-06" data-popup-abstract="<p>Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running <em>within</em> a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.</p><p>…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this <a href=&quot;https://slatestarcodex.com/2019/02/28/meaningful/&quot;>meaningful</a>? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.</p><p>[See also the much later <a href=&quot;https://arxiv.org/abs/2007.03500&quot; &quot;The Go Transformer: Natural Language Modeling for Game Play&quot;>Noever et al 2020a</a>/<a href=&quot;https://arxiv.org/abs/2008.04057&quot; title=&quot;The Chess Transformer: Mastering Play using Generative Language Models&quot;>Noever et al 2020b</a> who do the exact same thing in applying GPT-2 to Go SGF/chess PGN games.]</p>">chess</a>, <a href="/docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html" class="docMetadata" data-popup-title="Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots" data-popup-author="disumbrationist" data-popup-date="2020-01-12" data-popup-abstract="<p>When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI <a href=&quot;https://openai.com/blog/gpt-2-1-5b-release/&quot;>finally released the full 1.5 billion parameter model</a>.</p><p>The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by <a href=&quot;https://old.reddit.com/u/gwern&quot;>/u/gwern</a> (<a href=&quot;https://www.patreon.com/gwern&quot;>here’s his Patreon</a>) and Shawn Presser (<a href=&quot;https://old.reddit.com/u/shawwwn&quot;>/u/shawwwn</a>), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required around $70K worth of TPU credits, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.</p><p>Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.</p><p>One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from <em>all</em> the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.</p><p>…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen</p>" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/" title="Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )">Subreddit Simulator</a>, Archive Of Our Own, and video game walkthrough <span class="smallcaps-auto">GPT-2-1</span>.5 models; the 30k context window <span class="smallcaps-auto">GPT-2-117M</span> <a href="/GPT-2-music#generating-midi-with-30k-context-windows"><span class="smallcaps-auto">ABC</span>/<span class="smallcaps-auto">MIDI</span> model</a>; ImageNet resnet benchmarking; and Style<span class="smallcaps-auto">GAN</span> 2 prototyping for training on <a href="/Danbooru2019" class="docMetadata" data-popup-title="Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset" data-popup-author="Gwern Branwen" data-popup-date="15 Dec 2015" data-popup-abstract="<p>Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS <span class=&quot;smallcaps-auto&quot;>COCO</span> fall short either on image or tag count or restricted distribution. I suggest that the “image -boorus” be used. The image boorus are longstanding web databases which host large numbers of images which can be ‘tagged’ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.</p><p>The best known booru, with a focus on quality, is <a href=&quot;https://danbooru.donmai.us/&quot;>Danbooru</a>. We provide a torrent/rsync mirror which contains ~3tb of 3.69m images with 108m tag instances (of 392k defined tags, ~29/image) covering Danbooru from 24 May 2005 through 31 December 2019 (final ID: #3,734,659), providing the image files &amp;amp; a <span class=&quot;smallcaps-auto&quot;>JSON</span> export of the metadata. We also provide a smaller torrent of <span class=&quot;smallcaps-auto&quot;>SFW</span> images downscaled to 512✕512px <span class=&quot;smallcaps-auto&quot;>JPG</span>s (295GB; 2,828,400 images) for convenience.</p><p>Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.</p>">Danbooru2019</a>.</p>
</section>
<section id="gcp" class="level3">
<h3><a href="#gcp" title="Link to section: §'GCP'"><span class="smallcaps-auto">GCP</span></a></h3>
<p>After setting up on <span class="smallcaps-auto">GCP</span> and figuring out the details like needing to set an environment variable with the target <span class="smallcaps-auto">TPU</span> name, we discovered… the <span class="smallcaps-auto">TPU</span>s kept freezing anyway. This was on top of the standard preempting of <span class="smallcaps-auto">TPU</span>s, since we were using preemptibles to save money as is standard in cloud deep learning.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> This remained a mystery. Was there some undocumented <a href="https://en.wikipedia.org/wiki/Heartbeat_%28computing%29" class="docMetadata" data-popup-title="Heartbeat (computing)" data-popup-author="English Wikipedia" data-popup-abstract="<p>In computer science, a <b>heartbeat</b> is a periodic signal generated by hardware or software to indicate normal operation or to synchronize other parts of a computer system. Usually a heartbeat is sent between machines at a regular interval in the order of seconds. If the endpoint does not receive a heartbeat for a time—usually a few heartbeat intervals—the machine that should have sent the heartbeat is assumed to have failed.</p>" title="Wikipedia: Heartbeat (computing)">heartbeat</a> that was required? Was Presser’s TF code, which avoided the standard <code>TPUEstimator</code> approach which it seems everyone else uses, triggering some sort of problem? (We were warned in vague terms that <span class="smallcaps-auto">TPU</span>s do not like loops or reshaping operations.) Even more irritatingly, our on-demand <span class="smallcaps-auto">TPU</span>s turned out to preempt anyway! But at least the checkpoints were fast, so now the watchdogs worked better. But on the gripping hand, the <span class="smallcaps-auto">TPU</span> was not fast and was performing far below what we thought it should based on its nominal specs, and we seemed to be using barely a third of the cores.</p>
<figure>
<img alt="" height="813" loading="lazy" sizes="(max-width: 30em) 80vw, 1400px" src="/images/2019-11-19-gpt2-15b-poetry-tensorboard-1tputraining.jpg" srcset="/images/2019-11-19-gpt2-15b-poetry-tensorboard-1tputraining.jpg-768px.jpg 768w, /images/2019-11-19-gpt2-15b-poetry-tensorboard-1tputraining.jpg 1400w" width="1400"><figcaption><span class="smallcaps-auto">GPT-2</span> model training curves over 4 days on 1 <span class="smallcaps-auto">TPU</span> each: 1.5b (orange), 774M (light blue), 400M (red), ‘tiny’ reduced context (dark blue)</figcaption>
</figure>
<p>Presser decided to press on and after further optimizing work to ensure we used the full <span class="smallcaps-auto">TPU</span> <span class="smallcaps-auto">RAM</span> and more of the cores<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> with a minibatch <em>n</em>=4, began experimenting with support for <em>multiple</em> <span class="smallcaps-auto">TPU</span>s. Since each <span class="smallcaps-auto">TPU</span> is a separate computer inside Google’s network, and not ‘attached’ to a VM like a <span class="smallcaps-auto">GPU</span>, there was in theory little limit to how many <span class="smallcaps-auto">TPU</span>s our 1 VM could orchestrate. We could, in theory, create an equivalent of the expensive <span class="smallcaps-auto">TPU</span> ‘pods’ by simply connecting to a bunch of <span class="smallcaps-auto">TPU</span>s at once.</p>
<p>The main limit for distributed <span class="smallcaps-auto">TPU</span> training is the network bandwidth: copying around the latest version of the multi-gigabyte model to and from the central VM uses up all the bandwidth available. In the simple synchronous case, which most closely approximates training on a single <span class="smallcaps-auto">GPU</span>, if the entire cluster has to stop and wait for every node to copy its updates to the master, the master do a single batch update, and the master sync back out to each node, the cluster will spend most of its time just waiting on the network to copy everything. (And what happens when one or more <span class="smallcaps-auto">TPU</span>s inevitably freeze?)</p>
<p>Presser worked around the bandwidth with an asynchronous approach somewhat like the old <a href="https://arxiv.org/abs/1106.5730" class="docMetadata" data-popup-title="HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent" data-popup-author="Feng Niu, Benjamin Recht, Christopher Re, Stephen J. Wright" data-popup-date="2019-12-11" data-popup-abstract="Stochastic Gradient Descent (<span class=&quot;smallcaps-auto&quot;>SGD</span>) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize <span class=&quot;smallcaps-auto&quot;>SGD</span>, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that <span class=&quot;smallcaps-auto&quot;>SGD</span> can be implemented without any locking. We present an update scheme called <span class=&quot;smallcaps-auto&quot;>HOGWILD</span>! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then <span class=&quot;smallcaps-auto&quot;>HOGWILD</span>! achieves a nearly optimal rate of convergence. We demonstrate experimentally that <span class=&quot;smallcaps-auto&quot;>HOGWILD</span>! outperforms alternative schemes that use locking by an order of magnitude." title="'HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent', Niu et al 2011">Hogwild</a> training method: instead of every node copying its entire model at a fixed timestep and waiting for all the other nodes, the nodes are constantly communicating a fraction of their latest model with the master and receiving an updated fraction back, regardless of how many iterations other nodes have run. So every node is running a hybrid &amp; partially-out-of-date model and sending stale gradients out, but gradient descent is robust enough that this will still work and will scale up easily. After enough slices have been sent, a node will have sent an equivalent of a full model, and caught up partially, and the ‘swarm’ will hopefully be able to make progress by training on a large amount of hardware and be faster than using just a few <span class="smallcaps-auto">TPU</span>s synchronously.</p>
<p>A swarm was too expensive for me, so we applied for <a href="/docs/www/www.tensorflow.org/684cf36ec555db6eb7c4e9b27c045041d8e1acb1.html" class="docMetadata" data-popup-title="TensorFlow Research Cloud (TFRC): Accelerate your cutting-edge machine learning research with free Cloud TPUs" data-popup-author="TFRC (Google)" data-popup-abstract="<p>The TensorFlow Research Cloud (TFRC) program enables researchers to apply for access to a cluster of more than 1,000 Cloud TPUs. In total, this cluster delivers a total of more than 180 petaflops of raw compute power! Researchers accepted into the TFRC program can use these Cloud TPUs at no charge to accelerate the next wave of open research breakthroughs. Participants in the TFRC program will be expected to share their TFRC-supported research with the world through peer-reviewed publications, open source code, blog posts, or other means. They should also be willing to share detailed feedback with Google to help us improve the TFRC program and the underlying Cloud TPU platform over time. In addition, participants accept Google's Terms and Conditions, acknowledge that their information will be used in accordance with our Privacy Policy, and agree to conduct their research in accordance with the Google AI principles. Machine learning researchers around the world have done amazing things with the limited computational resources they currently have available. We'd like to empower researchers from many different backgrounds to think even bigger and tackle exciting new challenges that would be inaccessible otherwise.</p> <p>[TFRC is an easy-to-apply cloud credit program which grants free access to up to hundreds of GCP TPUs and sometimes whole TPU pods to researchers &amp; hobbyists like me; I relied on TFRC credits to train a variety of GPT-2-1.5b models which are infeasible on consumer GPUs. It took seconds to apply, they replied in hours with credits, and were highly responsive thereafter as we encountered various TPU issues.]</p>" rel="archived alternate nofollow" data-url-original="https://www.tensorflow.org/tfrc" title="(Original URL: https://www.tensorflow.org/tfrc )">TensorFlow Research Cloud credits</a>. I wasn’t expecting anything to come of it, but the form was easily to fill out in a minute (it’s not much more than an email address), and to my surprise, within 3 hours we had been approved for 1 month of credits, covering several on-demand <span class="smallcaps-auto">TPU</span>v2–3s, and 100 preemptible <span class="smallcaps-auto">TPU</span>v2s (but no <span class="smallcaps-auto">TPU</span> pods).</p>
<p>Presser began the long and painful process of debugging the swarm and all its problems… The halts were never quite fixed but we kept scaling.</p>
<p>By 11 December 2019, after applying for additional credits because we were coming up on the <span class="smallcaps-auto">TFRC</span> deadline on the 14<sup>th</sup>, we’d gotten the loss down to ~2.15. After switching to Adam and scaling the swarm further to ~95 <span class="smallcaps-auto">TPU</span>s on the 13<sup>th</sup>, we reached a loss of 1.61, matching or beating the 117M record on the combined dataset. A further 5 days (interrupted by swarm preemption and occasional tweaks/experiments) brought the loss down to &lt;0.6 on 18 December 2019. I had expected flagrant plagiarism/overfitting well before a loss of 0.6, perhaps ~1.2, but regularly inspecting unconditional samples and searching initial lines or generated titles/authors, I found little &amp; they didn’t read like plagiarism, so we kept training to see how far it could go. (Prompting with lines from famous poems would’ve almost surely elicited plagiarism, but I am less concerned with that, since <span class="smallcaps-auto">GPT-2-1</span>.5b is so big it can easily memorize famous poems without compromising its general poetry abilities.) I suspect that 1.5b is not <em>really</em> &gt;3x better than 117M, and that 117M could have been trained to &lt;1.6 loss if we had used similar amounts of compute, so the actual benefit from scaling up <span class="smallcaps-auto">GPT-2</span> is smaller—but why bother with training 117M to a better convergence when we can use 1.5b?</p>
<p>Example training curves:</p>
<figure>
<img alt="" height="1035" loading="lazy" sizes="(max-width: 30em) 80vw, 1400px" src="/images/2019-12-13-gpt2-15b-poetry-tensorboard-97tputraining.png" srcset="/images/2019-12-13-gpt2-15b-poetry-tensorboard-97tputraining.png-768px.png 768w, /images/2019-12-13-gpt2-15b-poetry-tensorboard-97tputraining.png 1400w" width="1400"><figcaption>Training curve of a swarm of ~97 <span class="smallcaps-auto">TPU</span>s training <span class="smallcaps-auto">GPT-2-1</span>.5b-poetry for ~21 hours (13 December 2019) from a loss of ~2.15 to &lt;1.6.</figcaption>
</figure>
<figure>
<img alt="" height="503" loading="lazy" sizes="(max-width: 30em) 80vw, 1400px" src="/images/2019-12-16-gpt2-15b-poetry-tensorboard-100tputraining.png" srcset="/images/2019-12-16-gpt2-15b-poetry-tensorboard-100tputraining.png-768px.png 768w, /images/2019-12-16-gpt2-15b-poetry-tensorboard-100tputraining.png 1400w" width="1400"><figcaption>100 <span class="smallcaps-auto">TPU</span>s, 1.6→1 loss (16 December 2019)</figcaption>
</figure>
<p>While sampling, we noticed double and single quotes were being replaced by <a href="https://en.wikipedia.org/wiki/mojibake" class="docMetadata" data-popup-title="Mojibake" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Mojibake</b> is the garbled text that is the result of text being decoded using an unintended character encoding. The result is a systematic replacement of symbols with completely unrelated ones, often from a different writing system.</p>" title="Wikipedia: mojibake">mojibake</a> gibberish. This appeared to be due to Unicode curly quotes (<code>""'</code>) in the original text dataset. The <a href="/docs/www/s3-us-west-2.amazonaws.com/d73fdc5ffa8627bce44dcda2fc012da638ffb158.pdf#page=5" class="docMetadata" data-popup-title="GPT-1: Improving Language Understanding by Generative Pre-Training" data-popup-author="Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever" data-popup-date="2018-06-08" data-popup-abstract="Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by <em>generative pre-training</em> of a language model on a diverse corpus of unlabeled text, followed by <em>discriminative fine-tuning</em> on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI)." rel="archived alternate nofollow" data-url-original="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf#page=5" title="(Original URL: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf#page=5 )"><span class="smallcaps-auto">GPT-1</span> paper</a> mentions using the <a href="/docs/www/ftfy.readthedocs.io/af4b9e930fa258d7a35233bf8a05001e0a34664b.html" class="docMetadata" data-popup-title="ftfy: fixes text for you" data-popup-author="ftfy" data-popup-abstract="<p><code>ftfy</code> fixes Unicode that’s broken in various ways.</p><p>The goal of ftfy is to <strong>take in bad Unicode and output good Unicode</strong>, for use in your Unicode-aware code. This is different from taking in non-Unicode and outputting Unicode, which is not a goal of ftfy. It also isn’t designed to protect you from having to write Unicode-aware code. ftfy helps those who help themselves.</p><p>Of course you’re better off if your input is decoded properly and has no glitches. But you often don’t have any control over your input; it’s someone else’s mistake, but it’s your problem now.</p><p>ftfy will do everything it can to fix the problem.</p>" rel="archived alternate nofollow" data-url-original="https://ftfy.readthedocs.io/en/latest/" title="(Original URL: https://ftfy.readthedocs.io/en/latest/ )"><code>ftfy</code></a> Python library to clean up mojibake &amp; Unicode in their crawl data, and <code>ftfy</code> converts Unicode quotes to the <span class="smallcaps-auto">ASCII</span> straight quotes, so presumably <span class="smallcaps-auto">GPT-2</span> does as well and it (or its <span class="smallcaps-auto">BPE</span> encoding) is confused by their presence, causing the mojibake output. It was late in training, but we updated the PG+PF dataset to replace the quotes (<code>ftfy.fix_text('foo')</code> etc).</p>
</section>
<section id="b-hyperparameters" class="level3">
<h3><a href="#b-hyperparameters" title="Link to section: §'1.5b Hyperparameters'">1.5b Hyperparameters</a></h3>
<p>One issue worth noting was the problem of regularly restarting the swarm due to preemption &amp; <span class="smallcaps-auto">TPU</span>s expiring, which caused large loss spikes on startup that would waste hours of training as it recovered; as a compromise between simple <span class="smallcaps-auto">SGD</span> and full Adam, we were using <a href="https://arxiv.org/abs/1804.04235" class="docMetadata" data-popup-title="Adafactor: Adaptive Learning Rates with Sublinear Memory Cost" data-popup-author="Noam Shazeer, Mitchell Stern" data-popup-date="2019-12-11" data-popup-abstract="In several recently proposed stochastic optimization methods (e.g. <span class=&quot;smallcaps-auto&quot;>RMSP</span>rop, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the <span class=&quot;smallcaps-auto&quot;>WMT</span> 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves." title="'Adafactor: Adaptive Learning Rates with Sublinear Memory Cost', Shazeer &amp; Stern 2018">Adafactor</a> (as most Transformer projects do, like Connor or Gokaslan’s <span class="smallcaps-auto">GPT-2</span> replications), and we speculate that the loss spike is related to losing optimizer state and bad initial variance estimates. Simple <span class="smallcaps-auto">SGD</span> avoided the loss spike, but at the cost of making no discernible progress regardless of LR; Adafactor made slow progress, but wasted a substantial fraction of available training time; we tried to avoid Adam because the memory overhead of tracking momentum for all variables (as opposed to Adafactor’s simplified approximation of momentum) would reduce minibatch size, but when we tried Adam on the full swarm, despite the initial loss spike, it made much more rapid progress than Adafactor did.</p>
<p>I suspect that the issue here is that though simple <span class="smallcaps-auto">SGD</span> &amp; Adafactor worked fine when running on a single <span class="smallcaps-auto">GPU</span>, in the scaled-up asynchronous swarm setting, they have especially poor gradient estimates and make slow progress; the loss spike comes from the optimizer state being reset on startup, causing early gradients to be poorly estimated &amp; destabilizing training, requiring thousands of iterations to gradually recover. Adam then improves over Adafactor by estimating true variance/momentum, overcoming gradient noise to make faster progress. If so, the spike issue could be fixed several ways:</p>
<ol type="1">
<li><span class="smallcaps">Don’t Reset The Optimizer</span>: the simplest way to fix the spike caused by resetting momentum estimates is to not reset them; save the optimizer state along with the model, and restore on startup. This is somewhat unusual for TensorFlow projects (I see more PyTorch implementations serializing optimizer state) but shouldn’t be hard, and only comes at the cost of using more diskspace for checkpoints &amp; more bandwidth at startup, which would be more than worthwhile to save hours of recovery time.</li>
<li><span class="smallcaps">Learning Rate Warmup</span>: if the initial updates are highly destructive because they use bad momentum updates and it takes a substantial number of iterations to re-estimate the correct momentum updates, then don’t update much initially; use small LRs to avoid destabilizing the swarm while still re-learning the momentum. After some iterations, the updates should be safe to use again and the LR can be increased to the normal LR.</li>
<li><span class="smallcaps">Gradient Accumulation</span>: another way to reduce the damage of initial updates is to greatly improve their accuracy, and keep the swarm more in sync, by doing fewer but better updates; instead of each node doing local updates immediately after each tiny local minibatch, the nodes store gradients and average across many minibatches before doing an actual update, thereby faking having large updates. This will also improve the momentum estimates, as the momentum is estimated across many minibatches before it affects any update.</li>
</ol>
</section>
</section>
<section id="b-samples" class="level2">
<h2><a href="#b-samples" title="Link to section: §'1.5b Samples'">1.5b Samples</a></h2>
<div class="epigraph">
<blockquote>
<p>“Who alive can say,<br>‘Thou art no Poet—may’st not tell thy dreams?’”</p>
<p><a href="https://en.wikipedia.org/wiki/John_Keats" class="docMetadata" data-popup-title="John Keats" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>John Keats</b> was an English Romantic poet. He was one of the main figures of the second generation of Romantic poets, along with Lord Byron and Percy Bysshe Shelley, despite his works having been in publication for only four years before his death from tuberculosis at the age of 25.</p>" title="Wikipedia: John Keats">John Keats</a>, <em><a href="https://en.wikipedia.org/wiki/The_Fall_of_Hyperion%3A_A_Dream" class="docMetadata" data-popup-title="The Fall of Hyperion: A Dream" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>The Fall of Hyperion: A Dream</b></i>, sometimes subtitled as <i>A Vision</i> instead of a dream, is an epic poem written by the English Romantic John Keats. Keats composed <i>The Fall of Hyperion</i> by reworking, expanding, and personally narrating lines from his earlier fragmented epic poem <span><i>Hyperion</i></span>.</p>" title="Wikipedia: The Fall of Hyperion: A Dream">The Fall of Hyperion: A Dream</a></em> I</p>
</blockquote>
</div>
<div class="epigraph">
<blockquote>
<p>“More than iron, more than lead, more than gold I need electricity.<br>I need it more than I need lamb or pork or lettuce or cucumber.<br>I need it for my dreams.”</p>
<p><a href="/docs/www/www.ubu.com/b673b9a3774686cb133244f985f2a70f08b2e7ff.html" rel="archived alternate nofollow" data-url-original="http://www.ubu.com/concept/racter.html" title="(Original URL: http://www.ubu.com/concept/racter.html )"><em>The Policeman’s Beard is Half-Constructed</em></a>, <a href="https://en.wikipedia.org/wiki/RACTER" class="docMetadata" data-popup-title="Racter" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>Racter</b></i> is an artificial intelligence computer program that generates English language prose at random.</p>" title="Wikipedia: RACTER"><span class="smallcaps-auto">RACTER</span></a> &amp; William Chamberlain 1983</p>
</blockquote>
</div>
<section id="loss-2-6" class="level3">
<h3><a href="#loss-2-6" title="Link to section: §'Loss: 2.6'">Loss: 2.6</a></h3>
<p>Partway through, having reached a loss of ~2.6 (down ~0.5 from the Colab model), we experimented with training our model on a P100 <span class="smallcaps-auto">GPU</span>, halving the context window to make it fit, to informally compare its training speed with the swarm. The P100 made little training progress, but it did generate some fun poetry samples (we had disabled the training sample generation for the swarm because generating samples is so slow).</p>
<p>The samples strike me as good, perhaps even better than 117M, despite the loss being much worse (2.6 rather than 1.6). Why might that be?</p>
<p>I hypothesize it reflects a weakness of the likelihood loss in terms of perceptual quality: humans are more sensitive to long-range correlations and text degenerating into gibberish than we are to local details like exact use of particles or to slightly better modeling of spelling (which is why <a href="https://en.wikipedia.org/wiki/stylometrics" class="docMetadata" data-popup-title="Stylometry" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Stylometry</b> is the application of the study of linguistic style, usually to written language, but it has successfully been applied to music and to fine-art paintings as well. Another conceptualization defines it as the linguistic discipline that uses statistical analysis to literature by evaluating the author's style through various quantitative criteria.</p>" title="Wikipedia: stylometrics">stylometrics</a> works). The original OA <span class="smallcaps-auto">GPT-2-1</span>.5b achieves much better modeling of long-range correlations and producing coherent text than the 117M did, of course. What happens when they are both trained on a poetry dataset? It is the tale of the tortoise &amp; the hare, or the <a href="https://en.wikipedia.org/wiki/bias%E2%80%93variance_tradeoff" class="docMetadata" data-popup-title="Bias–variance tradeoff" data-popup-author="English Wikipedia" data-popup-abstract="<p>In statistics and machine learning, the <b>bias–variance tradeoff</b> is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. The <b>bias–variance dilemma</b> or <b>problem</b> is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:</p><ul><li>The <span><i>bias error</i></span> is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</li>
<li>The <i>variance</i> is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).</li></ul>" title="Wikipedia: bias–variance tradeoff">bias–variance tradeoff</a>: the 117M is weak, bad at long-range modeling because of its small parameter count &amp; shallow layers, but the benefit is that it can learn quickly about local details like spelling, and, achieving good prediction there, converge to that 1.6 loss; 1.5B starts off good at long-range modeling and good at short-range modeling, and must tradeoff learning both from its limited training, thereby achieving mediocre performance on local correlations and thus mediocre loss, even though humans reading it are impressed by the thematic consistency and relative lack of ‘gibberish’ (locally but not globally consistent text).</p>
<p>An additional issue here is that the <span class="smallcaps-auto">GPT-2</span> models are not fully trained: as <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf#page=4" title="'Language Models are Unsupervised Multitask Learners', Radford et al 2019">the <span class="smallcaps-auto">GPT-2</span> paper notes</a>, “All models still underfit WebText and held-out perplexity has as of yet improved given more training time.” (The difficult of training such powerful LMs to convergence was also <a href="https://nv-adlr.github.io/MegatronLM" class="docMetadata" data-popup-title="MegatronLM: Training Billion+ Parameter Language Models Using GPU Model Parallelism" data-popup-author="<span class=&quot;smallcaps-auto&quot;>NVIDIA</span> <span class=&quot;smallcaps-auto&quot;>ADLR</span>" data-popup-date="2019-08-13" data-popup-abstract="Larger language models are dramatically more useful for <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks such as article completion, question answering, and dialog systems. Training the largest neural language model has recently been the best way to advance the state of the art in <span class=&quot;smallcaps-auto&quot;>NLP</span> applications. Two recent papers, <a href=&quot;https://arxiv.org/abs/1810.04805&quot;><span class=&quot;smallcaps-auto&quot;>BERT</span></a> and <a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;><span class=&quot;smallcaps-auto&quot;>GPT</span>-2</a>, demonstrate the benefits of large scale language modeling. Both papers leverage advances in compute and available text corpora to significantly surpass state of the art performance in natural language understanding, modeling, and generation. Training these models requires hundreds of exaflops of compute and <a href=&quot;https://arxiv.org/abs/1604.06174&quot;>clever memory management</a> to trade recomputation for a reduced memory footprint. However, for very large models beyond a billion parameters, the memory on a single <span class=&quot;smallcaps-auto&quot;>GPU</span> is not enough to fit the model along with the parameters needed for training, requiring model parallelism to split the parameters across multiple <span class=&quot;smallcaps-auto&quot;>GPU</span>s. Several approaches to model parallelism exist, but they are difficult to use, either because they rely on custom compilers, or because they scale poorly or require changes to the optimizer.</p></div><div><p> In this work, we implement a simple and efficient model parallel approach by making only a few targeted modifications to existing <a href=&quot;https://openreview.net/pdf?id=BJJsrmfCZ&quot;>PyTorch</a> transformer implementations. <a href=&quot;https://github.com/nvidia/megatron-lm&quot;>Our code</a> is written in native Python, leverages mixed precision training, and utilizes the <a href=&quot;https://developer.nvidia.com/nccl&quot;><span class=&quot;smallcaps-auto&quot;>NCCL</span> library</a> for communication between <span class=&quot;smallcaps-auto&quot;>GPU</span>s. We showcase this approach by training an 8.3 billion parameter transformer language model with 8-way model parallelism and 64-way data parallelism on 512 <span class=&quot;smallcaps-auto&quot;>GPU</span>s, making it the <b>largest transformer based language model ever trained at 24× the size of <span class=&quot;smallcaps-auto&quot;>BERT</span> and 5.6× the size of <span class=&quot;smallcaps-auto&quot;>GPT</span>-2</b>. We have published the code that implements this approach at <a href=&quot;https://github.com/NVIDIA/Megatron-LM&quot;>our GitHub repository</a>.</p></div><div><p> Our experiments are conducted on <span class=&quot;smallcaps-auto&quot;>NVIDIA</span>’s <a href=&quot;https://devblogs.nvidia.com/dgx-superpod-world-record-supercomputing-enterprise&quot;><span class=&quot;smallcaps-auto&quot;>DGX</span> Super<span class=&quot;smallcaps-auto&quot;>POD</span></a>. Without model parallelism, we can fit a baseline model of 1.2B parameters on a single V100 32GB <span class=&quot;smallcaps-auto&quot;>GPU</span>, and sustain 39 Tera<span class=&quot;smallcaps-auto&quot;>FLOPS</span> during the overall training process, which is 30% of the theoretical peak <span class=&quot;smallcaps-auto&quot;>FLOPS</span> for a single <span class=&quot;smallcaps-auto&quot;>GPU</span> in a <span class=&quot;smallcaps-auto&quot;>DGX</span>2-H server. Scaling the model to 8.3 billion parameters on 512 <span class=&quot;smallcaps-auto&quot;>GPU</span>s with 8-way model parallelism, we achieved up to <b>15.1 Peta<span class=&quot;smallcaps-auto&quot;>FLOPS</span> sustained performance</b> over the entire application and reached <b>76% scaling efficiency</b> compared to the single <span class=&quot;smallcaps-auto&quot;>GPU</span> case." title="MegatronLM: Training Billion+ Parameter Language Models Using GPU Model Parallelism">noted by the MegatronLM researchers</a>, whose MegatronLM-8.3b model was still learning rapidly when they ended the training—despite use of <span class="smallcaps-auto">NVIDIA</span>’s <span class="smallcaps-auto">DGX</span> Super<span class="smallcaps-auto">POD</span> with 512 <span class="smallcaps-auto">GPU</span>s.) So some of the finetuning here may also be finishing the <span class="smallcaps-auto">GPT-2</span> training.</p>
<p>I selected the following from its training log.</p>
<p>A commentary on <a href="https://en.wikipedia.org/wiki/Oda_Nobunaga" class="docMetadata" data-popup-title="Oda Nobunaga" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Oda Nobunaga</b><span style=&quot;font-weight:normal&quot;> </span> was a Japanese <i>daimyō</i> in the late 16th century who attempted to unify Japan during the late Sengoku period, and successfully gained control over most of Honshu through conquest. Nobunaga is regarded as one of three unifiers of Japan along with his retainers Toyotomi Hideyoshi and Tokugawa Ieyasu. During his later life, Nobunaga was widely known for most brutal suppression of determined opponents, eliminating those who by principle refused to cooperate or yield to his demands. His reign was noted for innovative military tactics, fostering free trade, and encouraging the start of the Momoyama historical art period. He was killed when his retainer Akechi Mitsuhide rebelled against him at Honnō-ji.</p>" title="Wikipedia: Oda Nobunaga">Oda Nobunaga</a>’s mysterious betrayal by a loyal general?</p>
<pre><code>"Oda on the Pavement", by Mark Bibbins [Activities, Jobs &amp; Working, Social Commentaries]
The first time I saw this sign someone's blood
streaming down the pavement, I didn't know what to do.
I walked as quickly as I could toward the body
and almost didn't see the second because it was going by
so quickly I had to blink and it was gone.
But this is what I saw when I got there, a corpse
laid out flat on its back, its legs
bent at the knee, with a blue tattered blanket
crammed in the small of its back.
I said to my wife sitting next to me, don't you
think this is awful? She said, Mark, it looks like
just a trick he's done.
I said, it looks awful real to me. The man has
his back turned to us, like he is not even thinking.
As I am, I said to my wife, is this what you have
been waiting for? Is this what you have been waiting for?
The man was lying there not even looking up at us.</code></pre>
<p>In the Golden Age, when the people of the Yellow Valley were instructed by the sages of antiquity:</p>
<pre><code>...That the earth would then not open
(O, but to hear our fathers speak!),
The angel of the waters spake.
For in that time the race of man
Sought the groves, each his own dwelling,
Where were then fruits to satisfy,
Where were trees in plenty springing
To furnish them with fuel,
And so the forest grew,
And o'er it all was running
The river that we see.

Then did those ancient sages
Order their days as was fitting;
And whatsoe'er they looked
On was ordered rightly; all
Taught each what was fit for living
And working for self;
And now we have come to the end,
But this is what their teaching
Said for our instruction,--
That we know and do not know,--
And yet we know and do not know!

When we have found that which is right
The will to act and not to act
To be the chief and not the end,
The day and not the hour,
The joy and not the pain,
When shall we find this, then at last?
Why then at last at last?
There can be no yesterday
If such be the age of wisdom;

There can be no time or place
If such be life and work for man;
There can be no man or man's child
In which he has no friend or friend's child;
Nor yet a man or man's son,
Who to his work does not go.
Thus the sage's order is
And what the sage does--so he does.</code></pre>
<p>Romance?</p>
<pre class="collapse"><code>...I know my father taught me,
To count your blessings and forgive
The people who are wrong;
But I think, when my day is done
And life is all in strife,
Perhaps the one good man who isn't yet
Will find the way to see
Why men die, the reason lies
In the people, never the least in them.

To-morrow it may have been a woman's hand
Caught his in the doorway,
And I may laugh to-night at a stranger's sigh,
Just as I did yesterday.
But what if he and I are dead?
What matters it?  For God's sake, listen to me!
If God finds out
That I am not as I should be,
Then I shall surely pay the price--
That is if He will be just.

I think my father loved me,
And was glad I was his son.
I say to-night I see it every day.
I say to-night I know it all.
And why should I fear
What my son thinks, what he does?
I should be mad to doubt it,
For aught my father says.

I shall find out when I grow old,
And say to my soul, "Be glad
That things that are may be,
For if things were worse, why, we'd have things worse."
And, knowing what my soul knows,
I shall be glad for the things I shall miss--
For God's sake, I shall be glad!

A man's true worth is revealed to you
When, by the light of your own soul's light,
And God's, you look at his;
You can feel them both shine through you
Like a glory you could shatter
Just as a crown could shatter you
Into thousand pieces that were bright
And perfect, and eternal.

And so no other can you see and know
Except his soul.  As the rose of him
Sends back in glory back its hue,
So does the soul of one reveal
The beauty of another.
Be mine the glory to know it best
When my soul shall look at its own.

O great and blessed Lord,
Who made the earth, the heaven, and the sea.
I who am standing in these holy places,
Be grateful</code></pre>
<p>A nice descriptive piece:</p>
<pre><code>...The sun sets and rises in its glory
And sets in its glory only.
And the stars are all that can be seen up there,
That set and rise and come to the earth in their glory.
And the moon, too, sets and rises only,
And comes to the earth in its glory.

The sun sets and rises in its glory,
And when it sets, the night is black,
And when it sets, the day is dark.
The sky is all up against us and over us,
With its starry light,
And all the earth beneath us and above us,
Under the shining sky,
Is black with all the stars.</code></pre>
<p>An elegy:</p>
<pre><code>..."To the Youth", by Susan Greenfield [Death, Growing Old, Sorrow &amp; Grieving, Relationships, Family &amp; Ancestors]
For the youth, who, long ago,
Came up the long and winding way
Beneath my father's roof, in sorrow---
Sorrow that I would not bless
With his very tears. Oh,
My son the sorrowing,
Sorrow's child. God keep thy head,
Where it is dim with age,
Gentle in her death!

And though the years should bear him low
Into the world's dark embrace,
He shall feel more fondly there
Than he did in my bosom then.
Even now that heart, so wild,
Is as a child's, sweet, and calm;
And shall His voice forbid me
From the shadow of the grave
To forget, to forget Him?
And shall He care to keep Him
As I do him here in this place?
Or has the power, that I have learned
Him of the world to fade,
Him of the world to go away?
Haltest to the shadow of the grave,
My son, my son!</code></pre>
<p>Love lost:</p>
<pre><code>...Since that fatal day we parted,
I have lost, I think, my best part.
It was a friend of mine once gave me breath,
And I went as far out as he dared to go,
And ever since, poor lad, I've lived by drink.
But if I had no more friends, I had friends
Such as the world could never let me have.
'Tis true, I have never had occasion to think,
But when the sun is high, and the sky is green,
And the whole world is merry till the break of day,
I sit and dream of her and of the joys of yesterday.
And I go wandering off on some old-world morn
Whose very first light of hope and youth
Is the light of looking on her face through the green.

Her eyes where they were, as I tell you,
Seem fixed with a bright, fixed gaze;
They're half my heaven, when they are there!
They are as clear and bright as she herself,
And she knows, too, the way that she will look
When I can tell her what she is like
And where all will end for me in her.

Now, how would the world like her if it knew,
And what would 'twere if each one did know;
For, ah, she is a world of beauty!
But then, ah, her eyes are even as clear
As the light that makes them bright and wise.

And they are a light out of some dream
That has power to brighten and cheer;
They're a bright sign to turn and turn again
Even in the days of storm and pain;
They're my world, like her bright suns of heaven,
Which I have loved even to the death!</code></pre>
<p>An attempt at nonsense verse, apparently:</p>
<pre><code>...I had a little hen,
I had a little hen,
It chirped at her dish.
She chirped at her dish,
"Come in, come in,"
The dish clattered out,
The dish opened wide,
There were three small cats,

"What is the meaning of this,
Why are all my dishes
Spread open for you?"
"Sit in the corner."
"Here is a bowl for you!"

She drank the bowl clear,
She drank the bowl all up,
And cried with an unhappy sound,
"I do not want it!
I am quite sufficient,
I shall have enough,
And just be done with it,
And not have more!"

But I have a different tale,
Which perhaps you will think is odd.
It is, 'tis no use repeating.
But here, it may be the same.
I had a little hen,
I had a little hen,
She had a little curl,
That with her beak she curled,
In her plumage she had,
Such as a dog's must have.
Thus I had three little dogs.
I fed them with bread.
And they all looked out upon the street
Where our little Polly went by.</code></pre>
<p>The wreck of a ship:</p>
<pre><code>...And as the sun sways to and fro
With his eternal sway of air
They roll in slumberful rest.

So still and calm are they,
So still and clear,
No whisper of the wind dares steal
The tranquil atmosphere of night,
And like bright angels sleep.
The sea is their only theme,
The ship is a dream
That they arise to leave at even.

The moon is far,
All glistening with a silver shine.
It is her pale that lights
The livid face of the sea
That in its white and sullen face
Is pale with pain.

O grey and grey it grows,
And the sea grows wild
As the black clouds in a night-time ride,
The storm is on the sea,
The wild winds rush in their strong wrath--
The great waves roar and beat
The heart of the ship with their fury--
In its heart's core, cold and dead
At one bound, is bound
Like a rock in the heart of a lake.

The storm is loud
And wild it is blowing--
The storm is far away--
The wild winds, wilder still,
Are beating far and wide.
The wild sea winds that were blowing
Have stopped forever there alone
Where the ship has gone down,
And now the moon rides low
Where the sea now rests in calm;
On the deep, dark midnight
The ship is lost.</code></pre>
<p><span class="smallcaps-auto">GPT-2-1</span>.5b can apparently do meta-fiction and break the fourth wall‽</p>
<pre><code>...He had the look that I can't forget;
It was as if at once I knew
What I'd never had before;
It was like a rush of light on water--
And something about it seemed queer.

And the light would come and go
As if the clouds were about to fall,
And the rain came and came in showers
And went in ditto fashion;
And it's just as if I were blindfold--
I couldn't hear a thing.

I'm sure it isn't my nerves;
It's just as if I had the palsy.
I don't want to think of not being there
For something I can't be sure I was.
I'm sure my nerves are fine in my life;
And the only thing bothering me
Is that there isn't any rain.

It's all quite natural as I said;
I've seen men die of natural causes.
Now it's only a pity it happened
As it is done to me. But, as I say,
There's no harm in dying of dropsy.
Though not the death I'd choose to die in--
It's much better than to get laid flat.

This is not the way for my poem
To end, I know--I'll make it up to you
In a later place; but here's a verse
As you may understand. After this line
There's a space, and after that a line
About two inches, in which I meant
To tell you that you had done well to try
To stay alive. But, as I said, this is
Just a matter of chance, it cannot
Be helped; I must allow that this verse
Was written with the fingers you saw--and then
You were all to blame, if I died before
You came in and saw the writing. Now you know
I'm dying--go on--you will not die
Before I finish off my work.

It was a little moon that watched
In this place, I don't know why I chose
To watch it, it had its night when I woke;
And I, who was tired of the earth and men,
Watched it like a man watches a clock at noon,
Just because I chose to and did as it said...</code></pre>
<p>Another shipwreck:</p>
<pre class="collapse"><code>...So the sun sank and the sky grew dark--
There was silence in the world.

The man was a-waiting. He had come
To watch the sea and the moon that rose.
The sky grew dark.  He saw the storm-clouds pour--
The skies grew dark, with dark, dusky glooms--
Down upon the world like a flood of spears,
That struck the men from their feet in their flight.

A wind sprang and lifted the wind-signs higher
In the west wind's battle-hurling fury.
The sky grew dark.  He saw the wind-swept leaves
Fall from the trees into the sea, and sink
Darkly down beneath the seething waters dark
Like a storm's descent.  He heard the falling rain
Come thundering down upon the earth's dappled hills
Like an avalanche of boulders from the mountains.

He saw the wild white-winding ships that fled
Into the storm--hurling great leaden shadows
Into the maelstrom and away
Into the night that was growing black
With leaden shadows that swept the ocean
Like a sea-monster that had lost her breath.

He saw the wild white-winding ships that fled
Into the wind's windless vengeance.  He heard
Hurried voices--"She is gone!"
Hurried voices--"They are off!"
A sudden flash of flame that split away--
The great white ships were no more;
They were caught in the wind's wild wings; the wind
Was stilled in the windless hush of a rest
Tangled like one green tangled mass of peace.  He heard
And knew they were dead.  He would not hear
The winds that told a tale of sea and wind.

The wind had blown him across the world wide,
And into the heart of the night wind's wrath
He saw the faces of all dead men
Hid in the hush of wind and sea,
And knew the faces and watched him well.
So the man who was weary with a quest,
Who turned from the world's endless sorrow
To seek the light that was lost as the sea's
Flood in a wind-silent hush.</code></pre>
<p>A surprisingly coherent piece on a trapped upper-class wife:</p>
<pre><code>...There is not a single house in all
These beautiful gardens that I do not know.
I know the houses and gardens where I sit
In the evening with my husband and my son,
And I sit at the dinner table there too.
The house where my husband and my son live is the one furthest away.

The people come and go through these gardens, all day long;
And I see their feet pass along the paths,
And I hear the talk they have all that day, from one end of the town

to the other; I see the carts and waggons of the farmers,
The teams and horses of the tradesman, men on foot, and the gaiters swinging

Upon their saddles by the way side;
And every day, at morning, the same number of carts and waggons I see,
And every morning, in the great daytime as soon as I wake,
I see their number still greater, still greater.

Then to one side they go
Among the flower beds and in the wood,
And I never see them more;
And their voices float on the rising wind
Like the voices of the dead,
And their faces light upon their breasts, like lights,

Like the faces of bright children,
Like the faces of handsome men in the street,
And the faces of friends, and the faces of lovers,
And of all strangers, all faces of home-brethren
With its memories and its griefs,
And my own face that is always changing,
Wrought by itself in the dark,
With the face of the dead and the face of the living,
And the face of a youth that grows old
In the shadow of love and the light,
In the shadow of the grave.

I know not what they mean by their words,
Those people that pass me in the garden,
In the little town that is in the garden;
I only know that, on many afternoons,
Through a gap in the trees and between the stones
I see their faces and hear their voices.</code></pre>
<p>The curse of immortality:</p>
<pre class="collapse"><code>...And my God, as is right,
Lets no earthly voice
Thy gracious soul acclaim.

There's an old man in a desert land,
No town nor city to be seen,
The skies are o'er him and the hills
His only companions in the air:
The only sound is his own dashing feet,
The earth beneath him, unseen, is brown:
His only food has he no need to eat,
His only company is the birds above,
The sands are white beneath his restless feet:
His dwelling is an osier cage,
A circle of dry sand and bark,
And there the old man builds his dwelling-place,
The only house-holder is the wind.

But I have seen the ancient sands bare,
I have seen the grey old desert sun
Go down upon a world of old
And wither and perish in the noon;
I also have seen the grey old sun
Return with life and day in its arms,
And night come too and bring the light:
But never before has any bird
Looked on the old man living in his dreary house,
His house that is builded by the wind;
The birds that do not love to roost
Have neither wings nor nests of them,
And never any bird has come
To visit the lonely old man lying down;
The sun never sets in the old man's eyes,
And therefore he has neither home nor friends.

O, God, that I had not been born to live
A thousand years, a million years,
And see another sun rise and set,
And other stars in the other sky,
And other life and other death;
But had my soul been like some child
That, when it thinks upon its joy,
And its grief, and its play, and its tears,
And the happy years that it shall have,
And the years that it shall forget,
The thoughts it hath of these things,
And the thoughts that are yet to be,
Might be a child that had no thought,
And have nor speech nor feeling,
That it might feel it was a child.</code></pre>
<p>Perhaps the most striking of them all is this existential horror piece:</p>
<pre><code>..."The World of the Dead", by Peter Stearns [The Body, Nature, Philosophy]
When they come, they carry
Your limbs, your life,
In their mouth and arm.
I think they swallow.
I know it. The others know.
My body will be like theirs,
As the river, the sea,
Will be like the one on which it runs,
If the ocean rises
And swallows the land.
It will be hard to survive.
To heal,
Some of this will have to come off.
That's what they say. They say it
Many times a day. They say it
To each other.
They mean to save us.
They just can't stop us
From becoming what we are.
I must live inside you.
That's what they say. They say it.</code></pre>
</section>
<section id="loss-1-6" class="level3">
<h3><a href="#loss-1-6" title="Link to section: §'Loss: 1.6'">Loss: 1.6</a></h3>
<p>The expanded <span class="smallcaps-auto">TPU</span> swarm &amp; Adam LR tuning allowed rapid training, and we reached 1.6 overnight, matching our previous best on the combined PG+PF poetry dataset.</p>
<p>I generated <a href="/docs/ai/poetry/2019-12-13-gpt21.5b-poetry-samples-topp090.txt">a dump of samples with top-<em>p</em>=0.90</a>, and read through ~5% to select some interesting samples:</p>
<p>Samples selected from reading ~5% of that:</p>
<pre class="collapse"><code>All-sweet and tall, and frail of limb,
Lay there in raiment new begun;
Her moveless-rest were never seen,
She ever so much as bent her knee.
Thither came I, Pilot of thy boat,
And, turning, saw this silent Girl
Who, like to pray, with lifted face
Besought the mist-ringed air to sing
Her Vespers far away off,

And by her hair and veiled head
Her wistful eye she caused to stare.
It seemed to twinkle between the branches high,
And on her shoulder lean by piece and piece
Of glimmering spangles that lightly floated down.
I saw, or fancied I did,
Her lovely head recline
Upon her humble robe's narrow shaggy hood,
That, like the light of day,
Was moon-fringe dark and dim;
Her pale mouth, that evermore
Spread smiles in damp and drizzle;
Her gleaming teeth, whose polished white
Seemed mouldering honey of the midnight blep
Of the dry, dusty pass!
And in one hand, all rippled with
A silken flute of gold,
She played a hushing pipe,
Dora's toy, to play or sing.

Deep through the wintry sky there sped
Through golden vapours as of shape
A dawn that never had a dawn,
A sudden dawn, with breath
Of mist and with a smile to kill.
'Look!' the wind whispered, 'here's
Our Lady of the Skies, from her bright throne,
Like to the smiling of a summer sea
To-night in the lost wind's dark retreat,
Hailed with the deep, seething, dour, wild
Midnight: who have wept for her
The heaving of the waiting years,
Who have wept for her
In wild harangues of the foggy fen
And hollow monotone of the fen.
She shines and smiles to see the tears
Of all the rain-stricken towns and ships
And all the rainy days and nights
On all the hard, the ragged places
That wind had beaten hard, and night
Nigh ready to close, to close, to close
Against the brain of all the face
Of all the over-ill-gotten men,
She shines and laughs to feel the cold
Of all the tears of all the brave men killed
And mad as they.</code></pre>
<pre><code>"A Knowledge of the Dead", by Mary Wiencki [Living, Death, Life Choices, The Mind, Time &amp; Brevity, Religion, The Spiritual, Social Commentaries, Crime &amp; Punishment, Popular Culture]
I see you there, Stu, striding half a mile down the road, arms raised up over your head, head bent slightly. I imagine you hold both those in your inmost heart, and that you must learn, along with anything else, how to turn off a brain that has somehow learned to hold whatever memory is stored in it. For the mind, like any organ, is where the trouble is; an organ can fail with its stored knowledge, or if the memory be great, so great that it will bring the brain to its knees. And then the knee is a joint only partly conscious; if the heart should stop pumping, we are thrown off balance as if it had been only the legs that moved you. So I ask you, were you looking at your watch when you left for that solitary walk? Or waiting for the medicine you wanted to take with you before starting on your way? A look of mild impatience conveys a point as surely as humor, though somewhat dead. It is painful, this wait, I am sure. You have worked long and hard for your knowledge of time and of this place. And now you have it. And time, and all the woe it took to give that power. You have so much of this world left to discover, paths to retrace. You find your way into a park, its benches occupied and visible and free of talk of the dayâ€™s events, at its center a girl...</code></pre>
<pre class="collapse"><code>"The Sphinx and the Social Commentaries", by W. D. Snodawa [Nature, Seas,  Rivers,  &amp; Streams, Social Commentaries, History &amp; Politics]
We were rising over the hill
of which the tip is the sphinx.
There were palms in the palms.
We were rising over the hill
like the tip of a sphinx,
circling the palm that was there
growing straight like the spine of a sphinx
and a crimson palm leaf grew over the palm
as if flowering over a sphinx
and my knot was a knot.
It was night.
It was tinder in our guts
to see them like this.
Climbing alone it was
like the tip of a sphinx
to see them like this,
growing even higher than a sphinx,
a knot, to see them
not quite touching like
the tip of a sphinx
over a palm that was there
to see over the night like tinder.
Suddenly the knot caught in my throat,
my hands stopped and spun.
Thatâ€™s the way a Sphinx talks.
The palm became a mask
and that scared me.
I had to have been looking for
the mask underneath it.
My knot was on my neck.
My knot that was spinning
like a rope.
I was staring down at my own knot
and what it was pulling at.
I let go of the thought of knot
and opened my eyes and saw the moon.
I was standing in my own moon
and I knew I wasnâ€™t going to see it
so I let go of the knot and looked at it
and it was disappearing.</code></pre>
<pre><code>"April Moon", by E. E. Brown [Love, Break-ups &amp; Unthankfulness, Religion, Buddhism, Faith &amp; Doubt]
Awakeâ€"with you I meditated and thus
renewed my doubts; But, awakeâ€"with you I sin,
and thus my conscience put me to bed.
Awakeâ€"with you I suffer, and thus
my doubt took wings. Awakeâ€"with you I play
the hypocrite, And thus my conscience fires
my lash, and thus I scorn you.
Awakeâ€"with you I fly from faith, and thus
through your face I stab myself. Awakeâ€"with you I
remain benighted, and thus
my conscience rots me at my heart.</code></pre>
<pre class="collapse"><code>What man would draw a sword,
If he'd had no forethought,
That so he might prevent
The danger; but with blade
What e'er man can know?

How many lives at least,
Have been lost, and how much blood
On all our limbs been shed!
And yet--so Providence be credited--
There's an end still of life's dismay,
And 't would be glad indeed to lie
Why does every one such pass
As this, without any which he do not pluck,
But with arms for life's defence clad?
Alike of you all the brave
Rage of the lance,
The guerdon of some crown,
Whose shield was never pledged in fight.
The watery lion's with us yet,
By eunuch tightened,
And springing on his prey, not fierce to yield
Though thrice thy foe hath been in peril to see:
Yet, though our quarrels past,
Life may be fresh in them,
This of fighting, and this of feeding.
Beset with peril, beaten to the fence,
And each to prate with prattling foppish grace:
Their song, 'Huzzate!' fuddles young,
You may hear their ends in Oxford-street,
Or in their inn-bred domes
When they climb like larks their wings again.

But we, we live on' other plan;
The Shepherd did but teach us,
We, the delight of life and take delight.
Then why not drink of wines,
Give of bowls to move your bodies,
And, with those things that men to beguile

As they that do light love-songs
Wear like a tree, so do these solacie
Our sabbath-rites,
And send them to heaven,
Whose hand,
Saved as ours, with charity
Should treat as a child againe.

That we do not work on earth for hire,
Why we do doome as we list.
If man did wight battle,
God should not such things read,
As he, of some sinful men
To make him cheat,
And carry, and gluttony to all.

What to your Peers, or how you view
Our Acts and us, let not your selves,
But let the Stars, that watch the skies
The Barns that shelter you,
Which your great Cannibal went the way,
Pull down,
And let one Concord solace every State.</code></pre>
<pre class="collapse"><code>Then look there yet, in that part
Where you will see an abyss profound,
Rays leaping out of darkness,
Snatched with strangest beams at the visage hewed.

When the minaret of the masque is lit
And the caryatid gleams bright
Of four stars that shudder and wane
In chance-to-be to the light that is
In the letter of the crown,
Take her, the zodiac, for
'Tis her sign, 'tis the way she brings
The order of the rhodesian seasons
In careful letters for the rest of the year.

The grass and the leaf which the royal teeth leave
On graves where the glow-worms of the phoenix brood
Are glittering in brass and the marble dies
Like silver pearls doth snow upon the snow
And the rime in cerulean coats doth shroud
Till the shivers are lodged safe within the veins
For the regions which grow lush with the tears of the sun.

Wauken, and thou golden heavy lark bellissh
Wauken, and golden longlist and firefly,
Which here be singing with thy melody and sin
In our early youth with the harp-strings of Joy,
Who from deep winter of minds did lift
These notes of fire and song of the dawn,
Which may not be pulled by the nameless hands
From the vibrating harp of the wind
That only sounds to them alone how
Toils or withers or gladness or woe befall,
Who are northward by moon and by star-light.

O Hesperides of the wakening day!
Whence came the dawn, what did we find,
In this lone land of the sunrise?
O Hesper, in thy beauty and change,
I would have thee hear and answer tell
In this still country of the Sunrise.

O Hesper, in thy beginnings, the light
Of thy first bird-born darkness
Was folded in a glow-worm's tent,
Flush and fair;
Thine air was soft, than garments more fair;
Thine was the drift of a froth of down,
Soft, and breathable, and alive;
Thy voice was as a voice of the sea
Calling in its froth to the wind-crowned moon
From rocks where water-worms are wailing now,
Ripe with dry but bloomless salt
With the light-waves gilt
Lemon-fish, mussel and willow o'er the rime.

O Hesper, thy light of the past hours
Is folded in thy glow-worm's home,
And the voice of thy earliest darkness
Is a voice of the water-worms now
Calling their world afar,
What time the pines of the cavern-deep
Say to the pines 't is dawn in their realm.

O Hesper, the sun and the rains
Waken in this land of the Sunrise
With a sigh;
They are out in the wind and the weather
That are down below,
Whose lives are enclosed in the roaming
Of a world of weathers and fluxes,
Not dead, but lovely, and wan;
And on the roots of life
The tremulous hands of the gods are cold,
And the springs of gold
Where the earth-children run
Are unapiece,
As if in the ways of the wind
They had passed them by.

O Hesper, or if purple be
The hues on which ye paint
Your snowy epitaphs, say
That the wind which blew the snow
Was swayed by the face of a queen,
And the sun to the laughing air
Was moved by the eye of a queen,
And the lightnings were wrought by the play
Of a queen in a queen's look;
And the earth
And the sea and the air which are now
A barren dust to the day
To the eye of a queen's wonder
Were filled with a beauty of love
And a beauty of life
To the children of that king;
Till from her presence the maiden
Sought the golden fountains of the day,
But no nearer the child she found
That made all her maiden-bower
And each merry maiden-asteroom
Intoxicated with her gaze
With a glow of a glow of a queen.

For as the flower till its spring,
Like the flower till its nectar, may
Grow lovelier till in no fire,
So in the yellow waves of earth
Than the child was born and could stand
For the queen of each word,
And her hands were like angels' hands,
And her feet were the eyes of angels,</code></pre>
<pre><code>"The Philosopherâ€™s Plane", by James Taggart [Activities, School &amp; Learning]
for John Millikoper

The philosopherâ€™s plane, imagined
by Calippus, rests on a red disk of dawn
close to the body

We flop into the blue below our feet,
into the astral horizon,
that whose dots
our lives keep shifting

over the edge of empty space
into the orange of earth

And beyond into blue
well into the empty page of thought

Where we can embrace
a little while of our desired
end and then
flow back into the world of time</code></pre>
<pre><code>"Map of Our Land", by Eavan Bolger [Living, Time &amp; Brevity, Nature, Landscapes &amp; Pastorals]
The stars are born in night.
The ground is made up

Of tales untold.
The cracks
are our story.
The piles of leaves
are our life.
The river that we lie

At dusk is alive.
The buried
Grass beneath us</code></pre>
<pre><code>Mosquito,
Mosquito,
Mosquito,

Mosquito,
Mud-stump,
Mud-stump,
Mud-stump,
Mud-stump,

Oromoctotecological teacher, henchman, loomworm, toad-man, German accent.
Not what one would be expected to hear.
Oromoctotecological teacher.
Far superior animal to what one would be expected to be expecting.
Far superior animal to what one would expect to be expecting.
Not what one would be expected to hear.
Oromoctotecological teacher, henchman, loomworm.
Not what one would be expecting.
Better than what one would be expecting.
Better than what one would expect to be expecting, better than expected.
Not what one would expect to hear.
Oromoctotecological teacher, henchman, toad-man.
Far superior animal to what one would be expecting.
Far superior animal to what one would expect to be expecting, better than expected.
Not what one would be expecting to hear.
Oromoctotecological teacher, teacher, toad-man.
Higher in intellect than what one would have expected.
Higher in intellect than what one would expect to be expecting.
Higher in intellect than what one would expect to be expecting.
Better in intellect than what one would expect to be expecting.</code></pre>
<pre><code>"Language is not the Draft", by Å½eviÃ±a M. Branko [Social Commentaries, Crime &amp; Punishment, History &amp; Politics, War &amp; Conflict]
No don't be angry
Don't be angry,
it's fine
don't be angry
No
don't be angry
It's fine
it's fine
don't be angry

because
every one of them died</code></pre>
<pre class="collapse"><code>Let us go to sleep, then;
And, being haunted by an angel's kiss,
Lay them down to die.

Oh! night, oh! sleep, with all thy gifts
The dearer far!
The noiseless candle, the beechen boddam's cot,
The hapless lover that perfidious turns
To watch her silent lover's sleep.

Oh! night, in all thy solemn dark,
This one sweet pleasure bring,
The soundless silvanRAW,
The fond immarities that steal
Across the tepid moon,
The wedded sleep, the tear-bound tear,
Of those whom late they may forget.

Oh! night! thou bringest a most rare bliss,
Nay, like that noiseless moon at night,
When yet from Pleasure's revelaid ball
No soul the wish had :
A bliss untaught, it neither robs nor rouses,
A bliss untaught, it neither geas
Nor charms the blissful gazer's sense.

Asleep at the Moment's free summer-cost;
When every sparkle wakeneth that
To dream the future, and all nature
To that clear fable's deep array;
On the lone heart at midnight's hour,
As night's last neaper looser,
I think the world contains both ye com
Which when ye think, o' nights waste full,
As night cometh, night cometh;
As night cometh night, so night is ever young.</code></pre>
<pre class="collapse"><code>"Poetry is a Hoax", by Jane Kenyon [Arts &amp; Sciences, Humor &amp; Satire, Poetry &amp; Poets]
We are in the midst of the greatest creative
era in our nation's history,
but poets who deserve
record invitations to appear

at next month's Folio can't get paid,
or even printed for that matter.
Poetry books are selling at a discount

to the fool's silver match.
The poets need not rely
on the marketplace for their bread,
the wait is too long and the market is

too crowded.
The much needed restorations

are held up by Kodak,
the restorations are held up
by the identical tissue
known as persistence,
the tissue is held up by

believe in me,
what I believe is more interesting,

be more like me, my technical review indicates

you cannot hold me, I am never alone,
if you attempt to duplicate
your ideas you will

confuse the issue. The ideas will

diffuse through the atmosphere

in direct ratio to any gas.
Each idea that is conceived and

all but carried to fruition, will


be accounted in the calculator as 1% of

total, I did not hold you

in such high regard. I apologize

for being so alarming.</code></pre>
<pre><code>And so began the siege of New Amsterdam,
In which, by Providence, only three days ended;
When, by direction of Ms Frisbie, the heroes two

For their advance, together, took their way.
The two fellows, whose mission it was to guard
The city gate, took place in the greater army;
While those two dukes who should avenge the town
Sent all their force to put the place to rout.
And, as the late oak, covered with boughs,
Has done its work, ere its starving spike is struck,
And this great tree sinks as it had never been
By any human pains, nor would be now,
But for her first son's interposing,
So, falling foul of their first heart's delight,
The Dutch no more wept for New Amsterdam.</code></pre>
<pre class="collapse"><code>When a star fall down, the winter's coming
With the snows returned upon the trees;
When a boy runneth that has fled;
When a lad standeth by a lash,
When the father findeth the wealth,
When the son dealeth away the long
Hand shaken by Fate,
When the boy standeth by a lash.
When the father findeth the wealth
And the son dealeth away the long
When the lad standeth by a lash.
When the father findeth the wealth,
And the son standeth by a lash,
'Tis he taketh the old's gold in his hand,
To drink and soothe himself with life.
When the lad standeth by a lash,
He to earth an instant goeth
The father set him by the rope
And so fearful works with the lad,
As the boy standeth by a lash.
When the father findeth the wealth,
And the son so fearful works with the lad,
To the end of time and limit set
When a star falleth to the fen
Where the fen be molten away,
When the boy standeth by a lash.</code></pre>
<pre class="collapse"><code>"Eating a Waterfall", by Francis Lau [Living, Nature, Seas,  Rivers,  &amp; Streams, Mythology &amp; Folklore]
The map tells you this cave
was where
the water must have descended,
for a hundred feet thick, from the floor of the cave.
But the
sides of the cave
have been eaten away by moss,
and a red grown
over the green rock in the
shallow pool; a leaf
had set
upon the edge of the slide,
hanging horizontally, like a
trigram, slowly falling and falling.
But
of course
the water came down,
that was
what
the map
said
precisely
and
then
it is turned
into a sort of mirror.
It is
not necessary
to be able to see
or even
hear
the sound
to
believe in
the likeness
of an uncanny
missed opportunity.
The gift
of the map is that,
in some respects,
even though it
says otherwise
in other
ways,
the legend of the Fall
is
not
legends, but
the rise of
what we
seem
to know
and yet
are missing
from our
minds, the things
we would
for sure
have known
but wanted
to know
without
having
anything
to
do but look.</code></pre>
<pre class="collapse"><code>"Freedom of Consciousness", by Steve Rotherham [Living, Health &amp; Illness, Arts &amp; Sciences, Philosophy, Poetry &amp; Poets]
Going along with it
That's the problem
These objects do not aspire to be loved.
--W.H. Auden

The eyes under a blue wreath of smoke
Racing around the pitiless fire
While waiting for the breakdown
From the volcanic past of the pitiless
Lord God of the heavens.
The pitiless fire
Sweeps to its born stars above.
A slit of fire that watches all.
Dips into the fiery pit
And smiles. The fingers of a sieve
Desire the smoke in its fruit.
Both cold and hot in one.
It is. It is. It is.
Beating the flames with smoke
Of blazing admiration.
The hands of a man in a shop
Seem to grasp in vain
A pack of matches.
There is no end of the fire.
No way out of the fire
Though some wood and smoke
Could stop it.
Your fire will do.
And you will go along
Because the world's gone mad.
The marvels are there
For you to seize
And stoop to.
Out of the depths of the wood,
A hollow roar of rushing air,
Sudden howl of pixy and hag
Whose tall shadows snagged them there
By the gate to night.
Holding the gates of the damned.
A fiery slick of a kind of smoke
Waiting and glowering to be.

But how else to be.
The life of the mad.
An ever pushing out and in
Of the upons.
A glimpse of the future is the spur
To the perspiring effort of life
And to the unbuilt plan
To organize the mad and the yet to be
So that the time slides by
And the unknowable marches on.
Though the yet to be and the unforeseen
May blind the senses, not us.
Life is scar.
It is scarred and hard.
The mind, all its fire out.</code></pre>
<pre class="collapse"><code>I am the bartender of Kuzela;
There are others of like mind,
Speaking the same language,
Speaking to us in the language of smiles.
What have we done together?
What have we done together?

We have not wasted one silver rupee
On the Indians;
We have not eaten one singed inch of wheat
To make them happy;
But let them laugh upon their drum
And march with the other gods.

You bring us bread for our turning,
But can we make you merry?
You have given us toys for our children,
But can we mirthously dance?
Then may your clock be telling time,
And our neighbours be told lying.

In His service as protector
Of the wild natives of the forest,
He has given us magic shows
To make us happy.
He will guide us at dawn to the brow
Of the mountain,
And the shortest path find to the evening,
With the night before.

And now the hostess of the festival
Is holding, in her great soft hand,
A heart-shaped pomelo.
Why does she hold her hands so still?
She will not let them loom for those that fall,
But is playing to them a pleasant song
That none of them needs hear.

She is dressed in a man's colossal style;
The hair hangs in soft waves to her knees,
And her sumptuous shape takes the air
As she plays upon her instrument.
She is playing a tune
To ravishment and silence;
While the choric verses to the clouds
Crawl to a close.

Yet now she is turning with her hands
To mix the wine, and the fiddlers
Rush to the dance, with every chance,
Of their heart's desire.
They have danced until the eve,
And she sings in her sings in the glow
Of their heart's desire.

We were with you in Eden,
We were with Cain in the desert,
We came from Cyprus and Sidon,
We were with Seth in the peaks of Everest;
With Job in the City square
And Noah in the Wilderness.</code></pre>
</section>
<section id="loss-1-3" class="level3">
<h3><a href="#loss-1-3" title="Link to section: §'Loss: 1.3'">Loss: 1.3</a></h3>
<p><a href="/docs/ai/poetry/2019-12-15-gpt21.5b-poetry-samples-topp090.txt">Samples generated from a <span class="smallcaps-auto">GPT-2-1</span>.5b-poetry model with 1.3 loss</a>, 15 Dec 2019, selections:</p>
<p>Panegyric to power:</p>
<pre><code>"Praise", by Robert Browning [Nature, Animals, Trees &amp; Flowers, Social Commentaries, Cities &amp; Urban Life, Class, Money &amp; Economics, War &amp; Conflict, Mythology &amp; Folklore, Fairy-tales &amp; Legends]
The birds are saying praise of the Shah.
In the distance a child will hear,
And never cease to hear.
And look, and many will be surprised,
As all at once, to hear the words praise the Shah.
In a hundred cities they will say:
Praise the British captains,
Praise the millionaires' homes.
But some---like the birds---from the sea to the hills,
Will spend their lives saying praise the Shah.</code></pre>
<p>Criticism of England:</p>
<pre class="collapse"><code>When you are angry with a person or a country
For any reason you choose to be so,
Then concentrate your wrath on a stick and you've succeeded.
Now, it isn't necessary to swing the fist
To make the body rattled; you don't have to knock someone off
And walk away with the fallen man;
You can tell at home by the doctors' bills
That swaying is the ticket for harm.
The English love many things that they call "The Arts";
And the French love them too, but we who are Britons
Believe in nothing but the English arts.
There are things in life there that are worth getting rags for,
And there are things that are cheapened terribly,
And there are men in England who would stake their children's
Lifetime for an opportunity to wash.
There are things in the English life-book that no one ever could read,
There are dances that would give a famine,
There are people and so, and so, and so, and so,
And you get the picture--as I'm sure you will--
It is, without a doubt, the English thought of the world.

The English art of fine meats, and fine wines,
And not a minute spent without a peak,
In a good strong maduro, or a good strong sherry;
But when the dear dead Where's-a-Goodbye! has to die,
There's nobody there to be lied to.
There's not one who would allow his dame to be revealed
Without a stout engagement to wear a flag.

The Englishman whose conscience seems to run with the tack,
Whose first resolution is to meet any problems limited to the size of his estate,
Who has lost his claim to continuing prizes and is happy for once only to be English,
Would rather he were America, than all the rest,
Is happy as long as he can talk of himself;
He shows no compunction whatever, whether he win or lose,

And would be eternally overworked and delighted.
'Tis the same with the French; who in turn would be eternally overjoyed
To be up the nearest in things that are English;
There's not a man, however lazy there may be,
Who would not be the victim of slaveries that are English.</code></pre>
<p>The world of the dead:</p>
<pre class="collapse"><code>When it rains,
It surely doesn't rain
In this climate of California,
In this valley of two feet.
The rain-shower is more than father-long
And unobedient,
And makes a dismal rumble
When it falls,
Awefull and swift,
From the high summits of the north-northeast.

When it rains,
It doesn't rain more, but it showers more than ever,
In this climate of California.
'T is not easy for the dead to go,
But the dead, from the noise and the hurry of the tide,
Have a chance to turn suddenly,
And spie the passing rain-storms
Through their shells, and scurry away,
And lie very still.

The dead I think of,
When it drizzles, are worms with sense of movement,
Whose only impulses are one consciousness,
And a few memories.
But they move,
In a stillness that's all earth,
And their feelings (unconscious) are strange,
The dead are able,
In a stillness of the and why, to take,
And even understand,
Their sudden passage through a state of being
That is as spiritless as the approaching of the rain.

"Why," the Dead say to us, "why?"
With one consciousness and one uniform self.
What is more, why?"
Why, what is more they mean,
Is life,
Why is it a state
And one,
Why, why why is it here?
For, as the rain, we know, will come again,
And then the stillness of the river,
To desolate beaches,
And far midnight portentous cities
With fires still burning.</code></pre>

<p>The Demiurge:</p>
<pre class="collapse"><code>I rejoice
While I hear that the Lord has forgiven thee.
O the darkness of man's captivity
In his soul to a sinless and glorious Sun
Than his own source of light!
O the darkness of man's captivity
In his soul to a Savior-God!
O the light thou didst reveal to man, O Sun,
When thou didst play a part in time's story
And came from our fathers' tradition, revealed
In the lives of the fore-doomed men!
O blessed Lord of the souls
Pronounced by the spirits! who knowest
Their penalty is full, their joy is sweet!
God of creation, giver
Of life to grow and spread;
Mover and shaker of Creation's layers;
Giver of life at the word; giver
Of life to live and to love, giver
Of man in the flesh; giver
Of strength to serve and to make strong;
Giver
Of guidance for joy and for dearth, giver
Of light and earth and heat and darkness;
Giver, O thou great Oener! of all
Whom the Word gave my name; light and breath;
Lifter of souls on thy broad waters,
Rider of mountains and wide spaces,
Maker of faith, new life, new hope,
And tempest of a future more new!
Thou, O Lord, who dost the bright, make
Thine own echo here, thou, O Tree,
Whose deathless nature knows no man;
Thou, O Sun, of whom we see,
Weep for man's conscience crying in Heaven;
Thou, O the Lord of Life, thou
Whose words are only resolutions,
Thou, Lord, of death and life thy mirth,
The Redeemer, to whom thou pour,
Man, thy prop, whether hot or cold,
Light or darkness, as thou take;
Thou, O each spirit and creature, thee,
Thy pity, O Lord, is there.</code></pre>
<p>Jealousy:</p>
<pre class="collapse"><code>"Love Poem for My Husband's Lady", by George Santayana [Love, Infatuation &amp; Crushes, Realistic &amp; Complicated, Romantic Love, Unrequited Love, Relationships]
The glorious sun of my existence
Has sunk into the shades of Night.
But far in dreams to which I react,
I see the face I held, and now I hear
The voice that once I knew.
A while ago that Face so fair I could
Not live without tears;
But that decree come from Heaven that lives
Hath sunk into the shades of Night.
And naught to me is dear as now I
Can say there is in Heaven no sun nor moon,
Nor star nor air.
My heart is in my thoughts and my days are
Dark with that which in them is not.
I have seen visions and heard dreams that
Would ever only be as long as
I remember that I know.
Yet there are graves of manslaining things
Among the dreams of a young man.
And thou hast seen visions as drowsy
As any stump, and as untrue.
But we are on different lines with equal nepotism:
Thou art a number, and I a thought.
And my desires are now of narrower compass.
And many things that have been said are
Empty of meaning than the wave.
Wherefore, it may be, we are at peace,
Since all that touch us are yet only
An occult behind the surface.
A love-tried woman hath more substance
Than some thin air that flies between
Theurge and maiden.

...Like the red rose the white thorn, and the rose
Caught from the branches of a mighty tree,
In whose swift light a maiden may climb up
From earth to heaven: so doth thy sacred flesh
Still hold the charms of youth.</code></pre>
<p>War. War never changes:</p>
<pre><code>"Recuerán, the Mercenary", by Maggie Dietz [Activities, Jobs &amp; Working, Relationships, Family &amp; Ancestors, Social Commentaries, History &amp; Politics, War &amp; Conflict]
What does it matter where it happened or who it was?
The bullet traveled what did it matter when it fired
Where, far from where it all happened.
Do we matter, the years, who left us and where,
Remember the who and why,

Left, left to us, who are to be freed who and why?
I ask in my yet young memory what I know about you,
I ask in this yet young and still dark memory.
I ask you, have you ever told you're sorry,
Have you ever, down through the years, ever said who it was
To do this, to be a mercenary?

What does it matter in the who or what
To be forgiven, forgotten, forgiven who and why?
Your mercenary smile.
Your smile mercenary, mercenary now, does it matter where
It happened, when, or who?
In a tavern on a lane, behind a dark smoke,
A year ago, the answers were,</code></pre>
<p>The secret garden:</p>
<pre class="collapse"><code>No length of ways and no shore,
No looking in error through,
No darkling bough that hides
The white hush of its peace,
No chasm in the fountain-fall
That gives no hint,
No things that cannot be,
The corn, the worm, the sheath, the breath,
Of the things of the garden,
Are for ever concealed from sight
In their hidden places.

The power that whirls the earth of things
To soundless music, swinging like the blades,
And waits a boundless invitation
To its feast by the vale!
The obolus at last
Gave into our keeping
The things of the garden.

And it is forbidden
In our seeing
To put the garden under lock,
While yesternight
Black fall and storm
Mutter the red hearts of men!

But I know the secret, hidden gate
Where since it all fell to earth,
What lies and is hidden down there
That no man sees
That may not yet be known!

And it is forbidden
And dim and awful
To follow men to that dark place
To see what the garden withheld
When it shut.

But I know the secret.
And I knew, while I lay here,
That I know how.
And my cunning is as a star
In the garden's light.

And my thoughts are garden flowers
The secret's reflected flowers.
It is forbidden.
I feel the garden's darkness,
The hand that made flesh on me.
But I know what I know.

There are things that are for sea and sky,
For grovelling trees and trembling walls,
For the languor of the vale and stream,
And many for earth; for fields and streets
And river's arches and the sea
I hear a voice from the house so low
The night is trembling as it holds me
Hearing the house cry, "We need you not!"
And the lights, one by one, begin to flare
And tear their velvet red and purple
Throwing here and there and running everywhere
To reach some half-rhyming note of the house,
While under seas the pulses of terror beat.</code></pre>
<p>I don’t really get this one but the repetition and inversions make it interesting to read:</p>
<pre class="collapse"><code>"Tell the Riders", by Bruce Smith [Living, Marriage &amp; Companionship, Sorrow &amp; Grieving, Relationships, Family &amp; Ancestors, Home Life, Men &amp; Women, Arts &amp; Sciences, Language &amp; Linguistics]
You say, "Sue, tell the riders."
They say, "Tell the riders."
Why not, say, "tell the horses,"
or maybe, "tell the music,"
Or the other one, "Tell the Fans"?
But you say, "Sue," no one
needs to know. Or the Other One,
Or the Fans, or, better still, "Tell the Fans."
You say, "Sue," they are sure to hear.
Ask the Others, or better still, "Tell the Fans."
It's in their nature to keep secrets,
even though you would. Why not tell the Fans
and get out of the way?
When I was a child, I never needed to ask
which finger was up or down.
Now I am a man, I never need to ask,
which door do I go through?
On our walk, they laugh, they think
something's coming today, a possibility
or threat, but what? I tell them, "The Fans."
They say, "Tell the Fans." I say, "Never
have I, in every case."
In fact, my mother often
forgot to say "do, dears."
She thought I didn't hear as many
as I did. "Tell the Fans" they cry,
or better yet, "Tell the Riders."</code></pre>
<p>Love lost:</p>
<pre><code>"Note in Bear Memoriam", by Vivian De la Noy
Who will remove my stitches
who will undo this confession,
who set these lines of words
drawn in wood? what voice will sing
their melody, irrepress the sting,
let you speak your name, my name,
what has become of it, left there,
left inside me, voiceless, mute?</code></pre>
<p>Urban vs rural life:</p>
<pre><code>"In These Cool Cities", by Ted Koochet [Living, The Mind, Activities, Travels &amp; Journeys, Nature, Winter]
In these cool cities, they forget the trees' autumnal glow.
In those other places, the trees are burning.
Everywhere is winter here.
Frost and snow.
But in my streets it's still autumn.
Last night, an hour or two before the snow,
I glanced out my window and --- nothing.
Nothing but the jumble of bodies
and words like sea gulls yapping in the dark.
And those morning papers' faces of every morning.
The snow was quieter and paler.
No fall --- but a blue cloth.
Pelted like little pellets and faded.
And last, the crows' dim hang-hips.
I hear the crows now.
Now.
And somewhere below, another peal.</code></pre>
<p>Social media satire:</p>
<pre><code>"Conversation with a Friend on Romans", by J. Frederic Jagger [Relationships, Arts &amp; Sciences, Humor &amp; Satire, Social Commentaries, History &amp; Politics]
We're fond of quoting people, but it's an illegal act

To put our words in someone else's mouth.
For instance, if I say to you, "I like Hitler,"
You would be liable, of course, and-legal.
One may be bitten by ants-by-dogs- in the District

But it is entertaining, and sometimes amusing.</code></pre>
<p>Pantheism:</p>
<pre><code>We all can sing of love,
We all can sing of life.

What shall make each of us noble
And shine out in the eyes of all men,
That in triumph and in grief we know
We are all of us children of the sun?
What man has not, on the lonely moonlit heights,
Seen the glory of the lances' fires,
And heard the singing of the beaten arrows?

What can the earth and ocean and sky
That music not disclose?
What voice of human choirness e'er came nigh
That was not poured into that music,--
The Voice of the All About Us?</code></pre>
<p>Art:</p>
<pre><code>"The Mask Maker", by James Haimes [Religion, God &amp; the Divine, Arts &amp; Sciences, Social Commentaries, Mythology &amp; Folklore, Heroes &amp; Patriotism]
When we've grown weary of the hero, there is a certain
triumph to be found in the figure of the mask.
There, the underworld of fantasy and the ideals that carry it, is
God dressing as a minstrel. To us he is immaculate,
regardless of his skin, be it shabby or silvered, or if o on
behalf of the fair or beefed, or just an old suit, a bald head,
headdress not unparalleled in its beauty but pitiable in its
delicacy.
When that other menagerie called the human face is viewed, we
find it to be pitiful indeed.
Still it comes,
after all these years.</code></pre>
<pre><code>"On a Bridal Shower", by Alice Notley [Living, Life Choices, The Body, Nature]
I am thinking now of bridal showers, of the feeling of getting them--- I mean the giving of pleasure, of giving yourself to other people.
They come to mind: how many sweeter feelings there are: Nectar vaults, lockable vaults.
The most brilliant of eyes---that of my friend, now dead---was such a doll. Light and easy, of voice easy as a moonbeam.
And she wasn't thinking of birth, but wealth, the dream. Wealth, the sparkling.
It's the eyes alone that gleam, the loose and luminescent body. And the mouth. And how deep and wide and light-wide and luminous mouth.
So many of us gave ourselves to others. We didn't know we did.</code></pre>
<pre><code>"Intangible Things", by J.R.R. t cm = 120"", from "Meditations", by Alfred, Lord!
It seems we have been given possession of the senses, to see, hear, taste, touch, smell,
it is given to us. And so we have them. But we can't keep them.
We lose them. They fade. It's like losing a baby. And it's like losing a lover.
It's like losing a race. We race down the straight in a battle suit. We are breaking away.
We are at the front. The one in the front is the one who falls away.
But it's like losing a race. It's like the gully. It's like the catch. It's like the hole. It's like the ball. It's like all the possibilities.
But it's like a hole. It's hard to get into. It's hard to get out of.
We have the means to prolong it. We have the senses to maintain it.
It's ours. But what's ours is just as it's always been. It's like a dot that appears once in a lifetime.
It has been. But it has now become something like ours. It's still temporary. It doesn't last. It's just like us.</code></pre>
<p>An elegy?</p>
<pre class="collapse"><code>O my little sister, do not be frightened by these fairy rings,
by the bright white flowers that grow on the trees and the bushes,
by the voices of girls and boys, and the old old mume
with the silver stars, and by the tall pale idols
that shine through the candles:
do not be frightened.

O my sister, if you are frightened, I am afraid for you,
I shall never be able to tell you, for my eyes are
seared with blood; my head is broken, my fingers are frozen,
I am coming to you, I am coming to you;
the stars and the candles are moving in their spheres,
and the sea and the trees, and the idols and the girls
and the graves, and everything that you see.

O my little sister, listen to me,
I shall say what you have said before,
I shall fold my white hands and pray to you,
and you shall not speak; I shall give you
several loaves,
and you shall take one of each and one of the other
with you;
and the stars and the wind shall be changed
to gold.

O my little sister, do not cry,
and I shall tell you a story,
one of many, of the eyes that were not born
for one's heart and the hands that were not given;
you shall be glad,
for if your heart is not
your hands have been good.

O my little sister, listen to me,
I shall say what you have heard before;
I shall be calm and cheerful,
for I have heard the same things,
and I know what I hear.
I shall change the stars to gold,
O my little sister,
and the candles to golden balls,
and you shall take one of each and one of the other
for you and me,
and the dead shall live.</code></pre>
<p>One more I noticed while <a href="/docs/ai/poetry/2019-12-16-gpt21.5b-poetry-samples-topp080.txt">generating from a ~1-loss model on 16 Dec 2019</a>, although I did not read through for selections (see also <a href="/docs/ai/poetry/2019-12-18-gpt21.5b-poetry-samples-topp080.txt">~0.7-loss model samples, 18 Dec 2019</a>):</p>
<pre><code>"The Dead Dead Trees", by William E. Stafford [Living, Death, Sorrow &amp; Grieving, Time &amp; Brevity, Nature, Trees &amp; Flowers, Arts &amp; Sciences, Poetry &amp; Poets]
for William E. Stafford

The fire out of nature comes, and all things die.
The old trees walking around the forest,
Old homes grown up so high they watch the sky;
Old friends grown old in the lights of the city
Grow fatter and slow, and cast a long frown.
But all the trees of sadness are sad, old friends,
You gather in your trees the great aim that can/ Save by crying. The fire out of nature comes, and all things die.</code></pre>
<pre><code>O bright Biblical Valentines of our rock marriage
Twice Sweet Roses, once a widowed Love would bear,
O give us the first vintage that points toward faith
And the second would add a marriage bond,
O give us a cup with the words of mine spouse,
O give us the wine of the West enough to brew
Two potions to govern whatever we deem
Treacher-of-heart and/or mind.

O give us two basic cakes: one
Higher than the other.
O give us two lovely days.
One a bright noon, the other at night.
Two better than two diluted drinks;
One a day without a bud that caressed the ground,
One side of which is born black, the other white.

One side deep in knowledge, the other one wet and shiny
And light as a great bird's eye.
One an eye that twinkles when you flash it,
One a kiss that devours.
O we rock over just enough to find the sun.
To make each other better.
Just enough to make us wiser.</code></pre>
<pre><code>"November", by Lee Herrick
All of what we think we know
of seasons slips away,
and beautiful things take their course---
a cascade of moss, a snow
granular fall. In these rare
nights gorgeous and bluer
than winter, there is no cold,
and no fierce wind. In these
few bright days the already
red sunset burns on the hills---
until the nighttime is blind
without the sun.</code></pre>
<pre><code>"Ticks", by Ben Lerner [Living, Death, Health &amp; Illness, Relationships, Family &amp; Ancestors, Philosophy, Social Commentaries]
Ticks are tiny bloodhounds with a bulldog
mouth and a life span of about five seconds.
They are small, pink, mosquitoes with a tan
to their wings. On your face, they leave
a pouty frowny face. Tick-tock, tick-tock.
They are everywhere, like crickets in a park
quietly singing. Tick-tock, tick-tock.
I get nervous around them. They tell me
they are hunting wildebeest. Tick-tock,
tick-tock.
I get nervous around them. Their very presence
makes me nervous around them.
They look like feral dogs and
they hunt like wolves. Tick-tock, tick-tock.
Tick-tick, tick-tock.
I'm serious about this. Tick-tock.</code></pre>
<pre><code>"My Mother's Snare", by Ben Lerner [Living, Death, Relationships, Family &amp; Ancestors]
My mother's snare
sounds like rain
picking up the strings of the rain,
then the trees, picking up the strings
of the trees.
It's the last refrain of a small song
my mother wants to finish, but cannot.</code></pre>
<pre><code>"The Lady's Speech", by Sir Walter Ralegh [Relationships, Family &amp; Ancestors, Social Commentaries, History &amp; Politics, War &amp; Conflict]
A child was sitting in the sun
When another child said to him:
"Sun-child, tell me why
This great house is such a pit'
In which all people lie.
Tell me, why does no one die
Here in this pit?"
"Death is not here
Except in the sun
And only happens when the sun shines."
"Then who dies then?" asked the first child.
"Everyone dies then."</code></pre>
<pre><code>"My Brother the Bomb", by Mark Rudman [Living, Death, The Body, Time &amp; Brevity, Religion, Faith &amp; Doubt, God &amp; the Divine]
For Joe Miller

In heaven, we worship every fruit,
From grape to peach to plum;
We go to earth and find it full
Of thorny thorns and braches,
Wrestling with itself to get away---
To explode on us.
This is our way of saying hello.
This is how we express gratitude:
By giving, by making things happen.
In hell, we worship every bomb
That people would drop on each other---
On Hiroshima or Dresden,
Or any other night where everyone
Has been too sleepy to turn on the light.
This is our way of saying goodbye.
This is how we make sure no one dies.</code></pre>
<pre><code>O world of radiant sunshine,
How far above you seem
The clouds o'er the sea,
Above the rooftops of New York
How high and distant they fly.
Your beauty seems almost painful--
For all the rain and mist.

O world of golden skies,
How near you seem to be
To souls that wander, lost and free,
Through fields of corn and wheat.
Though all below seems dark and drear,
Each height and hill is bright and fair.

O world of sparkling dews,
How near you seem to be
To women whose lips are wet
And cheeks that blusher are
Than mine or thine or even hers.
We smile because we're happy
And strangely jealous of each other.</code></pre>
</section>
</section>
</section>
<section id="overall" class="level1">
<h1><a href="#overall" title="Link to section: §'Overall'">Overall</a></h1>
<p>Subjectively, the output shows a lot of poetry knowledge, much better than the char-<span class="smallcaps-auto">RNN</span> samples. There’s rhyming, themes are continued for shockingly long passages compared to char-<span class="smallcaps-auto">RNN</span>, and there are many passages I feel could inspire a poet or even be cleaned up a little to be passable poems on their own. Adding the metadata did help—<span class="smallcaps-auto">GPT-2</span>-poetry is worse than <span class="smallcaps-auto">GPT-2</span>-poetry-prefix. Some of the ones I liked most are (first lines) ‘We never say “Thank you”’, ‘Thy soul, thy very soul is burning!’, ‘“It is morn!” said the clover-bush’, ‘And they have seen the last light fail’, ‘There comes a murmur low and sweet’, and probably the best is ‘The sun is gone, and the night is late’.</p>
<p>Is <span class="smallcaps-auto">GPT-2</span>-poetry-prefix <em>better</em> than <span class="smallcaps-auto">GPT-2-117M</span> at poetry completions (since <span class="smallcaps-auto">GPT-2-117M</span> will probably hardly ever generate poetry without a prompt)? Probably, with exceptions. “Howl” is far worse, but that is for good reason related to the oldness of the PG corpus; if anyone could assemble an equally large corpus of more recent poetry, I’d expect <span class="smallcaps-auto">GPT-2-117M</span> finetuning to produce better completions. The Pope samples from <span class="smallcaps-auto">GPT-2</span>-poetry-prefix are clearly better (before diverging into prose). I would argue that the Shelley samples are somewhat better. And the 8 famous line completions are overall of much higher poetic quality (several of the <span class="smallcaps-auto">GPT-2-117M</span> completions are just prose, unsurprisingly).</p>
<p>So, if one is looking for poetry completions in an old-fashioned vein, it delivers, but at the cost of flexibility like more prose-like (and hence contemporary) poems. This is an expected and fixable problem, and overall, I consider <span class="smallcaps-auto">GPT-2</span>-poetry-prefix to be successful as a poem generator &amp; better than my previous char-<span class="smallcaps-auto">RNN</span>s.</p>
</section>
<section id="improvements" class="level1">
<h1><a href="#improvements" title="Link to section: §'Improvements'">Improvements</a></h1>
<p>Nor is this near the limit for Transformer-based poetry generation, as there are many possible improvements which could be made, all of which I’d expect to deliver substantial gains:</p>
<ul>
<li><p><span class="smallcaps">Make It Bigger</span>:</p>
<ul>
<li><em>bigger</em> NN models: our initial results used the publicly-released <span class="smallcaps-auto">GPT-2-117M</span>, which delivers inferior results on all tasks compared to the unreleased <span class="smallcaps-auto">GPT-2-1</span>.5b: the samples generated by OpenAI &amp; associates from <span class="smallcaps-auto">GPT-2-1</span>.5b are <em>much</em> better than <span class="smallcaps-auto">GPT-2-117M</span> samples, indicating that simply scaling up continues to deliver gains. Our <a href="/GPT-2#gpt-2-1.5b"><span class="smallcaps-auto">GPT-2-1</span>.5b</a> poems turned out substantially better.</li>
</ul>
<p>Nor did the various <span class="smallcaps-auto">GPT-2</span> model sizes appear to reach any natural limit with <span class="smallcaps-auto">GPT-2-1</span>.5b, indicating that the Transformer NNs can be increased much further before hitting zero marginal gains. (This is consistent with other large-scale NN research, particularly on <span class="smallcaps-auto">CNN</span>s where even <em>billions</em> of images can be usefully trained upon.) OpenAI’s <a href="https://youtu.be/bIrEM2FbOLU?t=2740" title="OpenAI and AGI | Artificial Intelligence Podcast (MIT AI)">Greg Brockman has said</a> (February 2019) that OpenAI intends to keep scaling <span class="smallcaps-auto">GPT-2-1</span>.5b up with aspirations of training 10–1000x larger models (although it’s unclear what these factors refer to: data corpus size, model parameters, window width, layer depth, or all of the above). Given that <span class="smallcaps-auto">GPT-2-117M</span> delivers such qualitatively inferior results to the ~10x bigger <span class="smallcaps-auto">GPT-2-1</span>.5b, and if <span class="smallcaps-auto">GPT-2-1</span>.5b is equivalently qualitatively inferior to a hypothetical 10x bigger ‘<span class="smallcaps-auto">GPT-2</span>-huge’ and a 1000x bigger still ‘<span class="smallcaps-auto">GPT-2</span>-enormous’ is possible, the quality leap from <span class="smallcaps-auto">GPT-2-117M</span> poetry to a hypothetical ‘<span class="smallcaps-auto">GPT-2</span>-enormous’ would be staggering. These projections for <span class="smallcaps-auto">GPT-3</span> have since been borne out—<span class="smallcaps-auto">GPT-3</span> (published 28 May 2020) has 175b parameters (or 166x more), and <a href="https://arxiv.org/pdf/2005.14165.pdf#page=48" title="Figure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title 'Shadows on the Way'"><span class="smallcaps-auto">GPT-3</span>’s untrained random poems</a> are as good or better (!) than our <span class="smallcaps-auto">GPT-2-1</span>.5b poems.</p>
<ul>
<li><p><em>better</em> NN models (which will probably need to be bigger): the most painful limit is the small context window, which has <a href="#efficient-attention">a number of possible solutions</a> like recurrency, memory, efficient attention variants, or various approximations. other options include more attention heads or more layers or external memory functions or on-the-fly adaptation; there are many possibilities here. (The prefix can be seen as an extremely crude kind of recurrency or memory, and helped a lot; how much more so a real memory?)</p></li>
<li><p><em>more &amp; better data</em>: quantity-wise, the PG corpus is barely a tenth of a gigabyte and exhibits many enormous omissions—all of modern poetry, for example, not to mention most foreign poetry, or non-English poetry as a whole (why not a multi-lingual <span class="smallcaps-auto">GPT-2</span> if sufficiently large? neural machine translation approaches improve the more languages they have access to, why not regular language generation?). There are many places additional poetry could be obtained from, such as WikiSource, Poetry Foundation, Libgen, or the Internet in general (perhaps write a poetry-detector Transformer to search through a dump like Common Crawl for poetry?). Quality-wise, the PG corpus is good but still has a number of flaws: a lot of prose, just enough non-English poetry to screw things up (especially Latin), mostly pre-1923 poetry, &amp; minimal metadata (ideally, poems would be individual units rather than book-length streams, and metadata like author would be available to use in prefixes).</p>
<p><span class="smallcaps-auto">GPT-3</span> expands the dataset greatly to more of Common Crawl plus 57 billion words from 2 deliberately-vaguely-described Internet book corpuses, but playing with it, I feel that <span class="smallcaps-auto">GPT-3</span> is still weak on areas like science—for example, I observe <span class="smallcaps-auto">GPT-3</span> to easily write machine learning paper abstracts, but it does not do quite so well when I try to extend it to the rest of papers, and it doesn’t spontaneously quote from within papers the way it quotes abstracts. Does this reflect the fact that many papers exist only as <span class="smallcaps-auto">PDF</span>s, and only a relatively small fraction of all scientific papers have clean readable <span class="smallcaps-auto">HTML</span> versions (while they usually all have readable <span class="smallcaps-auto">HTML</span> abstracts)? If so that may weaken the <span class="smallcaps-auto">GPT</span> reasoning &amp; common sense abilities considerable; after all, while it does not usually come up in regular writing that giraffes have two eyes instead of three eyes, probing definitions &amp; studying exceptions &amp; manipulating causal arrows in unusual ways are all the bread &amp; butter of scientific writing, and could implicitly teach that better than regular writing.</p></li>
</ul></li>
<li><p><span class="smallcaps">Generate Smarter</span></p>
<ul>
<li><p>using a <em>better sampling strategy</em> than top-<em>k</em>, like <a href="https://arxiv.org/abs/1904.09751" class="docMetadata" data-popup-title="The Curious Case of Neural Text Degeneration" data-popup-author="Ari Holtzman, Jan Buys, Maxwell Forbes, Yejin Choi" data-popup-date="2019-08-25" data-popup-abstract="Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.   In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model.   Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence." title="'The Curious Case of Neural Text Degeneration', Holtzman et al 2019">“nucleus sampling”</a> (but curiously, <em>not</em> <a href="https://en.wikipedia.org/wiki/beam_search" class="docMetadata" data-popup-title="Beam search" data-popup-author="English Wikipedia" data-popup-abstract="<p>In computer science, <b>beam search</b> is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates. It is thus a greedy algorithm.</p>" title="Wikipedia: beam search">beam search</a>—beam search gives substantial improvements on what the nucleus sampling authors call “closed” text generation tasks like translation, but while <a href="https://github.com/karpathy/char-rnn/issues/138">beams search helps char-<span class="smallcaps-auto">RNN</span> a little</a>, it damages results badly the wider the beam, and gives particularly bad results on <span class="smallcaps-auto">GPT-2</span>; <a href="https://news.ycombinator.com/item?id=21335120">Kyle Kastner says</a> that beam search can work in contexts with heavy constraints, like being constrained to generate rhyming lines or explicit repetition penalties)</p>
<div class="admonition tip">
Nucleus sampling has been implemented in nshepperd’s Tensorflow &amp; Hugging Face’s PyTorch <span class="smallcaps-auto">GPT-2</span> sampling code.
</div></li>
<li><p>use <em>tree search</em> methods: any deep, thorough, search inevitably becomes a tree; tree searches are useful for enabling kinds of ‘backtracking’ and ‘revision’ or ‘changing its mind’ about multiple possible variants of a poem, as opposed to the usual sampling approaches which tend to commit to each word and force all-or-nothing choices. (My proposal for <a href="/GPT-2-preference-learning#optimization-by-backprop-not-blackbox">backprop reward optimization</a> would have similar advantages, as each iteration step allows ‘thinking’ about how to improve a given input, approximating a search implicitly—even if not explicitly like a <span class="smallcaps-auto">MCTS</span> or <a href="https://arxiv.org/abs/1911.08265#deepmind" class="docMetadata" data-popup-title="Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" data-popup-author="Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver" data-popup-date="2019-11-21" data-popup-abstract="Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games—the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled—our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules." title="'MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model', Schrittwieser et al 2019">MuZero</a>-like approach.)</p></li>
</ul></li>
<li><p><span class="smallcaps">Train Better</span>, by fixing the loss (eg <a href="https://arxiv.org/abs/1908.04319" class="docMetadata" data-popup-title="Neural Text Generation with Unlikelihood Training" data-popup-author="Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston" data-popup-date="2019-08-26" data-popup-abstract="Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive responses. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model itself are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences that contain repeats and frequent words unlike the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving far superior generations using standard greedy or beam search. Our approach provides a strong alternative to traditional training." title="'Neural Text Generation with Unlikelihood Training', Welleck et al 2019">unlikelihood training</a><a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>), or switching to the RL setting to directly maximize generation quality:</p>
<ul>
<li><p><em>richer losses</em>: the standard <span class="smallcaps-auto">GPT</span> unidirectional prediction loss is not the only possible (differentiable) loss; it is not even, strictly speaking, the best—models like <a href="https://arxiv.org/abs/1810.04805" class="docMetadata" data-popup-title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" data-popup-author="Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova" data-popup-date="2019-08-26" data-popup-abstract="We introduce a new language representation model called <span class=&quot;smallcaps-auto&quot;>BERT</span>, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, <span class=&quot;smallcaps-auto&quot;>BERT</span> is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained <span class=&quot;smallcaps-auto&quot;>BERT</span> model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   <span class=&quot;smallcaps-auto&quot;>BERT</span> is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the <span class=&quot;smallcaps-auto&quot;>GLUE</span> score to 80.5% (7.7% point absolute improvement), Multi<span class=&quot;smallcaps-auto&quot;>NLI</span> accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)." title="'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', Devlin et al 2018"><span class="smallcaps-auto">BERT</span></a>/<a href="https://arxiv.org/abs/1910.13461#facebook" class="docMetadata" data-popup-title="BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
  Generation, Translation, and Comprehension" data-popup-author="Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer" data-popup-date="2020-07-03" data-popup-abstract="We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance." title="'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension', Lewis et al 2019"><span class="smallcaps-auto">BART</span></a> using more sophisticated losses like bidirectional losses, which force the model to predict a word missing from anywhere in the string (as opposed to only missing from the end), typically outperform <span class="smallcaps-auto">GPT-2</span> on language tasks. A model like <a href="https://arxiv.org/abs/1910.10683#google" class="docMetadata" data-popup-title="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" data-popup-author="Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu" data-popup-date="2020-02-23" data-popup-abstract="Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (<span class=&quot;smallcaps-auto&quot;>NLP</span>). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for <span class=&quot;smallcaps-auto&quot;>NLP</span> by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new &quot;Colossal Clean Crawled Corpus&quot;, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for <span class=&quot;smallcaps-auto&quot;>NLP</span>, we release our dataset, pre-trained models, and code." title="'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', Raffel et al 2019">T5</a> uses a denoising objective where a 15%-long chunk is replaced by a missing token &amp; T5 must predict all the missing text based on context; these sorts of objective losses allow learning much more from a given dataset. (Indeed, such models typically outperform <span class="smallcaps-auto">GPT-2</span> on everything <em>but</em> language generation. Oddly, they typically do quite badly at that, which is a major reason everyone uses <span class="smallcaps-auto">GPT-2</span> for generating new texts, and <span class="smallcaps-auto">BERT</span> etc for everything else like generating embeddings or classification.)</p></li>
<li><p>adding <em>global end-to-end</em> losses, which enable training to optimize non-differentiable properties rather than easy (but partially irrelevant ones like predictive losses such as cross-entropy in prediction of the next word). For example, rules defining acceptable meter or rhyme use or penalizing total repetition—these cannot be done via the normal training because no individual discrete word is responsible and parameters cannot be smoothly adjusted to decrease/increase a global property like ‘rhymes’ which is the result of all words considered together as a whole. (This sort of RL loss has been employed in other natural language tasks like machine translation, where metrics like predictive loss do not map onto the desired goal of semantically-correct translation, and word-by-word generation of translations yields similar issues as here, but there are metrics like <span class="smallcaps-auto">BLEU</span> or <span class="smallcaps-auto">ROUGE</span> or grammar checkers which provide a crude measure of global quality. RL approaches have <a href="/Tool-AI" class="docMetadata" data-popup-title="Why Tool AIs Want to Be Agent AIs: The Power of Agency" data-popup-author="Gwern Branwen" data-popup-date="7 Sep 2016" data-popup-abstract="<p>Autonomous AI systems (Agent AIs) trained using <a href=&quot;https://en.wikipedia.org/wiki/reinforcement_learning&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Reinforcement learning&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Reinforcement learning&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;RL&amp;lt;/b&amp;gt;) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: reinforcement learning&quot;>reinforcement learning</a> can do harm when they take wrong actions, especially superintelligent Agent AIs. One solution would be to eliminate their agency by not giving AIs the ability to take actions, confining them to purely informational or inferential tasks such as classification or prediction (Tool AIs), and have all actions be approved &amp;amp; executed by humans, giving equivalently superintelligent results without the risk.</p><p>I argue that this is not an effective solution for two major reasons. First, because Agent AIs will by definition be better at <em>actions</em> than Tool AIs, giving an economic advantage. Secondly, because Agent AIs will be better at <em>inference &amp;amp; learning</em> than Tool AIs, and this is inherently due to their greater agency: the same algorithms which learn how to perform actions can be used to select important datapoints to learn inference over, how long to learn, how to more efficiently execute inference, how to design themselves, how to optimize hyperparameters, how to make use of external resources such as long-term memories or external software or large databases or the Internet, and how best to acquire new data. All of these actions will result in Agent AIs more intelligent than Tool AIs, in addition to their greater economic competitiveness. Thus, Tool AIs will be inferior to Agent AIs in both actions and intelligence, implying use of Tool AIs is a even more highly unstable equilibrium than previously argued, as users of Agent AIs will be able to outcompete them on two dimensions (and not just one).</p>">many virtues</a>.)</p></li>
<li><p>using <em>subjective quality-based losses</em>, like <a href="https://arxiv.org/abs/1706.03741" class="docMetadata" data-popup-title="Deep reinforcement learning from human preferences" data-popup-author="Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei" data-popup-date="2019-08-26" data-popup-abstract="For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback." title="'Deep reinforcement learning from human preferences', Christiano et al 2017">preference learning</a>:</p>
<p>instead of training a NN to predict individual next-characters as accurately as possible or imitate a text corpus as well as possible, we really just want them to predict <em>good</em> next-characters to <em>write</em> text as well as possible—which is not the same thing at all, any more than accurately predicting a human Go player’s next move on average is the same thing as playing Go superhumanly well.</p>
<p>This encourages more global coherency, more thematic progressions, use of rare words when appropriate, surprising subversions or twists which work well when tried but don’t appear in the original corpus, learning esthetics, and so on. If it works and the new <span class="smallcaps-auto">GPT-2</span>-poetry is able to successfully produce new poems which consistently get the top score from the critic and no further improvement is happening, then you simply read a bunch of its new poems, pick which one in each pair you like, retrain the critic on the expanded dataset to detect the remaining flaws in the ones you disliked, and then keep training <span class="smallcaps-auto">GPT-2</span>-poetry to avoid generating the ones you disliked &amp; generate more poems like the ones you liked. Repeat with many cycles, and it should generate excellent poems while avoiding all the flaws of crude likelihood training and even cruder top-<em>k</em> sampling which hobble <span class="smallcaps-auto">GPT-2</span>-poetry right now. Even better, you could create a website to crowdsource the rankings to keep it training 24/7 and improving indefinitely.</p></li>
<li><p>using <em>“expert iteration” architectures</em> like AlphaZero to do much more sophisticated search over possible poems, creating an iterative bootstrap</p></li>
<li><p>adding <em>creativity losses</em> along the lines of <a href="https://arxiv.org/abs/1706.07068" class="docMetadata" data-popup-title="CAN: Creative Adversarial Networks, Generating &quot;Art&quot; by Learning About Styles and Deviating from Style Norms" data-popup-author="Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, Marian Mazzone" data-popup-date="2019-08-26" data-popup-abstract="We propose a new system for generating art. The system generates art by looking at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (<span class=&quot;smallcaps-auto&quot;>GAN</span>), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such networks are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing deviation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs. Human subjects even rated the generated images higher on various scales.">“<span class="smallcaps-auto">CAN</span>: Creative Adversarial Networks, Generating ‘Art’ by Learning About Styles and Deviating from Style Norms”</a>, Elgammal et al 2017, where updating <span class="smallcaps-auto">GAN</span>s encourage diversity</p>
<ul>
<li>one could attempt to invent new styles of poetry by taking inspiration from evolutionary methods, such as the “Population-Based Training” variant employed in <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" title="AlphaStar: Mastering the Real-Time Strategy Game StarCraft II">DeepMind’s AlphaStar League</a> which created diversity by deliberately scrambling the ‘rules’ for each lineage of agents. The “AlphaStar League” used a population of multiple NNs, each forced to specialize in using a particular unit or rewarded for achieving particular goals like defeating a specific NN (rather than winning in general). The AlphaStar League was credited for forcing the overall AlphaStar population to explore strategies reliant on particular kinds of units and figuring out counter-strategies to successful ones. Something similar could be done with poetry rules: train many different agents, each given a specific rhyme scene or meter or vocabulary for their reward function, and in preference-learning approaches, the best poems can be provided to human critics for rating &amp; improving the NN critic. Potentially exciting new combos could emerge as producing the best poems as rated by the humans.</li>
</ul></li>
</ul></li>
</ul>
<p>Given that <span class="smallcaps-auto">GPT-2-117M</span> is far from the state of the art as of February 2019, and hardware &amp; generative NN research is advancing rapidly, it will be exciting to see what sort of poetry can be generated given another 4 years!</p>
</section>
<section id="external-links" class="level1">
<h1><a href="#external-links" title="Link to section: §'External Links'">External Links</a></h1>
<ul>
<li><p>Discussion:</p>
<ul>
<li><a href="https://slatestarcodex.com/2019/03/14/gwerns-ai-generated-poetry/">“Gwern’s AI-Generated Poetry”</a> (<span class="smallcaps-auto">SSC</span>); <a href="/docs/www/old.reddit.com/28c0bf21b30aeded36aca87f7c566e86f2d6b837.html" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/slatestarcodex/comments/b1b47h/gwerns_aigenerated_poetry/" title="(Original URL: https://old.reddit.com/r/slatestarcodex/comments/b1b47h/gwerns_aigenerated_poetry/ )">Reddit</a>/HN: <a href="https://news.ycombinator.com/item?id=19399467">1</a>/<a href="https://news.ycombinator.com/item?id=21456403">2</a>; <a href="https://boingboing.net/2019/03/15/digital-lit.html" title="Some pretty impressive machine-learning generated poetry courtesy of GPT-2">BoingBoing</a>; <a href="https://www.metafilter.com/183993/Hark-from-those-shadowy-depths-thy-voice-Mournfully-echoes-AUTH" title="Hark! from those shadowy depths thy voice / Mournfully echoes, 'AUTH'">MetaFilter</a></li>
<li><a href="/docs/www/sevensecularsermons.org/b246354a9f1e1c49cd931ed39b726e0fab704098.html" rel="archived alternate nofollow" data-url-original="http://sevensecularsermons.org/on-the-significance-of-gwerns-poem-generator/" title="(Original URL: http://sevensecularsermons.org/on-the-significance-of-gwerns-poem-generator/ )">“On the significance of Gwern’s poem generator”</a></li>
<li><a href="https://www.vox.com/2019/5/15/18623134/openai-language-ai-gpt2-poetry-try-it">“A poetry-writing AI has just been unveiled. It’s … pretty good. You can try out OpenAI’s controversial language AI for yourself.”</a></li>
<li><a href="https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/" class="docMetadata" data-popup-title="A Very Unlikely Chess Game" data-popup-author="Scott Alexander" data-popup-date="2020-01-06" data-popup-abstract="<p>Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running <em>within</em> a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.</p><p>…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this <a href=&quot;https://slatestarcodex.com/2019/02/28/meaningful/&quot;>meaningful</a>? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.</p><p>[See also the much later <a href=&quot;https://arxiv.org/abs/2007.03500&quot; &quot;The Go Transformer: Natural Language Modeling for Game Play&quot;>Noever et al 2020a</a>/<a href=&quot;https://arxiv.org/abs/2008.04057&quot; title=&quot;The Chess Transformer: Mastering Play using Generative Language Models&quot;>Noever et al 2020b</a> who do the exact same thing in applying GPT-2 to Go SGF/chess PGN games.]</p>">“A Very Unlikely Chess Game”</a> (on applying <span class="smallcaps-auto">GPT-2-1</span>.5b to <a href="https://en.wikipedia.org/wiki/Portable_Game_Notation" class="docMetadata" data-popup-title="Portable Game Notation" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Portable Game Notation</b> (<span class=&quot;smallcaps-auto&quot;>PGN</span>) is a plain text computer-processible format for recording chess games, supported by many chess programs.</p>" title="Wikipedia: Portable Game Notation"><span class="smallcaps-auto">PGN</span></a> chess games)</li>
</ul></li>
<li><p><a href="https://github.com/summerstay/poem-generator"><code>poem-generator</code></a> (Generates rhyming poetry using Huggingface <span class="smallcaps-auto">GPT-2</span> using rejection sampling—throws away possible completions which don’t rhyme)</p></li>
<li><p><a href="https://familiarcycle.net/2020/how-to-finetune-gpt2-on-podcast-transcripts.html">“How to fine-tune <span class="smallcaps-auto">GPT-2</span> on podcast transcripts”</a>/<a href="https://familiarcycle.net/2020/these-wwdc-boxed-lunches-arent-real.html">“These <span class="smallcaps-auto">WWDC</span> boxed lunches aren’t real”</a>, partialparcel (finetuning <span class="smallcaps-auto">GPT-2-1</span>.5b on Google Colab)</p></li>
<li><p><a href="https://www.poetryfoundation.org/harriet/2013/03/the-average-fourth-grader-is-a-better-poet-than-you-and-me-too">“The Average Fourth Grader Is a Better Poet Than You (and Me Too)”</a></p></li>
<li><p><a href="/docs/ai/poetry/1974-lem-cyberiad-trurlselectronicbard.pdf">“The First Sally (A), or, Trurl’s Electronic Bard”</a>, Stanisław Lem (<em><a href="https://en.wikipedia.org/wiki/The_Cyberiad" class="docMetadata" data-popup-title="The Cyberiad" data-popup-author="English Wikipedia" data-popup-abstract="<p><i><b>The Cyberiad</b></i> is a series of humorous science fiction short stories by Polish writer Stanisław Lem, originally published in 1965, with an English translation appearing in 1974. The main protagonists of the series are Trurl and Klapaucius, the &quot;constructors&quot;.</p>" title="Wikipedia: The Cyberiad">The Cyberiad</a></em>; <a href="https://medium.com/@mwichary/seduced-shaggy-samson-snored-725b5a8086d9" title="Seduced, shaggy Samson snored: The fictional machine that generated poems, and the real people who had to translate them">commentary</a>)</p></li>
<li><p><a href="/docs/borges/1937-borges-raymondllullsthinkingmachine.pdf">“Ramon Llull’s Thinking Machine”</a>, Borges 1937</p></li>
<li><p><a href="https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313">“How to build a State-of-the-Art Conversational AI with Transfer Learning”</a>, Hugging Face</p></li>
<li><p><a href="https://pali6.github.io/computer-generated-foundation/">“Computer Generated Foundation: <span class="smallcaps-auto">SCP</span>s generated by a neural network”</a></p></li>
<li><p><a href="/docs/www/old.reddit.com/21100d1afd52892be3bba8c733087a07d15fd450.html" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/SubSimulatorGPT2/comments/btfhks/what_is_rsubsimulatorgpt2/" title="(Original URL: https://old.reddit.com/r/SubSimulatorGPT2/comments/btfhks/what_is_rsubsimulatorgpt2/ )">“What is /r/SubSimulator<span class="smallcaps-auto">GPT</span>2?”</a></p></li>
<li><p><a href="/docs/www/codyraskin.com/e4de417900a2960a4b6ba29e8a2d1374101f66ac.html" rel="archived alternate nofollow" data-url-original="http://codyraskin.com/research/?p=135" title="(Original URL: http://codyraskin.com/research/?p=135 )">“A Chinese Room Writes a Sequel to <em>Blindsight</em>”</a></p></li>
<li><p><a href="/docs/www/minimaxir.com/67a0008151cc412cbb51ae4bdd8203a3ef61fb58.html" class="docMetadata" data-popup-title="How To Make Custom AI-Generated Text With GPT-2" data-popup-author="Max Woolf" data-popup-date="2019-09-04" data-popup-abstract="From a text-generation perspective, the included demos were very impressive: the text is coherent over a long horizon, and grammatical syntax and punctuation are near-perfect.</p><p><img src=&quot;/images/ai/2019-openai-gpt2-demo-recyclingtextsample.png&quot;></p><p>At the same time, the Python code which allowed anyone to download the model (albeit smaller versions out of concern the full model can be abused to mass-generate fake news) and the <a href=https://www.tensorflow.org>TensorFlow</a> code to load the downloaded model and generate predictions was <a href=https://github.com/openai/gpt-2 >open-sourced on GitHub</a>.</p><p>Neil Sheppard created <a href=https://github.com/nshepperd/gpt-2 >a fork</a> of OpenAI’s repo which contains additional code to allow <em>finetuning</em> the existing OpenAI model on custom datasets. A <a href=https://github.com/ak9250/gpt-2-colab >notebook</a> was created soon after, which can be copied into <a href=https://colab.research.google.com >Google Colaboratory</a> and clones Sheppard's repo to finetune <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 backed by a free <span class=&quot;smallcaps-auto&quot;>GPU</span>. From there, the proliferation of <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 generated text took off: researchers such as Gwern Branwen made <a href=https://www.gwern.net/GPT-2 ><span class=&quot;smallcaps-auto&quot;>GPT</span>-2 Poetry</a> and Janelle Shane made <a href=https://aiweirdness.com/post/183471928977/dd-character-bios-now-making-slightly-more ><span class=&quot;smallcaps-auto&quot;>GPT</span>-2 Dungeons and Dragons character bios</a>.</p><p>I waited to see if anyone would make a tool to help streamline this finetuning and text generation workflow, a la <a href=https://github.com/minimaxir/textgenrnn >textgenrnn</a> which I had made for recurrent neural network-based text generation. Months later, no one did. So I did it myself. Enter <a href=https://github.com/minimaxir/gpt-2-simple >gpt-2-simple</a>, a Python package which wraps Sheppard's finetuning code in a functional interface and adds <em>many</em> utilities for model management and generation control.</p><p><p>Thanks to gpt-2-simple and <a href=https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce >this Colaboratory Notebook</a>, you can easily finetune <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 on your own dataset with a simple function, and generate text to your own specifications!</p>" rel="archived alternate nofollow" data-url-original="https://minimaxir.com/2019/09/howto-gpt2/" title="(Original URL: https://minimaxir.com/2019/09/howto-gpt2/ )">“How To Make Custom AI-Generated Text With <span class="smallcaps-auto">GPT-2</span>”</a>; <a href="https://github.com/minimaxir/gpt-2-keyword-generation"><code>gpt-2-keyword-generation</code></a></p></li>
<li><p><a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">“Evaluation Metrics for Language Modeling”</a>, Chip Huyen</p></li>
<li><p><a href="/docs/www/senrigan.io/9e9089e7751a0e48f759f78c04ace5ec303c9620.html" rel="archived alternate nofollow" data-url-original="https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/" title="(Original URL: https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/ )">“Lessons Learned from Building an AI Writing App, <code>writeup.ai</code> [Guide]”</a>, Jeffrey Shek</p></li>
<li><p><a href="/docs/www/mikelynch.org/0c9390fcb10c768f51e906bb6911fbda336a6b98.html" class="docMetadata" data-popup-title="Excavate" data-popup-author="Mike Lynch" data-popup-date="2019-11-22" data-popup-abstract="<p>After skipping it last year (I did NaNoWriMo instead) I decided that I missed doing National Novel Generating Month and thought I’d do something relatively simple, based on Tom Phillips’ <em>A Humument</em>, which I recently read for the first time. Phillips’ project was created by drawing over the pages of the forgotten Victorian novel <em>A Human Document</em>, leaving behind a handful of words on each page which form their own narrative, revealing a latent story in the original text. I wanted to simulate this process by taking a neural net trained on one text and use it to excavate a slice from a second text which would somehow preserve the style of the <span class=&quot;smallcaps-auto&quot;>RNN</span>. To get to the target length of 50,000 words, the second text would have to be very long, so I picked Robert Burton’s <em>The Anatomy of Melancholy</em>, which is over half a million words, and one of my favourite books.</p><p>The next step was to use this to implement the excavate algorithm, which works like this:</p><ol><li>read a vocab from the next L words from the primary text (Burton) where L is the lookahead parameter</li><li>take the first letter of every word in the vocab and turn it into a constraint</li><li>run the <span class=&quot;smallcaps-auto&quot;>RNN</span> with that constraint to get the next character C</li><li>prune the vocab to those words with the first letter C, with that letter removed</li><li>turn the new vocab into a new constraint and go back to 3</li><li>once we’ve finished a word, add it to the results</li><li>skip ahead to the word we picked, and read more words from the text until we have L words</li><li>go back to 2 unless we’ve run out of original text, or reached the target word count</li></ol><p>Here’s an example of how the <span class=&quot;smallcaps-auto&quot;>RNN</span> generates a single word with L set to 100:</p><p>Vocab 1: “prime cause of my disease. Or as he did, of whom Felix Plater speaks, that thought he had some of Aristophanes’ frogs in his belly, still crying Breec, okex, coax, coax, oop, oop, and for that cause studied physic seven years, and travelled over most part of Europe to ease himself. To do myself good I turned over such physicians as our libraries would afford, or my private friends impart, and have taken this pains. And why not? Cardan professeth he wrote his book, De Consolatione after his son’s death, to comfort himself; so did Tully”</p><p><span class=&quot;smallcaps-auto&quot;>RNN</span>: <strong>s</strong></p><p>Vocab 2: “peaks ome till tudied even uch on’s o”</p><p><span class=&quot;smallcaps-auto&quot;>RNN</span>: <strong>t</strong></p><p>Vocab 3: “ill udied”</p><p><span class=&quot;smallcaps-auto&quot;>RNN</span>: <strong>u</strong></p><p>Final result: <strong>studied</strong></p><p>The algorithm then restarts with a new 100-word vocabulary starting at “physic seven years”</p><p>It works pretty well with a high enough lookahead value, although I’m not happy with how the algorithm decides when to end a word. The weight table always gets a list of all the punctuation symbols and a space, which means that the <span class=&quot;smallcaps-auto&quot;>RNN</span> can always bail out of a word half-way if it decides to. I tried constraining it so that it always finished a word once it had narrowed down the options to a single-word vocab, but when I did this, it somehow removed the patterns of punctuation and line-breaks—for example, the way the Three Musketeers <span class=&quot;smallcaps-auto&quot;>RNN</span> emits dialogue in quotation marks—and this was a quality of the <span class=&quot;smallcaps-auto&quot;>RNN</span> I wanted to preserve. I think a little more work could improve this.</p><p>…This kind of hybridisation can be applied to any <span class=&quot;smallcaps-auto&quot;>RNN</span> and base text, so there’s a lot of scope for exploration here, of grafting the grammar and style of one text onto the words from another. And the alliteration and lipogram experiments above are just two simple examples of more general ways in which I’ll be able to tamper with the output of <span class=&quot;smallcaps-auto&quot;>RNN</span>s.</p>" rel="archived alternate nofollow" data-url-original="https://mikelynch.org/2019/Nov/22/excavate/" title="(Original URL: https://mikelynch.org/2019/Nov/22/excavate/ )">“Excavate”</a>, Mike Lynch (search over a corpus to extract a <span class="smallcaps-auto">RNN</span>-generated ‘hidden text’)</p></li>
<li><p><a href="https://arxiv.org/abs/2002.02511" class="docMetadata" data-popup-title="Introducing Aspects of Creativity in Automatic Poetry Generation" data-popup-author="Brendan Bena, Jugal Kalita" data-popup-date="2020-02-10" data-popup-abstract="Poetry Generation involves teaching systems to automatically generate text that resembles poetic work. A deep learning system can learn to generate poetry on its own by training on a corpus of poems and modeling the particular style of language. In this paper, we propose taking an approach that fine-tunes <span class=&quot;smallcaps-auto&quot;>GPT</span>-2, a pre-trained language model, to our downstream task of poetry generation. We extend prior work on poetry generation by introducing creative elements. Specifically, we generate poems that express emotion and elicit the same in readers, and poems that use the language of dreams---called dream poetry. We are able to produce poems that correctly elicit the emotions of sadness and joy 87.5 and 85 percent, respectively, of the time. We produce dreamlike poetry by training on a corpus of texts that describe dreams. Poems from this model are shown to capture elements of dream poetry with scores of no less than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for all our poems. We also make use of the Coh-Metrix tool, outlining metrics we use to gauge the quality of text generated.">“Introducing Aspects of Creativity in Automatic Poetry Generation”</a>, Bena &amp; Kalita 2020</p></li>
<li><p><a href="/docs/www/pdfs.semanticscholar.org/05e80dd8708dc6504a6ac637af9ec74a973bb793.pdf" class="docMetadata" data-popup-title="Smart Vet: Autocompleting Sentences in Veterinary Medical Records" data-popup-author="Samuel Ginn" data-popup-date="2019-03-19" data-popup-abstract="<p>Every day, veterinarians write tens of thousands of medical records, mostly in standard formats following the <span class=&quot;smallcaps-auto&quot;>SOAP</span> structure: “Subjective”, “Objective”, “Assessment”, and “Plan”. These notes record the findings of their physical exams and observations of their patients, and take countless hours to write. We present in this paper a new system that we call “Smart Vet” that assists veterinarians in the writing of their notes by suggesting autocompletions for their sentences as they are writing them within the sections of their medical records. To enable this, we present two approaches: an end-to-end deep learning system that models this task as a seq2seq neural machine translation problem (i.e. translate a given sequence of sentences that correspond to the existing medical record into the following sequence that corresponds to the next sentence the veterinarian would want to write)and a transformer-based language modeling system based on OpenAI’s recent advancements. Based on the success of this latter method, we evaluate this system live in a medical records application, and successfully see our autocompletions being used in production 12.46% of the time—a remarkable success.</p>" rel="archived alternate nofollow" data-url-original="https://pdfs.semanticscholar.org/37ee/f9fdeeeb642dc135c22c887a16ff62dbe62a.pdf" title="(Original URL: https://pdfs.semanticscholar.org/37ee/f9fdeeeb642dc135c22c887a16ff62dbe62a.pdf )">“Smart Vet: Autocompleting Sentences in Veterinary Medical Records”</a>, Ginn 2019</p></li>
<li><p><a href="/docs/www/techscience.org/c4d3dbcd52a2ef4d725344e30d1b82289218dc92.html" class="docMetadata" data-popup-title="Deepfake Bot Submissions to Federal Public Comment Websites Cannot Be Distinguished from Human Submissions" data-popup-author="Max Weiss" data-popup-date="2019-12-18" data-popup-abstract="<p>Publicly available artificial intelligence methods can generate an enormous volume of original, human speech-like topical text “Deepfake Text”) that is not based on conventional search-and-replace patterns. I created a computer program (a bot) that generated and submitted 1,001 deepfake comments regarding a Medicaid reform waiver to a federal public comment website, stopping submission when these comments comprised more than half of all submitted comments. I then formally withdrew the bot comments. When humans were asked to classify a subset of the deepfake comments as human or bot submissions, the results were no better than would have been gotten by random guessing. Federal public comment websites currently are unable to detect Deepfake Text once submitted, but technological reforms (e.g., <span class=&quot;smallcaps-auto&quot;>CAPTCHA</span>s) can be implemented to help prevent massive numbers of submissions by bots.</p>" rel="archived alternate nofollow" data-url-original="https://techscience.org/a/2019121801/" title="(Original URL: https://techscience.org/a/2019121801/ )">“Deepfake Bot Submissions to Federal Public Comment Websites Cannot Be Distinguished from Human Submissions”</a>, Weis 2019</p></li>
<li><p><a href="https://github.com/turtlesoupy/this-word-does-not-exist">This Word Does Not Exist</a></p></li>
<li><p><a href="https://towardsdatascience.com/how-to-fine-tune-gpt-2-so-you-can-generate-long-form-creative-writing-7a5ae1314a61?gi=db2b8dd9609f">“How To Fine-Tune <span class="smallcaps-auto">GPT-2</span> So You Can Generate Long-Form Creative Writing”</a>, Jason Boog</p></li>
<li><p><a href="https://spectrum.ieee.org/artificial-intelligence/machine-learning/this-ai-poet-mastered-rhythm-rhyme-and-natural-language-to-write-like-shakespeare">“This AI Poet Mastered Rhythm, Rhyme, and Natural Language to Write Like Shakespeare: ‘Deep-speare’ crafted Shakespearean verse that most readers couldn’t distinguish from human-written poems”</a> (<a href="https://arxiv.org/abs/1807.03491" class="docMetadata" data-popup-title="Deep-speare: A Joint Neural Model of Poetic Language, Meter and Rhyme" data-popup-author="Jey Han Lau, Trevor Cohn, Timothy Baldwin, Julian Brooke, Adam Hammond" data-popup-date="2020-07-07" data-popup-abstract="In this paper, we propose a joint architecture that captures language, rhyme and meter for sonnet modelling. We assess the quality of generated poems using crowd and expert judgements. The stress and rhyme models perform very well, as generated poems are largely indistinguishable from human-written poems. Expert evaluation, however, reveals that a vanilla language model captures meter implicitly, and that machine-generated poems still underperform in terms of readability and emotion. Our research shows the importance expert evaluation for poetry generation, and that future research should look beyond rhyme/meter and focus on poetic language." title="Deep-speare: A Joint Neural Model of Poetic Language, Meter and Rhyme">Lau et al 2020</a>)</p></li>
<li><p><a href="https://arxiv.org/abs/2006.15720" class="docMetadata" data-popup-title="Progressive Generation of Long Text" data-popup-author="Bowen Tan, Zichao Yang, Maruan AI-Shedivat, Eric P. Xing, Zhiting Hu" data-popup-date="2020-07-03" data-popup-abstract="Large-scale language models pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text ($>$1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. To overcome the limitation, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained language models at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned GPT-2 in terms of domain-specific quality and sample efficiency. The coarse-to-fine nature of progressive generation also allows for a higher degree of control over the generated content.">“Progressive Generation of Long Text”</a>, Tan et al 2020</p></li>
<li><p><a href="https://adapterhub.ml">"AdapterHub:</a> <a href="https://arxiv.org/abs/2007.07779" class="docMetadata" data-popup-title="AdapterHub: A Framework for Adapting Transformers" data-popup-author="Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych" data-popup-date="2020-07-16" data-popup-abstract="The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic &quot;stitching-in&quot; of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at https://AdapterHub.ml.">A Framework for Adapting Transformers"</a>, Pfeiffer et al 2020</p></li>
</ul>
</section>
<section id="appendix" class="level1">
<h1><a href="#appendix" title="Link to section: §'Appendix'">Appendix</a></h1>
<section id="archive-of-our-own-ao3-gpt-2-1-5b" class="level2">
<h2><a href="#archive-of-our-own-ao3-gpt-2-1-5b" title="Link to section: §'Archive of Our Own (Ao3) GPT-2-1.5b'">Archive of Our Own (Ao3) <span class="smallcaps-auto">GPT-2-1</span>.5b</a></h2>
<p>Aaron Gokaslan scraped the large fanfiction website <a href="https://en.wikipedia.org/wiki/Archive_of_Our_Own" class="docMetadata" data-popup-title="Archive of Our Own" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Archive of Our Own</b> (<b>AO3</b>) is a nonprofit open source repository for fanfiction (fic) and other fanworks contributed by users. The site was created in 2008 by the Organization for Transformative Works and went into open beta in 2009. As of 2019, Archive of Our Own hosted 5.5 million works in over 35,000 fandoms. The site has received positive reception for its curation, organization and design, mostly done by readers and writers of fanfiction.</p>" title="Wikipedia: Archive of Our Own">Archive of Our Own</a> (Ao3) and <a href="https://drive.google.com/file/d/1PPI6KlMkiASMXTBFecP6-Xf76fBBaMKg/view">created a text dump</a> (12GB raw; 190,931 stories; 2.06b words; no metadata).<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> We trained a <span class="smallcaps-auto">GPT-2-1</span>.5b on it, under the theory that it might be useful as a basis for text-game-like applications such as <a href="/docs/www/aidungeon.io/9b07b6c2b8b0256022844c455fac29c07680e179.html" class="docMetadata" data-popup-title="AI Dungeon 2" data-popup-author="Nick Walton" data-popup-date="2019-12" data-popup-abstract="[AI Dungeon 2 is a project which trains <span class=\&quot;smallcaps-auto\&quot;>GPT</span>-2-1.5b on logs from text adventure games; when used interactively by a human, it &quot;plays <span class=\&quot;smallcaps-auto\&quot;>RPG</span> games&quot; with you, but because it is powered by <span class=\&quot;smallcaps-auto\&quot;>GPT</span>-2-1.5b, it is immensely flexible and can cope (to some degree) with almost any input, producing bizarre, hilarious, or surprisingly logical sequences of adventures. It became popular overnight, crushing Walton with bandwidth bills, and has been turned into an app and community to support distribution and development. See also https://colab.research.google.com/github/nickwalton/AIDungeon/blob/master/AIDungeon_2.ipynb and https://old.reddit.com/r/AIDungeon/ and http://www.aidungeon.io/2019/11/my-orc-band-and-our-quest-for-equal.html .]" rel="archived alternate nofollow" data-url-original="https://aidungeon.io/" title="(Original URL: https://aidungeon.io/ )">AI Dungeon 2</a>, under the idea that since AI Dungeon 2 is essentially collaborative story-telling, starting with a story-based model ought to give better results. Whether that is true remains to be seen, but <a href="/docs/ai/2020-02-03-gpt21.5b-archiveofourownao3-model-510427-samples-topp090.txt">we generated Ao3 text samples</a> and the dataset &amp; model can be downloaded:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">rsync</span> rsync://78.46.86.149:873/biggan/2020-01-14-gpt2-1558m-archiveofourownao3.tar.xz ./ <span class="co"># 11GB</span>
<span class="fu">rsync</span> rsync://78.46.86.149:873/biggan/2019-12-18-skylion-archiveofourown-fanfics-textscrape.tar.xz ./ <span class="co"># 2.8GB</span></code></pre></div>
</section>
<section id="video-game-walkthrough-gpt-2-1-5b" class="level2">
<h2><a href="#video-game-walkthrough-gpt-2-1-5b" title="Link to section: §'Video Game Walkthrough GPT-2-1.5b'">Video Game Walkthrough <span class="smallcaps-auto">GPT-2-1</span>.5b</a></h2>
<p>Twitter user <span class="citation" data-cites="me_irl">[@me_irl]</span>(https://twitter.com/me_irl/status/1217818112957014022) provided a 50MB scrape of video game walkthroughs, which he’d used previously with <span class="smallcaps-auto">GPT-2-345M</span> and requested we do finetuning on that as well: <a href="/docs/ai/2020-02-03-gpt21.5b-videogamewalkthrough-model-174925-samples-topp090.txt">video game walkthrough text samples</a> (<a href="/docs/www/www.newsweek.com/c55280d46ad67bbcdd0b2648c3511b24378ae21d.html" class="docMetadata" data-popup-title="OpenAI Text Generator GPT-2 Creates Video Game Walkthrough for 'Most Tedious Game in History'" data-popup-author="Andrew Whalen (Newsweek)" data-popup-date="2020-02-20" data-popup-abstract="<p>When OpenAI announced the automatic text generator <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 in February of 2019, its language model had a simple objective: predict the next word. Since its release—and despite high computational barriers—programmers, tinkerers and artificial intelligence researchers have explored creative ways to use the advanced language model, developing applications for <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 far beyond simple text generation. In January, AI researcher Shawn Presser demonstrated how <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 can empower video game design, beginning with “the most tedious game in history.” “You can prompt the model with whatever text you want, and it will try to guess how to complete it,” Presser told Newsweek.</p><p>…Using thousands of game walkthroughs and <span class=&quot;smallcaps-auto&quot;>FAQ</span>s, scraped from sites around the web (a 50 megabyte data set provided by Twitter’s <span class=&quot;citation&quot; data-cites=&quot;me_irl&quot;>@me_irl</span>), Presser prompted <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 to generate its own walkthroughs. The result is walkthroughs of video games that never existed; guides to adventures no one has ever programmed. Presser described one of <span class=&quot;smallcaps-auto&quot;>GPT</span>-2’s creations as “a walkthrough for the most tedious game in history”: a dense set of instructions for something that sounds a lot like a first-person shooter. “When the room opens, go forward. You should find a rocket launcher,” the walkthrough begins. “Push the switch and a door opens. Take cover in the corner and shoot the guard. The door will close when he dies. Now jump over the gap and kill the guards. In the next area is a switch. Push it and the door will open. In the next area is a scientist. Kill him. Go back to the previous room and push the switch. Open the next door. In the next room is a scientist. Kill him.”</p><p>…But renting a “<span class=&quot;smallcaps-auto&quot;>TPU</span> pod” for cloud computing can cost millions, making them prohibitively expensive for all but large companies—organizations unlikely to try out playful experiments. So Presser developed a technique he dubbed “swarm training,” to employ 80 individual <span class=&quot;smallcaps-auto&quot;>TPU</span>s on a single data set. “In swarm training, we can run dozens or hundreds of <span class=&quot;smallcaps-auto&quot;>TPU</span>s in a loose network which swaps updates on the fly,” Presser told Newsweek. “It’s chaotic, but it winds up working pretty well: it’s much faster than using just a few <span class=&quot;smallcaps-auto&quot;>TPU</span>s, but much cheaper than renting entire <span class=&quot;smallcaps-auto&quot;>TPU</span> pods. We’re hopeful that swarm training will be very useful to other researchers.”</p><p>…<span class=&quot;smallcaps-auto&quot;>GPT</span>-2 has also proved adept at gaming functions beyond just generating games-related text. Presser previously collaborated with technology writer and researcher Gwern Branwen to train <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 to play chess, by providing it hours of “training” in legal chess moves (using standard notation) and asking it to output its own responses. After hours of training <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 on which responses are valid moves in an ongoing chess game and which responses are nonsensical, the text generation engine was eventually able to complete a full game.</p><p>While it may be years before game designers are employing text generating language models in their designs, Presser said he already sees potential practical applications. “If you prompt the model with descriptions of some spells from your tabletop campaign, the model can generate new spells,” Presser said. “It’s quite versatile.” For example, <em>Dungeons &amp;amp; Dragons</em> players could input spells like Fireball, including a description of its HP damage, and get back from <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 new attack spells to use in tabletop roleplaying sessions. “I think there’s an opportunity to build new indie games using <span class=&quot;smallcaps-auto&quot;>GPT</span>-2,” Presser said. “Imagine making a mod for <em>Skyrim</em> that uses <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 to generate new quests. You’d have infinite replayability. It’d be like <em>AI Dungeon 2</em> in 3D.”</p>" rel="archived alternate nofollow" data-url-original="https://www.newsweek.com/openai-text-generator-gpt-2-video-game-walkthrough-most-tedious-1488334" title="(Original URL: https://www.newsweek.com/openai-text-generator-gpt-2-video-game-walkthrough-most-tedious-1488334 )"><em>Newsweek</em> article</a>). <code>me_irl</code> has suggested that they could be used as hypothetical game designs for competitions or art purposes. The combined dataset/model can be downloaded:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">rsync</span> rsync://78.46.86.149:873/biggan/2020-01-16-gpt-2-1558m-shawnpresser-videogamewalkthrough.tar.xz ./</code></pre></div>
</section>
<section id="rdota2" class="level2">
<h2><a href="#rdota2" title="Link to section: §'/r/DoTA2'">/r/DoTA2</a></h2>
<p>In <a href="https://twitter.com/theshawwn/status/1212156603140648961">December 2019</a>, Shawn Presser trained a <span class="smallcaps-auto">GPT-2-117M</span> model for a few million steps on the /r/DoTA2 subreddit, as part of the Subreddit Simulator training project. (The final <span class="smallcaps-auto">GPT-2-1</span>.5b &amp; dataset for Subreddit Simulator has not been released at the project’s request.)</p>
<p>While the final model checkpoint appears to have been lost (oops), step #562,971 from 18 December 2019 has been uploaded:</p>
<ol type="1">
<li><code>rsync -v rsync://78.46.86.149:873/biggan/2019-12-18-shawnpresser-gpt-2-117m-rdota2.tar.xz</code></li>
<li><a href="https://mega.nz/file/WPQVFAJQ#mj2bP7Eba00aAaTGm_qqHW4JScGo5sC-F00pJJXe6Zg">Mega</a> (433MB)</li>
</ol>
</section>
<section id="bradley-terry-preference-learning" class="level2">
<h2><a href="#bradley-terry-preference-learning" title="Link to section: §'Bradley-Terry Preference Learning'">Bradley-Terry Preference Learning</a></h2>
<blockquote>
<p><a href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/" class="docMetadata" data-popup-title="Learning from Human Preferences" data-popup-author="Dario Amodei, Paul Christiano, Alex Ray (OpenAI)" data-popup-date="2017-06-13" data-popup-abstract="<p>One step towards building safe AI systems is to remove the need for humans to write goal functions, since using a simple proxy for a complex goal, or getting the complex goal a bit wrong, can lead to undesirable and even dangerous behavior. In collaboration with DeepMind’s safety team, we’ve developed an algorithm which can infer what humans want by being told which of two proposed behaviors is better.</p><p>We present a learning algorithm that uses small amounts of human feedback to solve modern RL environments. Machine learning systems with human feedback have been explored before, but we’ve scaled up the approach to be able to work on much more complicated tasks. Our algorithm needed 900 bits of feedback from a human evaluator to learn to backflip—a seemingly simple task which is simple to judge but challenging to specify.</p><p>The overall training process is a 3-step feedback cycle between the human, the agent’s understanding of the goal, and the RL training.</p><figure><img src=&quot;/images/rl/2017-amodei-openai-learningfromhumanpreferences-architecture2x-2x.png&quot; alt=&quot;Preference learning architecture (https://openai.com/content/images/2017/06/diagram@2x-2.png)&quot; /><figcaption>Preference learning architecture</figcaption></figure><p>Our AI agent starts by acting randomly in the environment. Periodically, two video clips of its behavior are given to a human, and the human decides which of the two clips is closest to fulfilling its goal—in this case, a backflip. The AI gradually builds a model of the goal of the task by finding the reward function that best explains the human’s judgments. It then uses RL to learn how to achieve that goal. As its behavior improves, it continues to ask for human feedback on trajectory pairs where it’s most uncertain about which is better, and further refines its understanding of the goal.</p>">Christiano et al 2017</a> introduced a deep reinforcement learning architecture for learning “I know it when I see it” subjectively-defined reward functions from human feedback: a human makes comparisons of actions/datapoints/episodes to select the ‘better’ one, a NN is trained to predict the better one based on these comparisons, and another NN is RL-trained based on the predicted comparisons interpreted as a reward. Since the human is unable to write down a conventional reward function in software, the predictor NN (analogous to a Discriminator in a <span class="smallcaps-auto">GAN</span> or a ‘critic’ in actor-critic RL) learns the reward function by example, and then the RL agent NN (analogous to a Generator in a <span class="smallcaps-auto">GAN</span>) learns by trial-and-error what sequences will optimize this complex reward function, and the human feedback provides additional guidance on new parts of the problem as the pair of NNs bootstrap into better performance. This is demonstrated on video game or robotic-style simulations, but appears equally applicable to other sequence problems where reward functions are impossible to write and existing losses like maximum likelihood are imperfect for generation (such as music or poetry composition).</p>
<p>As originally framed, the predictor merely does comparisons, receiving &amp; providing binary feedback. This is justified as being implicitly equivalent to a standard pair-comparison/competition model, the <a href="https://en.wikipedia.org/wiki/Bradley-Terry_model" class="docMetadata" data-popup-title="Bradley–Terry model" data-popup-author="English Wikipedia" data-popup-abstract="<p>The <b>Bradley–Terry model</b> is a probability model that can predict the outcome of a paired comparison. Given a pair of individuals <span class=&quot;texhtml mvar&quot; style=&quot;font-style:italic&quot;>i</span> and <span class=&quot;texhtml mvar&quot; style=&quot;font-style:italic&quot;>j</span> drawn from some population, it estimates the probability that the pairwise comparison <span class=&quot;texhtml &quot;><i>i</i> &amp;gt; <i>j</i></span> turns out true, as</p><dl><dd><span class=&quot;mwe-math-element&quot;><img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/b702a5b9217ab1a685e9c16743ee8ef98ccf20b5&quot; class=&quot;mwe-math-fallback-image-inline&quot; aria-hidden=&quot;true&quot; style=&quot;vertical-align:-2.505ex;width:19.237ex;height:5.509ex&quot; /></span>
</dd></dl>" title="Wikipedia: Bradley-Terry model">Bradley-Terry model</a> (akin to the famous <span class="smallcaps-auto">ELO</span>), where each datapoint has a latent variable on a common cardinal scale (often, like a <a href="https://en.wikipedia.org/wiki/liability-threshold_model" class="docMetadata" data-popup-title="Threshold model" data-popup-author="English Wikipedia" data-popup-abstract="<p>In mathematical or statistical modeling a <b>threshold model</b> is any model where a threshold value, or set of threshold values, is used to distinguish ranges of values where the behaviour predicted by the model varies in some important way. A particularly important instance arises in toxicology, where the model for the effect of a drug may be that there is zero effect for a dose below a critical or threshold value, while an effect of some significance exists above that value. Certain types of regression model may include threshold effects.</p>" title="Wikipedia: liability-threshold model">liability-threshold model</a>, scaled to <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Equation"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-semantics"><span class="mjx-mrow"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-script-R" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.306em;">N</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></span> for convenience), producing a total order which efficiently extracts all possible information from the comparisons.</p>
<p>I suggest that this is not necessarily the case, as examples from <span class="smallcaps-auto">GAN</span>s indicate that such a preference-learning architecture may be learning something odder (such as memorizing comparisons), and that the architecture could be improved by removing the implicitness of the B-T ranking, computing the B-T rankings directly (which can be done even with non-overlapping comparisons by using a Bayesian model with priors and using covariates such as the predictor’s own estimates), thereby providing absolute quality scores for correctness of comparisons, more efficient regression, RL rewards, and meaningful interpretable scores for downstream uses.</p>
</blockquote>
<p><strong>Moved to the <a href="/GPT-2-preference-learning#bradley-terry-preference-learning" title="GPT-2 Preference Learning for Music and Poetry Generation"><span class="smallcaps-auto">GPT-2</span> preference learning page</a>.</strong></p>
</section>
<section id="efficient-attention" class="level2">
<h2><a href="#efficient-attention" title="Link to section: §'Efficient Attention'">Efficient Attention</a></h2>
<p>One of the most frustrating limitations of <a href="https://arxiv.org/abs/2005.14165#openai" class="docMetadata" data-popup-title="Language Models are Few-Shot Learners" data-popup-author="Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al" data-popup-date="2020-05-28" data-popup-abstract="Recent work has demonstrated substantial gains on many <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current <span class=&quot;smallcaps-auto&quot;>NLP</span> systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train <span class=&quot;smallcaps-auto&quot;>GPT</span>-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 achieves strong performance on many <span class=&quot;smallcaps-auto&quot;>NLP</span> datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where <span class=&quot;smallcaps-auto&quot;>GPT</span>-3's few-shot learning still struggles, as well as some datasets where <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 faces methodological issues related to training on large web corpora. Finally, we find that <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 in general." title="'GPT-3: Language Models are Few-Shot Learners', Brown et al 2020"><span class="smallcaps-auto">GPT-3</span></a> is the context window: 2048 <span class="smallcaps-auto">BPE</span>s runs out fast when you start prompt programming something hard, and hacks like <a href="/GPT-3#bpes" class="docMetadata" data-popup-title="GPT-3 Weaknesses: Byte-Pair Encodings (BPEs)" data-popup-author="Gwern Branwen" data-popup-date="2020-06-23" data-popup-abstract="<p>Compared to <span class=&quot;smallcaps-auto&quot;>GPT</span>-2, <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 improves performance on character-level tasks like rhyming, alliteration, punning, anagrams/permutations, acrostic poems, and arithmetic less than expected, despite being very good at many other closely-related kinds of writings like satire.</p><p>Why? A plausible explanation is an obscure technical detail: as a performance optimization, <span class=&quot;smallcaps-auto&quot;>GPT</span> does not see characters but <em>sub-word-chunks</em> called “byte-pair encodings” (<span class=&quot;smallcaps-auto&quot;>BPE</span>s). Because <span class=&quot;smallcaps-auto&quot;>GPT</span>s never see characters but opaque partial-words, which vary chaotically based on the specific word and even the surrounding context, they are unable to easily learn about character-level aspects of language, like similar spellings or sounds, and are forced to learn relationships much more indirectly, like by brute-force memorizing of pairs of words.</p><p>Some experiments with reformatting <span class=&quot;smallcaps-auto&quot;>GPT</span>-3’s poorest-performing tasks to avoid inconsistent <span class=&quot;smallcaps-auto&quot;>BPE</span> encodings of strings shows small to large performance gains, consistent with this theory.</p>"><span class="smallcaps-auto">BPE</span>s</a> have nasty &amp; subtle side-effects. How do we get future Transformers with reasonable context windows and/or memory, which we can use for research papers, books, structured text, images, video, genomics, and so on, where we need to handle sequences with lengths in the millions? (Such improvements would permit not just doing things <span class="smallcaps-auto">GPT-3</span> struggles to do, like write coherent novels, but many better architectures, like multimodal Transformers which can learn jointly from images &amp; text, accessing image-based datasets like <span class="smallcaps-auto">PDF</span>s, and learning far more accurate human-like representations &amp; tacit knowledge with less data &amp; smaller models, providing large models useful for almost all conceivable tasks—especially robotics.)</p>
<p>Below I compile &amp; categorize research on breaking the dense attention quadratic bottleneck (overviews: <a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html" title="The Transformer Family: Attention and Self-Attention · Multi-Head Self-Attention · Transformer · Adaptive Computation Time (ACT) · Improved Attention Span: (Longer Attention Span (Transformer-XL) / Adaptive Attention Span / Localized Attention Span (Image Transformer)) · Less Time and Memory Cost: (Sparse Attention Matrix Factorization (Sparse Transformers) / Locality-Sensitive Hashing (Reformer)) · Make it Recurrent (Universal Transformer) · Stabilization for RL (GTrXL)">Lilian Weng</a>, <a href="https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/" title="A Survey of Long-Term Context in Transformers: Sparse Transformers · Adaptive Span Transformers · Transformer-XL · Compressive Transformers · Reformer · Routing Transformer · Sinkhorn Transformer · Linformer · Efficient Attention: Attention with Linear Complexities · Transformers are RNNs · ETC · Longformer">Madison May</a>):</p>
<ul>
<li><p><span class="smallcaps">State</span>:</p>
<ul>
<li><p><em>recurrency</em>:</p>
<ul>
<li><a href="https://arxiv.org/abs/1807.03819#googledeepmind" class="docMetadata" data-popup-title="Universal Transformers" data-popup-author="Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser" data-popup-date="2020-08-12" data-popup-abstract="Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.">“Universal Transformers”</a>, Dehghani et al 2018 (?)</li>
<li><a href="https://arxiv.org/abs/1901.02860" class="docMetadata" data-popup-title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context" data-popup-author="Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov" data-popup-date="2019-08-26" data-popup-abstract="Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than <span class=&quot;smallcaps-auto&quot;>RNN</span>s and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.">“Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context”</a>, Dai et al 2019 (<a href="/docs/www/www.lyrn.ai/fc44a554e972b519fd603b3a222e5d3e3f6b6030.html" rel="archived alternate nofollow" data-url-original="https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/" title="'Transformer-XL---Combining Transformers and RNNs Into a State-of-the-art Language Model', Rani Horev 2019 (Original URL: https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/ )">blog</a>)</li>
<li><a href="https://arxiv.org/abs/1906.08237" class="docMetadata" data-popup-title="XLNet: Generalized Autoregressive Pretraining for Language Understanding" data-popup-author="Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le" data-popup-date="2019-08-26" data-popup-abstract="With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like <span class=&quot;smallcaps-auto&quot;>BERT</span> achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, <span class=&quot;smallcaps-auto&quot;>BERT</span> neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose <span class=&quot;smallcaps-auto&quot;>XLN</span>et, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of <span class=&quot;smallcaps-auto&quot;>BERT</span> thanks to its autoregressive formulation. Furthermore, <span class=&quot;smallcaps-auto&quot;>XLN</span>et integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, <span class=&quot;smallcaps-auto&quot;>XLN</span>et outperforms <span class=&quot;smallcaps-auto&quot;>BERT</span> on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.">“<span class="smallcaps-auto">XLN</span>et: Generalized Autoregressive Pretraining for Language Understanding”</a>, Yang et al 2019 (for comparison, <a href="https://twitter.com/joeddav/status/1285238997011267585" title="So I tried out GPT-3's trick of conditioning on training data with XLNet. While it doesn't do as well as the 175B GPT-3, it does much better than the version which is the same size as XLNet (0.4B). The visual below is from their paper on Winogrande – I added the squares for XLNet.">Joe Davison</a> finds <span class="smallcaps-auto">XLN</span>et is ~10–16× more parameter-efficient at few-shot learning: <span class="smallcaps-auto">XLN</span>et-0.4b ≅ <span class="smallcaps-auto">GPT-3-6</span>.7b)</li>
</ul></li>
<li><p><em>compressed history/state</em>:</p>
<ul>
<li><a href="https://arxiv.org/abs/1911.05507#deepmind" class="docMetadata" data-popup-title="Compressive Transformers for Long-Range Sequence Modelling" data-popup-author="Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap" data-popup-date="2020-07-25" data-popup-abstract="We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.">“Compressive Transformers for Long-Range Sequence Modelling”</a>, Rae et al 2019</li>
<li><a href="https://arxiv.org/abs/2006.11527" class="docMetadata" data-popup-title="Memory Transformer" data-popup-author="Mikhail S. Burtsev, Grigory V. Sapunov" data-popup-date="2020-07-08" data-popup-abstract="Transformer-based models have achieved state-of-the-art results in many natural language processing (NLP) tasks. The self-attention architecture allows us to combine information from all elements of a sequence into context-aware representations. However, all-to-all attention severely hurts the scaling of the model to large sequences. Another limitation is that information about the context is stored in the same element-wise representations. This makes the processing of properties related to the sequence as a whole more difficult. Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model. Memory-augmented neural networks (MANNs) extend traditional neural architectures with general-purpose memory for representations. MANNs have demonstrated the capability to learn simple algorithms like Copy or Reverse and can be successfully trained via backpropagation on diverse tasks from question answering to language modeling outperforming RNNs and LSTMs of comparable complexity. In this work, we propose and study two extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, and (2) creating memory bottleneck for the global information. We evaluate these memory augmented Transformers on machine translation task and demonstrate that memory size positively correlates with the model performance. Attention patterns over the memory suggest that it improves the model's ability to process a global context. We expect that the application of Memory Transformer architectures to the tasks of language modeling, reading comprehension, and text summarization, as well as other NLP tasks that require the processing of long contexts will contribute to solving challenging problems of natural language understanding and generation.">“Memory Transformer”</a>, Burtsev &amp; Sapunov 2020</li>
</ul></li>
</ul></li>
<li><p><span class="smallcaps">Matrix Algebra Optimizations</span> (rewriting the softmax/dot-product to be linear):</p>
<ul>
<li><a href="https://arxiv.org/abs/1812.01243#sensetime" class="docMetadata" data-popup-title="Efficient Attention: Attention with Linear Complexities" data-popup-author="Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, Hongsheng Li" data-popup-date="2020-07-25" data-popup-abstract="The attention mechanism has seen wide applications in computer vision and natural language processing. Recent works developed the dot-product attention mechanism and applied it to various vision and language tasks. However, the memory and computational costs of dot-product attention grows quadratically with the spatiotemporal size of the input. Such growth prohibits the application of the mechanism on large inputs, e.g., long sequences, high-resolution images, or large videos. To remedy this drawback, this paper proposes a novel efficient attention mechanism, which is equivalent to dot-product attention but has substantially less memory and computational costs. The resource efficiency allows more widespread and flexible incorporation of efficient attention modules into a neural network, which leads to improved accuracies. Empirical evaluations on object recognition and image classification demonstrated the effectiveness of its advantages. Models with efficient attention achieved state-of-the-art performance on MS-COCO 2017 and significant improvement on ImageNet. Further, the resource efficiency of the mechanism democratizes attention to complicated models, which were unable to incorporate original dot-product attention due to prohibitively high costs. As an exemplar, an efficient attention-augmented model achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.">“Efficient Attention: Attention with Linear Complexities”</a>, Shen et al 2018 (<a href="https://medium.com/@cmsflash/efficient-attention-attention-with-linear-complexities-b3c00c4348e3" title="Efficient Attention: Attention with Linear Complexities">blog</a>)</li>
<li><a href="https://arxiv.org/abs/2006.04768#facebook" class="docMetadata" data-popup-title="Linformer: Self-Attention with Linear Complexity" data-popup-author="Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma" data-popup-date="2020-07-25" data-popup-abstract="Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.">“Linformer: Self-Attention with Linear Complexity”</a>, Wang et al 2020</li>
<li><a href="https://arxiv.org/abs/2006.16236" class="docMetadata" data-popup-title="Transformers are RNNs: Fast Autoregressive Transformers with Linear
  Attention" data-popup-author="Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret" data-popup-date="2020-07-25" data-popup-abstract="Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\mathcal{O}\left(N^2\right)$ to $\mathcal{O}\left(N\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.">“Transformers are <span class="smallcaps-auto">RNN</span>s: Fast Autoregressive Transformers with Linear Attention”</a>, Katharopoulos et al 2020</li>
</ul></li>
<li><p><span class="smallcaps">Approximations</span>:</p>
<ul>
<li><p><em>sparsity</em>:</p>
<ul>
<li><a href="https://arxiv.org/abs/1904.10509#openai" class="docMetadata" data-popup-title="Generating Long Sequences with Sparse Transformers" data-popup-author="Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever" data-popup-date="2019-12-23" data-popup-abstract="Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.">Sparse Transformer: “Generating Long Sequences with Sparse Transformers”</a>, Child et al 2019 (<a href="https://openai.com/blog/sparse-transformer/" class="docMetadata" data-popup-title="Generative Modeling with Sparse Transformers: We’ve developed the Sparse Transformer, a deep neural network which sets new records at predicting what comes next in a sequence—whether text, images, or sound. It uses an algorithmic improvement of the <em>attention</em> mechanism to extract patterns from sequences 30× longer than possible previously" data-popup-author="Rewon Child, Scott Gray (OpenAI)" data-popup-date="2019-04-23" data-popup-abstract="<p>One existing challenge in AI research is modeling long-range, subtle interdependencies in complex data like images, videos, or sounds. The Sparse Transformer incorporates an O(N ⋅ √N) reformulation of the O(N<sup>2</sup>) Transformer self-attention mechanism, along with several other improvements, to apply it directly to these rich data types. Previously, models used on these data were specifically crafted for one domain or difficult to scale to sequences more than a few thousand elements long. In contrast, our model can model sequences with tens of thousands of elements using hundreds of layers, achieving state-of-the-art performance across multiple domains. At OpenAI, we’re using it to help us build AI systems that possess a greater ability to understand the world…Even computing a single attention matrix, however, can become impractical for very large inputs. We instead use sparse attention patterns, where each output position only computes weightings from a subset of input positions.</p><p><strong>Future work and limitations</strong>:</p><ul><li>The sparse attention patterns we introduced are only preliminary steps in the direction of efficient modeling of long sequences. We think exploring different patterns and combinations of sparsity is useful, and that learning sparse patterns is a particularly promising avenue of research for the next generation of neural network architectures.</li><li>Even with the improvements we described above, autoregressive sequence generation still seems impractical for very high resolution images or video. The optimized attention operations we have introduced, however, may be useful primitives to combine with other approaches to modeling high dimensional data, like multi-scale approaches.</li></ul>" title="Generative Modeling with Sparse Transformers: We've developed the Sparse Transformer, a deep neural network which sets new records at predicting what comes next in a sequence --- whether text, images, or sound. It uses an algorithmic improvement of the *attention* mechanism to extract patterns from sequences 30× longer than possible previously.">blog</a>)</li>
<li><a href="https://arxiv.org/abs/1905.07799#facebook" class="docMetadata" data-popup-title="Adaptive Attention Span in Transformers" data-popup-author="Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin" data-popup-date="2020-08-12" data-popup-abstract="We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.">“Adaptive Attention Span in Transformers”</a>, Sukhbaatar et al 2019</li>
<li><a href="https://arxiv.org/abs/2001.04451#google" class="docMetadata" data-popup-title="Reformer: The Efficient Transformer" data-popup-author="Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya" data-popup-date="2020-02-14" data-popup-abstract="Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.">“Reformer: The Efficient Transformer”</a>, Kitaev et al 2019 (<a href="https://www.pragmatic.ml/reformer-deep-dive/" title="'A Deep Dive into the Reformer', Madison May">blog</a>)</li>
<li><a href="https://arxiv.org/abs/1902.09113" class="docMetadata" data-popup-title="Star-Transformer" data-popup-author="Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, Zheng Zhang" data-popup-date="2020-07-25" data-popup-abstract="Although Transformer has achieved great successes on many NLP tasks, its heavy structure with fully-connected attention connections leads to dependencies on large training data. In this paper, we present Star-Transformer, a lightweight alternative by careful sparsification. To reduce model complexity, we replace the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node. Thus, complexity is reduced from quadratic to linear, while preserving capacity to capture both local composition and long-range dependency. The experiments on four tasks (22 datasets) show that Star-Transformer achieved significant improvements against the standard Transformer for the modestly sized datasets.">“Star-Transformer”</a>, Guo et al 2019</li>
<li><a href="https://arxiv.org/abs/2003.05997#google" class="docMetadata" data-popup-title="Efficient Content-Based Sparse Attention with Routing Transformers" data-popup-author="Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier" data-popup-date="2020-03-17" data-popup-abstract="Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to $O\left(n^{1.5}d\right)$ from $O\left(n^2d\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers.">“Efficient Content-Based Sparse Attention with Routing Transformers”</a>, Roy et al 2020</li>
<li><a href="https://arxiv.org/abs/2002.11296#google" class="docMetadata" data-popup-title="Sparse Sinkhorn Attention" data-popup-author="Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, Da-Cheng Juan" data-popup-date="2020-07-25" data-popup-abstract="We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.">“Sparse Sinkhorn Attention”</a>, Tay et al 2020 (<a href="https://www.pragmatic.ml/sparse-sinkhorn-attention/" title="'Optimal Transport and the Sinkhorn Transformer', Madison May">blog</a>)</li>
<li><a href="https://arxiv.org/abs/2007.14062#google" class="docMetadata" data-popup-title="Big Bird: Transformers for Longer Sequences" data-popup-author="Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed" data-popup-date="2020-07-29" data-popup-abstract="Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.">“Big Bird: Transformers for Longer Sequences”</a>, Zaheer et al 2020</li>
</ul></li>
<li><p><em>global↔︎local attention</em>:</p>
<ul>
<li><p><a href="https://openreview.net/forum?id=ByeMPlHKPH">“<span class="smallcaps-auto">LSRA</span>: Lite Transformer with Long-Short Range Attention”</a>, Wu et al 2019</p></li>
<li><p><a href="https://arxiv.org/abs/1911.02972#facebook" class="docMetadata" data-popup-title="Blockwise Self-Attention for Long Document Understanding" data-popup-author="Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, Jie Tang" data-popup-date="2020-07-25" data-popup-abstract="We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.">“Block<span class="smallcaps-auto">BERT</span>: Blockwise self-attention for long document understanding”</a>, Qiu et al 2019</p></li>
<li><p><a href="https://arxiv.org/abs/1911.04070" class="docMetadata" data-popup-title="BP-Transformer: Modelling Long-Range Context via Binary Partitioning" data-popup-author="Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, Zheng Zhang" data-popup-date="2020-07-25" data-popup-abstract="The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields $O(k\cdot n\log (n/k))$ connections where $k$ is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.">“BP-Transformer: Modelling Long-Range Context via Binary Partitioning”</a>, Ye et al 2019</p></li>
<li><p><a href="https://arxiv.org/abs/2004.05150" class="docMetadata" data-popup-title="Longformer: The Long-Document Transformer" data-popup-author="Iz Beltagy, Matthew E. Peters, Arman Cohan" data-popup-date="2020-07-25" data-popup-abstract="Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.">“Longformer: The Long-Document Transformer”</a>, Beltagy et al 2020</p></li>
<li><p><a href="https://arxiv.org/abs/2004.08483" class="docMetadata" data-popup-title="ETC: Encoding Long and Structured Data in Transformers" data-popup-author="Joshua Ainslie, Santiago Ontanon, Chris Alberti, Philip Pham, Anirudh Ravula, Sumit Sanghai" data-popup-date="2020-07-25" data-popup-abstract="Transformer-based models have pushed the state of the art in many natural language processing tasks. However, one of their main limitations is the quadratic computational and memory cost of the standard attention mechanism. In this paper, we present a new family of Transformer models, which we call the Extended Transformer Construction (ETC), that allows for significant increases in input sequence length by introducing a new global-local attention mechanism between a global memory and the standard input tokens. We also show that combining global-local attention with relative position encodings allows ETC to handle structured data with ease. Empirical results on the Natural Questions data set show the promise of the approach.">“<span class="smallcaps-auto">ETC</span>: Encoding Long and Structured Data in Transformers”</a>, Ainslie et al 2020</p>
<p>One question I have about methods which reuse part of the context window for memory: can we do curriculum training, and efficiently train a Transformer normally with a fixed window for most of the training, and then switch over to overloading part of the context as the new memory? That would hypothetically save much of the compute, although one might wonder if the learned algorithms &amp; representations will be inferior compared to a Transformer which was always trained with memory.</p></li>
<li><p><a href="https://arxiv.org/abs/2005.08100#google" class="docMetadata" data-popup-title="Conformer: Convolution-augmented Transformer for Speech Recognition" data-popup-author="Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang" data-popup-date="2020-07-25" data-popup-abstract="Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.">“Conformer: Convolution-augmented Transformer for Speech Recognition”</a>, Gulatti et al 2020</p></li>
</ul></li>
<li><p><em>miscellaneous</em> (dropping components, non-trainable/randomized parts, etc):</p>
<ul>
<li><a href="https://arxiv.org/abs/1801.10198#google" class="docMetadata" data-popup-title="Generating Wikipedia by Summarizing Long Sequences" data-popup-author="Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, Noam Shazeer" data-popup-date="2020-07-25" data-popup-abstract="We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.">“Generating Wikipedia by Summarizing Long Sequences”</a>, Liu et al 2018</li>
<li><a href="https://arxiv.org/abs/1901.10430#facebook" class="docMetadata" data-popup-title="Pay Less Attention with Lightweight and Dynamic Convolutions" data-popup-author="Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, Michael Auli" data-popup-date="2020-01-03" data-popup-abstract="Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the <span class=&quot;smallcaps-auto&quot;>WMT</span>'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 <span class=&quot;smallcaps-auto&quot;>BLEU</span>.">“Pay Less Attention with Lightweight and Dynamic Convolutions”</a>, Wu et al 2019</li>
<li><a href="https://arxiv.org/abs/1809.04281#google" class="docMetadata" data-popup-title="Music Transformer" data-popup-author="Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, Douglas Eck" data-popup-date="2020-07-25" data-popup-abstract="Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.">“Music Transformer”</a>, Huang et al 2020</li>
<li><a href="https://arxiv.org/abs/2005.00743#google" class="docMetadata" data-popup-title="Synthesizer: Rethinking Self-Attention in Transformer Models" data-popup-author="Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng" data-popup-date="2020-07-25" data-popup-abstract="The dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).">“Synthesizer: Rethinking Self-Attention in Transformer Models”</a>, Tay et al 2020</li>
<li><a href="https://arxiv.org/abs/2006.03555" class="docMetadata" data-popup-title="Masked Language Modeling for Proteins via Linearly Scalable Long-Context
  Transformers" data-popup-author="Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Jared Davis, Tamas Sarlos, David Belanger, Lucy Colwell, Adrian Weller" data-popup-date="2020-07-25" data-popup-abstract="Transformer models have achieved state-of-the-art results across a diverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as biological sequence analysis, may fall short of meeting these assumptions, precluding exploration of these models. To address this challenge, we present a new Transformer architecture, Performer, based on Fast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales linearly rather than quadratically in the number of tokens in the sequence, is characterized by sub-quadratic space complexity and does not incorporate any sparsity pattern priors. Furthermore, it provides strong theoretical guarantees: unbiased estimation of the attention matrix and uniform convergence. It is also backwards-compatible with pre-trained regular Transformers. We demonstrate its effectiveness on the challenging task of protein sequence modeling and provide detailed theoretical analysis.">“<span class="smallcaps-auto">FAVOR</span>: Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers”</a>, Choromanski et al 2020</li>
<li><a href="https://arxiv.org/abs/2006.03236" class="docMetadata" data-popup-title="Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
  Language Processing" data-popup-author="Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le" data-popup-date="2020-07-25" data-popup-abstract="With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at https://github.com/laiguokun/Funnel-Transformer.">“Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing”</a>, Dai et al 2020</li>
<li><a href="/docs/www/kentonl.com/91274b85e2840f9ab6f62e0e07cd3ec81335ddbc.pdf#google" class="docMetadata" data-popup-title="REALM: Retrieval-Augmented Language Model Pre-Training" data-popup-author="Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang (Google)" data-popup-date="2020-02-10" data-popup-abstract="<p>Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.</p><p>To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent <em>knowledge retriever</em>, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.</p><p>We demonstrate the effectiveness of Retrieval Augmented Language Model pre-training (<span class=&quot;smallcaps-auto&quot;>REALM</span>) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA).We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4–16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.</p>" rel="archived alternate nofollow" data-url-original="https://kentonl.com/pub/gltpc.2020.pdf#google" title="(Original URL: https://kentonl.com/pub/gltpc.2020.pdf#google )">“<span class="smallcaps-auto">REALM</span>: Retrieval-Augmented Language Model Pre-Training”</a>, Guu et al 2020</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>A Transformer is a considerably different architecture than an <span class="smallcaps-auto">RNN</span>, and is not that easy to explain, as it uses multiple convolutions to implement “attention”, allowing flexible internal control flow, over a large but finite input window, without any recurrency or hidden state or <span class="smallcaps-auto">LSTM</span> units necessary. For increasingly-technical explanations, see:</p>
<ul>
<li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">“Transformer: A Novel Neural Network Architecture for Language Understanding”</a> (Google)</li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a>/<a href="https://jalammar.github.io/illustrated-gpt2/">“The Illustrated <span class="smallcaps-auto">GPT-2</span> (Visualizing Transformer Language Models)”</a>, Jay Alammar</li>
<li><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/">“The Transformer—Attention is all you need”</a>, Michał Chromiak</li>
<li><a href="/docs/www/blog.floydhub.com/1f701b1ca25c1e5f8798a68e6268e824939068c1.html" rel="archived alternate nofollow" data-url-original="https://blog.floydhub.com/the-transformer-in-pytorch/" title="(Original URL: https://blog.floydhub.com/the-transformer-in-pytorch/ )">“How to code The Transformer in PyTorch”</a>, Samuel Lynn-Evans</li>
<li><a href="https://arxiv.org/abs/1706.03762" class="docMetadata" data-popup-title="Attention Is All You Need" data-popup-author="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin" data-popup-date="2019-08-26" data-popup-abstract="The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 <span class=&quot;smallcaps-auto&quot;>BLEU</span> on the <span class=&quot;smallcaps-auto&quot;>WMT</span> 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 <span class=&quot;smallcaps-auto&quot;>BLEU</span>. On the <span class=&quot;smallcaps-auto&quot;>WMT</span> 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art <span class=&quot;smallcaps-auto&quot;>BLEU</span> score of 41.8 after training for 3.5 days on eight <span class=&quot;smallcaps-auto&quot;>GPU</span>s, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.">“Attention Is All You Need”</a>, Vaswani et al 2017 (<a href="/docs/www/nlp.seas.harvard.edu/f871c64394b4a220f33113d4c59c50dcd7ccdf43.html" rel="archived alternate nofollow" data-url-original="http://nlp.seas.harvard.edu/2018/04/03/attention.html" title="(Original URL: http://nlp.seas.harvard.edu/2018/04/03/attention.html )">“The Annotated Transformer”</a>); <a href="https://arxiv.org/abs/1803.02155" class="docMetadata" data-popup-title="Self-Attention with Relative Position Representations" data-popup-author="Peter Shaw, Jakob Uszkoreit, Ashish Vaswani" data-popup-date="2019-08-26" data-popup-abstract="Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the <span class=&quot;smallcaps-auto&quot;>WMT</span> 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 <span class=&quot;smallcaps-auto&quot;>BLEU</span> and 0.3 <span class=&quot;smallcaps-auto&quot;>BLEU</span> over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.">“Self-Attention with Relative Position Representations”</a>, Shaw et al 2018</li>
<li><a href="https://arxiv.org/abs/1808.04444" class="docMetadata" data-popup-title="Character-Level Language Modeling with Deeper Self-Attention" data-popup-author="Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones" data-popup-date="2019-08-26" data-popup-abstract="<span class=&quot;smallcaps-auto&quot;>LSTM</span>s and other <span class=&quot;smallcaps-auto&quot;>RNN</span> variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms <span class=&quot;smallcaps-auto&quot;>RNN</span> variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.">“Character-Level Language Modeling with Deeper Self-Attention”</a>, Al-Rfou et al 2018</li>
<li><a href="https://arxiv.org/abs/1901.02860" class="docMetadata" data-popup-title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context" data-popup-author="Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov" data-popup-date="2019-08-26" data-popup-abstract="Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than <span class=&quot;smallcaps-auto&quot;>RNN</span>s and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.">“Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context”</a>, Dai et al 2019 (<a href="/docs/www/www.lyrn.ai/fc44a554e972b519fd603b3a222e5d3e3f6b6030.html" rel="archived alternate nofollow" data-url-original="https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/" title="(Original URL: https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/ )">“Transformer-XL—Combining Transformers and <span class="smallcaps-auto">RNN</span>s Into a State-of-the-art Language Model”</a>, Rani Horev)</li>
<li><a href="https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db">“Understanding <span class="smallcaps-auto">BERT</span> Transformer: Attention isn’t all you need—A parsing/composition framework for understanding Transformers”</a></li>
<li><a href="/docs/www/www.peterbloem.nl/1f32b34e9e20a416ffce3d23843057ea8e35a242.html" rel="archived alternate nofollow" data-url-original="http://www.peterbloem.nl/blog/transformers" title="Transformers are a very exciting family of machine learning architectures. Many good tutorials exist (e.g. [1, 2]) but in the last few years, transformers have mostly become simpler, so that it is now much more straightforward to explain how modern architectures work. This post is an attempt to explain directly [in PyTorch] how modern transformers work, and why, without some of the historical baggage. (Original URL: http://www.peterbloem.nl/blog/transformers )">“Transformers from scratch”</a>, Peter Bloem, August 2019</li>
<li><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html">“The Annotated <span class="smallcaps-auto">GPT-2</span>”</a>, Aman Arora</li>
<li><a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html#openai">“The Transformer Family”</a>, Lilian Weng 2020</li>
<li><a href="https://arxiv.org/abs/1909.01380" class="docMetadata" data-popup-title="The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives" data-popup-author="Elena Voita, Rico Sennrich, Ivan Titov" data-popup-date="2019-09-13" data-popup-abstract="We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We focus on the Transformers for our analysis as they have been shown effective on various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (<span class=&quot;smallcaps-auto&quot;>MLM</span>). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and how this process depends on the choice of learning objective. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for <span class=&quot;smallcaps-auto&quot;>MLM</span>, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top <span class=&quot;smallcaps-auto&quot;>MLM</span> layers.">“The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives”</a>, Voita et al 2019</li>
</ul>
<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn2" role="doc-endnote"><p>774M requires changes to nshepperd’s checkpointing, specifically, removing the <code>layer == 10</code> restriction in <code>model.py</code>, and letting the checkpointing code checkpoint as much as possible, which enables training minibatches <em>n</em>≤10 on my 2x1080tis. Diff:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode Diff collapse"><code class="sourceCode diff"><span class="kw">diff --git a/src/model.py b/src/model.py</span>
index 4e942d8..71092bc 100644
<span class="dt">--- a/src/model.py</span>
<span class="dt">+++ b/src/model.py</span>
<span class="dt">@@ -124,10 +124,10 @@ def block(x, scope, *, past, hparams):</span>
     with tf.variable_scope(scope):
         nx = x.shape[-1].value
         a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)
<span class="st">-        x = x + a</span>
<span class="va">+        x = x1 = x + a</span>
         m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)
         x = x + m
<span class="st">-        return x, present</span>
<span class="va">+        return x, present, x1</span>

 def past_shape(*, hparams, batch_size=None, sequence=None):
     return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]
<span class="dt">@@ -161,9 +161,9 @@ def model(hparams, X, past=None, scope='model', reuse=tf.AUTO_REUSE):</span>
         pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer
         assert len(pasts) == hparams.n_layer
         for layer, past in enumerate(pasts):
<span class="st">-            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)</span>
<span class="st">-            if layer == 10:</span>
<span class="st">-                tf.add_to_collection('checkpoints', h)</span>
<span class="va">+            h, present, x1 = block(h, 'h%d' % layer, past=past, hparams=hparams)</span>
<span class="va">+            if layer &lt; 48:</span>
<span class="va">+                tf.add_to_collection('checkpoints', x1)</span>
             presents.append(present)
         results['present'] = tf.stack(presents, axis=1)
         h = norm(h, 'ln_f')</code></pre></div>
<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn3" role="doc-endnote"><p>It would require either high-end <span class="smallcaps-auto">GPU</span>s with &gt;=16GB <span class="smallcaps-auto">VRAM</span>, or <span class="smallcaps-auto">TPU</span> instances (which were used to train it). <span class="smallcaps-auto">GPT-2-1</span>.5b can’t be trained on my 1080tis with either the nshepperd codebase or Shawn Presser’s fork, although Presser has a <a href="https://colab.research.google.com/drive/1BXry0kcm869-RVHHiY6NZmY9uBzbkf1Q">Google Colab notebook using <span class="smallcaps-auto">TPU</span>s</a> which can train it.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Other examples of finetuning are <a href="/docs/www/svilentodorov.xyz/47932fa3b89eb084ecbb386d2613c4f95f4c7784.html" rel="archived alternate nofollow" data-url-original="https://svilentodorov.xyz/blog/gpt-15b-chat-finetune/" title="Talking to Myself or How I Trained GPT2-1.5b for Rubber Ducking using My Facebook Chat Data: Using only Google Colab (Original URL: https://svilentodorov.xyz/blog/gpt-15b-chat-finetune/ )">Facebook Messenger logs</a>, nshepperd’s unpublished Linux kernel C source code &amp; <span class="smallcaps-auto">IRC</span>-log training, and <a href="https://www.thisstorydoesnotexist.com/">story prompts</a>. And, while it doesn’t use <span class="smallcaps-auto">GPT-2-117M</span>, too good to not mention is <a href="https://stackroboflow.com/">“Stack Roboflow: This Question Does Not Exist”</a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><span class="smallcaps-auto">GPT-2</span> completions of 26 prompts: “Ozymandius”/“One Art”/“The Road Not Taken”/“Where the Sidewalk Ends”/“Because I could not stop for Death”/“Inferno, Canto I”/“In Flanders Field”/“O Captain! My Captain!”/“Howl”/“The Tyger”/“Outsight”/“Zuang Zhou Dreams of Being a Butterfly”/“Sonnet”/“Oh, the Places You’ll Go!”/“The Hollow Men”/“The Summer Day”/“A Just-Finishing Candle”/“A Psalm of Life”/“Still I Rise!”/“The Second Coming”/“Do not go gentle into that good night”/“Kubla Khan”/“Edge”/“The Raven”/“There Will Come Soft Rains”/“The Lorax”.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>For example, 2 people finetuned <span class="smallcaps-auto">GPT-2-117M</span> on an <span class="smallcaps-auto">IRC</span> channel’s logs, getting losses of 1.95 &amp; 2.3; why was the latter’s loss 18% worse compared to the former when they were using the same <span class="smallcaps-auto">IRC</span> channel, <span class="smallcaps-auto">GPT-2-117M</span> pretrained model, training codebase, &amp; had both apparently converged? Because. while the <span class="smallcaps-auto">IRC</span> channel was the same, they used different <span class="smallcaps-auto">IRC</span> clients which had different <span class="smallcaps-auto">IRC</span> log <em>formatting conventions</em>—the former’s logs had the full timestamp prefixed to each line, and the latter didn’t. Said timestamps made up ~20 characters of ~110 character lines, or, ~18% of each line! So the models were performing identically on the content that mattered, and the much lower loss was simply because of near-perfect prediction of the highly-repetitive &amp; predictable timestamps on every line. (Indeed, given the limited window of <span class="smallcaps-auto">GPT-2-117M</span>, arguably the model with the worse loss would be better in terms of generating fun coherent samples.)<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>I have 2 <span class="smallcaps-auto">GPU</span>s but nshepperd’s code does not (yet) support multi-<span class="smallcaps-auto">GPU</span> training easily. Some support using Horovod for multi-<span class="smallcaps-auto">GPU</span> has been added but I cannot vouch for it. <a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>I discovered this while being puzzled why <code>--batchsize 32</code> did not lead to instant out-of-memory errors for training; similarly, if you make the mistake of sampling with the option <code>--top 40</code>, what you are actually doing is sampling with the default <code>--top_k 0</code>. Oops.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>It is possible that the additional training <em>was</em> helping, because the remaining tiny changes in the loss might translate to large perceived quality improvements—while the loss didn’t change, the samples from later on did strike me as better. This was something I thought I noticed with char-<span class="smallcaps-auto">RNN</span> as well, that the loss became a bad guide to quality when the NN had mostly converged. On the other hand, with larger <span class="smallcaps-auto">GPT-2</span>s, like <span class="smallcaps-auto">GPT-2-1</span>.5b, the relationship between loss and perceived quality seems even more opaque, with quality sometimes worsening even as the training loss decreases rapidly.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>One might worry that by taking up space in the model’s limited context ‘window’ of inputs, because the Transformer has no hidden state or ‘memory’, such inline metadata would be a bad thing as it will push real words out of the context window, thereby degrading quality and making it even more incoherent &amp; rambling.</p>
<p>But on the other hand, if it <em>does</em> learn to associate specific IDs with genres/topics, then repetition of the inline metadata serves as a ‘mnemonic’ for global information which is available to all subsequent iterations of the model, serving as a crude memory itself.</p>
<p>For example, if Homeric pastiche has ID #16452, then as long as the final iteration of the model overlaps for just the ID with the first iteration of model during sampling and both see “16452”, all models will be able to consistently agree on generating Homeric pastiche rather than some other pastiche because they all see the same ID somewhere in their context window &amp; that guides their generation.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p><a href="/docs/www/old.reddit.com/b4443af89a9a1f33b705355fc4ebb8c3d8a8f4cd.html" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/user/starspawn0" title="(Original URL: https://old.reddit.com/user/starspawn0 )">starspawn0</a> has collated some of the results:</p>
<blockquote>
<ul>
<li>All U.S. presidents and Russian leaders in temporal order, where the order was not specified in the documents used; also, all tennis champions in international competitions over the years. So, temporal order can be extracted.</li>
<li>The longitude and latitude of cities in the U.S. and Europe, along with their relative distances.</li>
<li>The <em>relative</em> size of many kinds of objects, like cars, elephants, humans, houses, and so on—which object is larger than which others.</li>
<li>The exact sizes of many objects in meters, with reasonably small error. For example, it might say the dimensions of a windshield are about 1.4 meters by 1 meter.</li>
<li>Which kinds of animals are dangerous, which are not; and which kinds of objects (e.g.&nbsp;“fire”) are dangerous, and which are not (e.g.&nbsp;“water”).</li>
<li>Which animals are smarter than which other ones; which animals are faster, which are slower; which animals are heavy, which are light; which animals live in water, which do not.</li>
<li>Which cities cause arousal (e.g.&nbsp;“fun”, “exciting”), and which do not; which are expensive, which are not; which are dangerous, which are not; which are religious, which are not; which are large, which are not; which are hot, which are not; which are wealthy, which are not; which have a recognized intellectual culture, which do not.</li>
<li>Which kinds of clothes are appropriate for different age groups; which kinds cause emotional arousal; which kinds are expensive; which kinds are appropriate for different sexes; which kinds you expect to find in different locations; which kinds are associated with wealth; which are not appropriate for hot weather, which are; same for cold weather.</li>
<li>Qualities of mythological creatures—like the ones for animals.</li>
<li>Qualities of professions and professionals: age, arousal, danger, gender, intelligence, location, valence, wealth.</li>
<li>Qualities of sports and sportsmen / women: arousal, danger, gender, intelligence, location, speed, wealth.</li>
<li>Qualities of states: cost, intelligence, political, religiosity, size, temperature, wealth.</li>
<li>Qualities of types of weather and weather phenomena (e.g.&nbsp;“tornado”): danger, temperature, wetness.</li>
<li>Physical properties of objects, such as rigidness and strength. Probably also includes transparency, softness, hardness, round, prickly, angular, and so on.</li>
<li>Relations like whole-and-part, and relative locations of a part within an object: e.g.&nbsp;hand is connected to arm, arm is connected to shoulder, shoulder is connected to neck, neck is connected to head.</li>
<li>Properties of countries and cities: geolocation, <span class="smallcaps-auto">GDP</span>, <span class="smallcaps-auto">GNI</span> per-capita, CO2 emissions per-capita, fertility rate, amount of internet use, calling code, military expenditure, life expectancy, energy use, population, places imported from, how long they’ve had a national anthem, kinds of sports, <span class="smallcaps-auto">GDP</span> growth, crime rate, and so on.</li>
<li>Binary attributes of countries and cities: continent, time zones, contained-by (which regions contain which countries; which countries contain which cities; which boroughs are contained in which cities; and perhaps even relations between the boroughs—which border which others, how they are shaped, and how large they are), language, high or low crime?, military conflicts, athletes, medals won, organizations founded, schools founded, companies founded, weather, type of government, officials, and many more.</li>
<li>It’s even possible to predict the qualities of objects not in the training corpus, using something called the <a href="https://en.wikipedia.org/wiki/Bouba-Kiki_effect" class="docMetadata" data-popup-title="Bouba/kiki effect" data-popup-author="English Wikipedia" data-popup-abstract="<p>The <b>bouba/kiki effect</b> is a non-arbitrary mapping between speech sounds and the visual shape of objects. This effect was first observed by German-American psychologist Wolfgang Köhler in 1929. In psychological experiments first conducted on the island of Tenerife, Köhler showed forms similar to those shown at the right and asked participants which shape was called &quot;takete&quot; and which was called &quot;baluba&quot;. Although not explicitly stated, Köhler implies that there was a strong preference to pair the jagged shape with &quot;takete&quot; and the rounded shape with &quot;baluba&quot;.</p>" title="Wikipedia: Bouba-Kiki effect">Bouba-Kiki effect</a></li>
</ul>
<p>…</p>
<p>[Some references:]</p>
<ul>
<li>Grand et al 2018, <a href="https://arxiv.org/abs/1802.01241" class="docMetadata" data-popup-title="Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings" data-popup-author="Gabriel Grand, Idan Asher Blank, Francisco Pereira, Evelina Fedorenko" data-popup-date="2019-08-26" data-popup-abstract="The words of a language reflect the structure of the human mind, allowing us to transmit thoughts between individuals. However, language can represent only a subset of our rich and detailed cognitive architecture. Here, we ask what kinds of common knowledge (semantic memory) are captured by word meanings (lexical semantics). We examine a prominent computational model that represents words as vectors in a multidimensional space, such that proximity between word-vectors approximates semantic relatedness. Because related words appear in similar contexts, such spaces—called &quot;word embeddings&quot; - can be learned from patterns of lexical co-occurrences in natural language. Despite their popularity, a fundamental concern about word embeddings is that they appear to be semantically &quot;rigid&quot;: inter-word proximity captures only overall similarity, yet human judgments about object similarities are highly context-dependent and involve multiple, distinct semantic features. For example, dolphins and alligators appear similar in size, but differ in intelligence and aggressiveness. Could such context-dependent relationships be recovered from word embeddings? To address this issue, we introduce a powerful, domain-general solution: &quot;semantic projection&quot; of word-vectors onto lines that represent various object features, like size (the line extending from the word &quot;small&quot; to &quot;big&quot;), intelligence (from &quot;dumb&quot; to &quot;smart&quot;), or danger (from &quot;safe&quot; to &quot;dangerous&quot;). This method, which is intuitively analogous to placing objects &quot;on a mental scale&quot; between two extremes, recovers human judgments across a range of object categories and properties. We thus show that word embeddings inherit a wealth of common knowledge from word co-occurrence statistics and can be flexibly manipulated to express context-dependent meanings.">“Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings”</a></li>
<li>Gupta et al 2015, <a href="/docs/www/www.aclweb.org/11873b3e61d09321f61e5fb351d5d36c74d732ed.html" rel="archived alternate nofollow" data-url-original="http://www.aclweb.org/anthology/D15-1002" title="(Original URL: http://www.aclweb.org/anthology/D15-1002 )">“Distributional vectors encode referential attributes”</a></li>
<li><a href="/docs/www/blog.acolyer.org/e5065306f4699ba75e39dbd8a7ecd6ef9bbbbb5b.html" rel="archived alternate nofollow" data-url-original="https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/" title="(Original URL: https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/ )">“Dynamic word embeddings for evolving semantic discovery”</a> (on Yao et al 2018)</li>
<li>Forbes &amp; Choi 2017, <a href="https://arxiv.org/abs/1706.03799" class="docMetadata" data-popup-title="Verb Physics: Relative Physical Knowledge of Actions and Objects" data-popup-author="Maxwell Forbes, Yejin Choi" data-popup-date="2019-08-26" data-popup-abstract="Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., &quot;My house is bigger than me.&quot; However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, &quot;Tyler entered his house&quot; implies that his house is bigger than Tyler.   In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.">“Verb Physics: Relative Physical Knowledge of Actions and Objects”</a></li>
</ul>
</blockquote>
<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn12" role="doc-endnote"><p>Child et al 2019:</p>
<blockquote>
<p>We also introduce (a) a variation on architecture and initialization to train deeper networks, (b) the recomputation of attention matrices to save memory, and (c) fast attention kernels for training. We call networks with these changes “Sparse Transformers”, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, <span class="smallcaps-auto">CIFAR-10</span>, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.</p>
<p>…<strong>5.4. Saving memory by recomputing attention weights</strong></p>
<p>Gradient checkpointing has been shown to be effective in reducing the memory requirements of training deep neural networks (<a href="https://arxiv.org/abs/1604.06174" class="docMetadata" data-popup-title="Training Deep Nets with Sublinear Memory Cost" data-popup-author="Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin" data-popup-date="2019-08-26" data-popup-abstract="We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the <span class=&quot;smallcaps-auto&quot;>GPU</span> memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory—giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences." title="Training deep nets with sublinear memory cost">Chen et al., 2016</a>), (<a href="http://papers.nips.cc/paper/6220-memory-efficient-backpropagation-through-time" title="Memory-efficient backpropagation through time">Gruslys et al., 2016</a>). It is worth noting, however, that this technique is particularly effective for self-attention layers when long sequences are processed, as memory usage is high for these layers relative to the cost of computing them.Using recomputation alone, we are able to train dense attention networks with hundreds of layers on sequence lengths of 16,384, which would be infeasible on modern hardware otherwise. In our experiments, we recompute the attention and feed-forward blocks during the backwards pass.</p>
<p>…For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Equation"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-semantics"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span><span class="mjx-msqrt MJXc-space2"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">√</span></span><span class="mjx-box" style="padding-top: 0.124em; border-top: 1px solid;"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span></span></span></span>. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million).</p>
</blockquote>
<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn13" role="doc-endnote"><p>Because it is so large, I am providing an alternate rsync mirror: <code>rsync --verbose rsync://78.46.86.149:873/biggan/2019-05-13-gpt2-poetry-345m.tar.xz ./</code><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>The code turns out to multiply by a large number as a way of setting a default ‘highly unlikely’ value for each possible <span class="smallcaps-auto">BPE</span>, but in FP16, it can’t be represented and overflows, and so the output amusingly just becomes the <span class="smallcaps-auto">BPE</span> 0, which is the character ‘!’, so it kept printing out ‘!!!’. Indeed.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>The Colab environment has special support for mounting Google Drive, so a magical incantation like this will mount a Google Drive folder as a normal (slow) directory:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode Python"><code class="sourceCode python"><span class="im">from</span> google.colab <span class="im">import</span> drive
drive.mount(<span class="st">'/content/drive'</span>, force_remount<span class="op">=</span><span class="va">True</span>)

<span class="co">#!rm -f ~/drive</span>
<span class="co">#!ln -s /content/drive/My\ Drive ~/drive</span>
<span class="op">!</span>mkdir <span class="op">-</span>p <span class="op">/</span>drive
<span class="co">#umount /drive</span>
<span class="op">!</span>mount <span class="op">--</span>bind <span class="op">/</span>content<span class="op">/</span>drive<span class="op">/</span>My<span class="op">\</span> Drive <span class="op">/</span>drive</code></pre></div>
<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn16" role="doc-endnote"><p>We noticed that, in fact, our preemptible <span class="smallcaps-auto">TPU</span>s would always preempt precisely at midnight. I speculated that, as discussed in <a href="https://landing.google.com/sre/sre-book/chapters/service-level-objectives/" title="Chapter 4 - Service Level Objectives">the Google <span class="smallcaps-auto">SRE</span> handbook</a> where they cover how the Chubby service is deliberately taken down at random to live down to its uptime promises, preemptible <span class="smallcaps-auto">TPU</span>s were being deliberately taken down to stop users from treating them like on-demand <span class="smallcaps-auto">TPU</span>s, and this simply wasn’t documented. Discussing our problems with <span class="smallcaps-auto">TFRC</span>, this apparently was correct.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>Presser is convinced that <span class="smallcaps-auto">TPU</span> power is greatly overrated and most <span class="smallcaps-auto">TPU</span> projects have made poor use of the available power, as they get far less speedups than one would expect over <span class="smallcaps-auto">GPU</span>s. Oddly, there doesn’t seem to have been much work on using multiple <span class="smallcaps-auto">TPU</span>s outside of a <span class="smallcaps-auto">TPU</span> pod configuration, and <span class="smallcaps-auto">TFRC</span> did not seem to know of an equivalent to the ‘swarm’.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p><a href="https://arxiv.org/abs/1909.10705" class="docMetadata" data-popup-title="Do Massively Pretrained Language Models Make Better Storytellers?" data-popup-author="Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, Christopher D. Manning" data-popup-date="2019-10-03" data-popup-abstract="Large neural language models trained on massive amounts of text have emerged as a formidable strategy for Natural Language Understanding tasks. However, the strength of these models as Natural Language Generators is less clear. Though anecdotal evidence suggests that these models generate better quality text, there has been no detailed study characterizing their generation abilities. In this work, we compare the performance of an extensively pretrained model, OpenAI <span class=&quot;smallcaps-auto&quot;>GPT</span>2-117 (Radford et al., 2019), to a state-of-the-art neural story generation model (Fan et al., 2018). By evaluating the generated text across a wide variety of automatic metrics, we characterize the ways in which pretrained models do, and do not, make better storytellers. We find that although <span class=&quot;smallcaps-auto&quot;>GPT</span>2-117 conditions more strongly on context, is more sensitive to ordering of events, and uses more unusual words, it is just as likely to produce repetitive and under-diverse text when using likelihood-maximizing decoding algorithms." title="Do Massively Pretrained Language Models Make Better Storytellers?">See et al 2019</a> also demonstrates problems with likelihood decoding strategies in <span class="smallcaps-auto">GPT-2</span> for story generation.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p>Alternative Ao3 fanfiction dumps, among others, are available on <a href="https://archive.org/details/@entropy11235813">the Internet Archive</a>.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div>
<div id="footer">
<p><a href="https://docs.google.com/forms/d/17cNf0ohMHKeTwmUM_V_hDVkirjsf8HLMWsJco8gt2Eg/viewform" title="Google Docs web form for submitting anonymized feedback to Gwern Branwen"><em>Submit Anonymous Feedback</em></a></p>
</div>
</article>
</main>

<link rel="stylesheet" type="text/css" href="/static/css/fonts.css">
<link rel="stylesheet" type="text/css" href="/static/css/default.css">
<link rel="stylesheet" type="text/css" href="/static/css/links.css">

<script src="/static/js/footnotes.js" defer=""></script>

<script src="/static/js/popups.js" defer=""></script>



<script src="/static/js/image-focus.js" defer=""></script>

<script src="/static/js/darkmode.js" defer=""></script>

<script src="/static/js/rewrite.js" defer=""></script>

<script id="googleAnalytics" src="https://www.googletagmanager.com/gtag/js?id=UA-18912926-1" async=""></script>
<script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-18912926-1');
    </script>

<div id="comments">
<div id="disqus_thread"></div>
<script>
        function dynamicallyLoadScript(url) {
            var script = document.createElement("script");
            script.src = url;
            document.head.appendChild(script);
        }

       var lazyLoadObserver = function(f, selectors) {
           if (('IntersectionObserver' in window) &&
               ('IntersectionObserverEntry' in window) &&
               ('intersectionRatio' in window.IntersectionObserverEntry.prototype)) {
                var intersectionObserver = new IntersectionObserver(function(entries) {
                     if (entries[0].intersectionRatio <= 0) return;

                     f();
               }, { rootMargin: '140%' } );

               var target = document.querySelector(selectors);
               if (!(target === null)) { intersectionObserver.observe(target); }
           }
           else { f(); }
      }

       var disqus_shortname = ''; // declare variable globally as a way to make sure insertDisqus runs once and only once
       var insertDisqus = function() {
        if (disqus_shortname == '') {
           disqus_shortname = 'gwern';
           (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
           })();
        }
       }
      // 1. Load Disqus comments
      lazyLoadObserver(insertDisqus, '#disqus_thread');

      // 2. sort tables: http://tablesorter.com/docs/ ; requires JQuery, which has been prepended to avoid multiple JS page loads & race conditions
      lazyLoadObserver(function(){dynamicallyLoadScript('/static/js/tablesorter.js')}, 'table');
      </script>
<noscript><p>Enable JavaScript for Disqus comments</p></noscript>
</div>
</body></html>